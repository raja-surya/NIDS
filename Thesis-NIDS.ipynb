{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c205db50",
   "metadata": {},
   "source": [
    "# Deep Learning Based Automatic Feature Extraction for Network Intrusion Detection System\n",
    "<br>\n",
    "\n",
    "## Project\n",
    "\n",
    "This research study proposes a combination of a deep learning technique and an ensemble model approach for detecting network intrusion attacks. In specific, we propose to focus on unsupervised pre-training using autoencoder for effective feature extraction from network traffic data. Our proposed NIDS model will consist of a stacked autoencoder which will extract useful latent features hidden in network traffic flow records, and then feed those in to an Stacking ensemble classifier which will detect and separate network attack traffic from normal traffic.\n",
    "<br>\n",
    "\n",
    "## DataSet \n",
    "\n",
    "https://research.unsw.edu.au/projects/unsw-nb15-dataset\n",
    "\n",
    "A partition from this dataset was configured as a training set and testing set, namely, UNSW_NB15_training-set.csv and UNSW_NB15_testing-set.csv respectively, by the creators of this dataset. The number of records in the training set is 175,341 records and the testing set is 82,332 records from the different types, attack and normal.\n",
    "<br>\n",
    "\n",
    "## Execution Steps\n",
    "\n",
    "###  1. Load Dataset and Examine the features\n",
    "###  2. Basic Pre-processing - Missing Values, Outliers Treatment etc.\n",
    "###  3. Data Preparation For Modelling\n",
    "\n",
    "#####     3.1 Check class imbalance\n",
    "#####     3.2 Transformation (One-hot encoding) of categorical variables\n",
    "#####     3.3 Split original dataset in to Train-Test data\n",
    "#####     3.4 Standard Scaling of Features\n",
    "\n",
    "###  4. Modelling\n",
    "\n",
    "###     4.1 Auto Encoder\n",
    "#####       4.1.1 Training AutoEncoder\n",
    "#####          4.1.1.1 Hyper-Parameter Tuning of Auto Encoder\n",
    "#####          4.1.1.2 Fit AutoEncoder\n",
    "#####       4.1.2 Verify AutoEncoder\n",
    "#####       4.1.3 Extract Features Using AutoEncoder\n",
    "\n",
    "\n",
    "###    4.2 Stacking Classifier Model\n",
    "\n",
    "#####      4.2.1 Base Models \n",
    "#####         4.2.1.1 Base Model-1 ( Support Vector Machine Classifier )\n",
    "#####       4.2.1.2 Base Model-2 ( Random Forest Classifier )\n",
    "#####       4.2.1.3 Base Model-3 (XGBoost Classifier)\n",
    "\n",
    "#####      4.2.2 Stacking Blender Model (Logistic Regression Classifier)\n",
    "\n",
    "### 5. Final Whole Model Validation\n",
    "###  6. Conclusion <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcac598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2f91320",
   "metadata": {},
   "source": [
    "#### Import essential libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba73391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6783f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b241af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2359cae7",
   "metadata": {},
   "source": [
    "#### Helper Methods Used For Various Purposes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82f77b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plotting functions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def data_type(variable):\n",
    "    if variable.dtype == np.int64 or variable.dtype == np.float64 or variable.dtype == np.uint8:\n",
    "        return 'numerical'\n",
    "    elif variable.dtype == 'category':\n",
    "        return 'categorical'\n",
    "    \n",
    "def univariate(variable, stats=True):\n",
    "    \n",
    "    if data_type(variable) == 'numerical':\n",
    "        sns.distplot(variable)\n",
    "        if stats == True:\n",
    "            print(variable.describe())\n",
    "    \n",
    "    elif data_type(variable) == 'categorical':\n",
    "        sns.countplot(variable)\n",
    "        if stats == True:\n",
    "            print(variable.value_counts())\n",
    "            \n",
    "    else:\n",
    "        print(\"Invalid variable passed: either pass a numeric variable or a categorical vairable.\")\n",
    "        \n",
    "def bivariate(var1, var2):\n",
    "    if data_type(var1) == 'numerical' and data_type(var2) == 'numerical':\n",
    "        sns.regplot(var1, var2)\n",
    "    elif (data_type(var1) == 'categorical' and data_type(var2) == 'numerical') or (data_type(var1) == 'numerical' and data_type(var2) == 'categorical'):        \n",
    "        sns.boxplot(var1, var2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "664c418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function to print confusion matrix, accuracy, sensitivity, FPR, precision, specificity etc\n",
    "\n",
    "def print_binary_classification_summary(y, y_pred):\n",
    "    \n",
    "    confusion = metrics.confusion_matrix(y, y_pred)\n",
    "    \n",
    "    print(\"\\n\\nConfusion Matrix :\\n\\n\", confusion)\n",
    "    TP = confusion[1,1] # true positive \n",
    "    TN = confusion[0,0] # true negatives\n",
    "    FP = confusion[0,1] # false positives\n",
    "    FN = confusion[1,0] # false negatives\n",
    "    \n",
    "    sensitivity = TP / float(TP+FN)\n",
    "    specificity = TN / float(TN+FP)\n",
    "    fpr = FP/ float(TN+FP)\n",
    "    precision = TP / float(TP+FP)\n",
    "    \n",
    "    print(\"\\nTN :\", TN)\n",
    "    print(\"\\nFP :\", FP)\n",
    "    print(\"\\nFN :\", FN)\n",
    "    print(\"\\nTP :\", TP)\n",
    "       \n",
    "    print(\"\\nACCURACY : \", float(TP + TN)/float(TP + TN + FP + FN))\n",
    "    \n",
    "    print(\"\\nSENSITIVITY : \", sensitivity)\n",
    "    \n",
    "    print(\"\\nPRECISION : \", precision)\n",
    "    \n",
    "    print(\"\\nFALSE POSITIVITY RATE : \", fpr)\n",
    "    \n",
    "    print(\"\\nSPECIFICITY : \", specificity)\n",
    "    \n",
    "    print(\"\\n\\n Classification Report :\\n\\n\", classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f7b694df",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Function to predict - Both classification labels and probabilities.\n",
    "\n",
    "def predict_and_proba(model, X):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_prob = model.predict_proba(X)\n",
    "    return y_pred, y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e46d4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to print grid search results like best scores and best hyper-parameters\n",
    "\n",
    "def grid_result_summary(grd_res, scoring_measures, refit):\n",
    "    \n",
    "    keys= list(scoring_measures.keys())\n",
    "    score_print_list = []\n",
    "    for key in keys:\n",
    "        score_print_list.append('mean_test_{}'.format(key))\n",
    "    \n",
    "    refit = 'mean_test_{}'.format(refit)\n",
    "    print(refit)\n",
    "    \n",
    "    # scores of GridSearch CV\n",
    "    scores = pd.DataFrame(grd_res.cv_results_)\n",
    "    \n",
    "    # print best hyperparameters\n",
    "    print(\"\\nBest hyperparameters: \", grd_res.best_params_)\n",
    "    \n",
    "    print(\"\\nBest Score : %0.5f\" % grd_res.best_score_)\n",
    "    \n",
    "    print(scores.loc[scores[refit] == grd_res.best_score_ , score_print_list])\n",
    "    \n",
    "    return grd_res.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b0096f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to plot learning curves\n",
    "\n",
    "def plot_learning_curves(loss, val_loss):\n",
    "    plt.plot(np.arange(len(loss)), loss, \"b.-\", label=\"Training loss\")\n",
    "    plt.plot(np.arange(len(val_loss)), val_loss, \"r.-\", label=\"Validation loss\")\n",
    "   # plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "    #plt.axis([0, 50, 0, 0.6])\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6237fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function to plot ROC\n",
    "\n",
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.3f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nAUC SCORE = %0.3f\" % auc_score)\n",
    "    \n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e0026",
   "metadata": {},
   "source": [
    "# 1. Load Dataset and Examine the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d55a2dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f45f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original public dataset. The original dataset is provided as two files as below.\n",
    "orig_train_df = pd.read_csv(\"data/training-set.csv\")\n",
    "orig_test_df = pd.read_csv(\"data/testing-set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e202af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orig_train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7efa881e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n",
       "       'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',\n",
       "       'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n",
       "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
       "       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
       "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
       "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
       "       'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b804e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.121478</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>258</td>\n",
       "      <td>172</td>\n",
       "      <td>74.087490</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.649902</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>734</td>\n",
       "      <td>42014</td>\n",
       "      <td>78.473372</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.623129</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>364</td>\n",
       "      <td>13186</td>\n",
       "      <td>14.170161</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.681642</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>628</td>\n",
       "      <td>770</td>\n",
       "      <td>13.677108</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.449454</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>534</td>\n",
       "      <td>268</td>\n",
       "      <td>33.373826</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.380537</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>534</td>\n",
       "      <td>268</td>\n",
       "      <td>39.417980</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.637109</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>534</td>\n",
       "      <td>354</td>\n",
       "      <td>26.683033</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.521584</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>534</td>\n",
       "      <td>354</td>\n",
       "      <td>32.593026</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.542905</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>534</td>\n",
       "      <td>354</td>\n",
       "      <td>31.313031</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.258687</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>534</td>\n",
       "      <td>268</td>\n",
       "      <td>57.985135</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       dur proto service state  spkts  dpkts  sbytes  dbytes       rate  \\\n",
       "0   1  0.121478   tcp       -   FIN      6      4     258     172  74.087490   \n",
       "1   2  0.649902   tcp       -   FIN     14     38     734   42014  78.473372   \n",
       "2   3  1.623129   tcp       -   FIN      8     16     364   13186  14.170161   \n",
       "3   4  1.681642   tcp     ftp   FIN     12     12     628     770  13.677108   \n",
       "4   5  0.449454   tcp       -   FIN     10      6     534     268  33.373826   \n",
       "5   6  0.380537   tcp       -   FIN     10      6     534     268  39.417980   \n",
       "6   7  0.637109   tcp       -   FIN     10      8     534     354  26.683033   \n",
       "7   8  0.521584   tcp       -   FIN     10      8     534     354  32.593026   \n",
       "8   9  0.542905   tcp       -   FIN     10      8     534     354  31.313031   \n",
       "9  10  0.258687   tcp       -   FIN     10      6     534     268  57.985135   \n",
       "\n",
       "   ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  ct_ftp_cmd  \\\n",
       "0  ...                 1               1             0           0   \n",
       "1  ...                 1               2             0           0   \n",
       "2  ...                 1               3             0           0   \n",
       "3  ...                 1               3             1           1   \n",
       "4  ...                 1              40             0           0   \n",
       "5  ...                 1              40             0           0   \n",
       "6  ...                 1              40             0           0   \n",
       "7  ...                 1              40             0           0   \n",
       "8  ...                 1              40             0           0   \n",
       "9  ...                 1              40             0           0   \n",
       "\n",
       "   ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  attack_cat  \\\n",
       "0                 0           1           1                0      Normal   \n",
       "1                 0           1           6                0      Normal   \n",
       "2                 0           2           6                0      Normal   \n",
       "3                 0           2           1                0      Normal   \n",
       "4                 0           2          39                0      Normal   \n",
       "5                 0           2          39                0      Normal   \n",
       "6                 0           1          39                0      Normal   \n",
       "7                 0           3          39                0      Normal   \n",
       "8                 0           3          39                0      Normal   \n",
       "9                 0           3          39                0      Normal   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      0  \n",
       "7      0  \n",
       "8      0  \n",
       "9      0  \n",
       "\n",
       "[10 rows x 45 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da8ee2",
   "metadata": {},
   "source": [
    "#### Load the features description file. This contains the description of the features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e15bafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_df = pd.read_csv(\"data/NUSW-NB15_features.csv\", encoding='mac_roman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "383160b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>srcip</td>\n",
       "      <td>nominal</td>\n",
       "      <td>Source IP address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>integer</td>\n",
       "      <td>Source port number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>dstip</td>\n",
       "      <td>nominal</td>\n",
       "      <td>Destination IP address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>dsport</td>\n",
       "      <td>integer</td>\n",
       "      <td>Destination port number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>proto</td>\n",
       "      <td>nominal</td>\n",
       "      <td>Transaction protocol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>state</td>\n",
       "      <td>nominal</td>\n",
       "      <td>Indicates to the state and its dependent proto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>dur</td>\n",
       "      <td>Float</td>\n",
       "      <td>Record total duration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>sbytes</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Source to destination transaction bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>dbytes</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Destination to source transaction bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>sttl</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Source to destination time to live value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>dttl</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Destination to source time to live value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>sloss</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Source packets retransmitted or dropped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>dloss</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Destination packets retransmitted or dropped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>service</td>\n",
       "      <td>nominal</td>\n",
       "      <td>http, ftp, smtp, ssh, dns, ftp-data ,irc  and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Sload</td>\n",
       "      <td>Float</td>\n",
       "      <td>Source bits per second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Dload</td>\n",
       "      <td>Float</td>\n",
       "      <td>Destination bits per second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Spkts</td>\n",
       "      <td>integer</td>\n",
       "      <td>Source to destination packet count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Dpkts</td>\n",
       "      <td>integer</td>\n",
       "      <td>Destination to source packet count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>swin</td>\n",
       "      <td>integer</td>\n",
       "      <td>Source TCP window advertisement value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>dwin</td>\n",
       "      <td>integer</td>\n",
       "      <td>Destination TCP window advertisement value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>stcpb</td>\n",
       "      <td>integer</td>\n",
       "      <td>Source TCP base sequence number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>dtcpb</td>\n",
       "      <td>integer</td>\n",
       "      <td>Destination TCP base sequence number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>smeansz</td>\n",
       "      <td>integer</td>\n",
       "      <td>Mean of the ?ow packet size transmitted by the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>dmeansz</td>\n",
       "      <td>integer</td>\n",
       "      <td>Mean of the ?ow packet size transmitted by the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>trans_depth</td>\n",
       "      <td>integer</td>\n",
       "      <td>Represents the pipelined depth into the connec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>res_bdy_len</td>\n",
       "      <td>integer</td>\n",
       "      <td>Actual uncompressed content size of the data t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Sjit</td>\n",
       "      <td>Float</td>\n",
       "      <td>Source jitter (mSec)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Djit</td>\n",
       "      <td>Float</td>\n",
       "      <td>Destination jitter (mSec)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Stime</td>\n",
       "      <td>Timestamp</td>\n",
       "      <td>record start time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Ltime</td>\n",
       "      <td>Timestamp</td>\n",
       "      <td>record last time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Sintpkt</td>\n",
       "      <td>Float</td>\n",
       "      <td>Source interpacket arrival time (mSec)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Dintpkt</td>\n",
       "      <td>Float</td>\n",
       "      <td>Destination interpacket arrival time (mSec)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>tcprtt</td>\n",
       "      <td>Float</td>\n",
       "      <td>TCP connection setup round-trip time, the sum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>synack</td>\n",
       "      <td>Float</td>\n",
       "      <td>TCP connection setup time, the time between th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>ackdat</td>\n",
       "      <td>Float</td>\n",
       "      <td>TCP connection setup time, the time between th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>is_sm_ips_ports</td>\n",
       "      <td>Binary</td>\n",
       "      <td>If source (1) and destination (3)IP addresses ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>ct_state_ttl</td>\n",
       "      <td>Integer</td>\n",
       "      <td>No. for each state (6) according to specific r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>ct_flw_http_mthd</td>\n",
       "      <td>Integer</td>\n",
       "      <td>No. of flows that has methods such as Get and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>is_ftp_login</td>\n",
       "      <td>Binary</td>\n",
       "      <td>If the ftp session is accessed by user and pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>ct_ftp_cmd</td>\n",
       "      <td>integer</td>\n",
       "      <td>No of flows that has a command in ftp session.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>ct_srv_src</td>\n",
       "      <td>integer</td>\n",
       "      <td>No. of connections that contain the same servi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>ct_srv_dst</td>\n",
       "      <td>integer</td>\n",
       "      <td>No. of connections that contain the same servi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>ct_dst_ltm</td>\n",
       "      <td>integer</td>\n",
       "      <td>No. of connections of the same destination add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>ct_src_ ltm</td>\n",
       "      <td>integer</td>\n",
       "      <td>No. of connections of the same source address ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>ct_src_dport_ltm</td>\n",
       "      <td>integer</td>\n",
       "      <td>No of connections of the same source address (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>ct_dst_sport_ltm</td>\n",
       "      <td>integer</td>\n",
       "      <td>No of connections of the same destination addr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>ct_dst_src_ltm</td>\n",
       "      <td>integer</td>\n",
       "      <td>No of connections of the same source (1) and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>attack_cat</td>\n",
       "      <td>nominal</td>\n",
       "      <td>The name of each attack category. In this data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>Label</td>\n",
       "      <td>binary</td>\n",
       "      <td>0 for normal and 1 for attack records</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    No.              Name      Type   \\\n",
       "0     1             srcip    nominal   \n",
       "1     2             sport    integer   \n",
       "2     3             dstip    nominal   \n",
       "3     4            dsport    integer   \n",
       "4     5             proto    nominal   \n",
       "5     6             state    nominal   \n",
       "6     7               dur      Float   \n",
       "7     8            sbytes    Integer   \n",
       "8     9            dbytes    Integer   \n",
       "9    10              sttl    Integer   \n",
       "10   11              dttl    Integer   \n",
       "11   12             sloss    Integer   \n",
       "12   13             dloss    Integer   \n",
       "13   14           service    nominal   \n",
       "14   15             Sload      Float   \n",
       "15   16             Dload      Float   \n",
       "16   17             Spkts    integer   \n",
       "17   18             Dpkts    integer   \n",
       "18   19              swin    integer   \n",
       "19   20              dwin    integer   \n",
       "20   21             stcpb    integer   \n",
       "21   22             dtcpb    integer   \n",
       "22   23           smeansz    integer   \n",
       "23   24           dmeansz    integer   \n",
       "24   25       trans_depth    integer   \n",
       "25   26       res_bdy_len    integer   \n",
       "26   27              Sjit      Float   \n",
       "27   28              Djit      Float   \n",
       "28   29             Stime  Timestamp   \n",
       "29   30             Ltime  Timestamp   \n",
       "30   31           Sintpkt      Float   \n",
       "31   32           Dintpkt      Float   \n",
       "32   33            tcprtt      Float   \n",
       "33   34            synack      Float   \n",
       "34   35            ackdat      Float   \n",
       "35   36   is_sm_ips_ports     Binary   \n",
       "36   37      ct_state_ttl    Integer   \n",
       "37   38  ct_flw_http_mthd    Integer   \n",
       "38   39      is_ftp_login     Binary   \n",
       "39   40        ct_ftp_cmd    integer   \n",
       "40   41        ct_srv_src    integer   \n",
       "41   42        ct_srv_dst    integer   \n",
       "42   43        ct_dst_ltm    integer   \n",
       "43   44       ct_src_ ltm    integer   \n",
       "44   45  ct_src_dport_ltm    integer   \n",
       "45   46  ct_dst_sport_ltm    integer   \n",
       "46   47    ct_dst_src_ltm    integer   \n",
       "47   48        attack_cat    nominal   \n",
       "48   49             Label     binary   \n",
       "\n",
       "                                          Description  \n",
       "0                                   Source IP address  \n",
       "1                                  Source port number  \n",
       "2                              Destination IP address  \n",
       "3                             Destination port number  \n",
       "4                                Transaction protocol  \n",
       "5   Indicates to the state and its dependent proto...  \n",
       "6                               Record total duration  \n",
       "7            Source to destination transaction bytes   \n",
       "8             Destination to source transaction bytes  \n",
       "9           Source to destination time to live value   \n",
       "10           Destination to source time to live value  \n",
       "11           Source packets retransmitted or dropped   \n",
       "12       Destination packets retransmitted or dropped  \n",
       "13  http, ftp, smtp, ssh, dns, ftp-data ,irc  and ...  \n",
       "14                             Source bits per second  \n",
       "15                        Destination bits per second  \n",
       "16                Source to destination packet count   \n",
       "17                 Destination to source packet count  \n",
       "18              Source TCP window advertisement value  \n",
       "19         Destination TCP window advertisement value  \n",
       "20                    Source TCP base sequence number  \n",
       "21               Destination TCP base sequence number  \n",
       "22  Mean of the ?ow packet size transmitted by the...  \n",
       "23  Mean of the ?ow packet size transmitted by the...  \n",
       "24  Represents the pipelined depth into the connec...  \n",
       "25  Actual uncompressed content size of the data t...  \n",
       "26                               Source jitter (mSec)  \n",
       "27                          Destination jitter (mSec)  \n",
       "28                                  record start time  \n",
       "29                                   record last time  \n",
       "30             Source interpacket arrival time (mSec)  \n",
       "31        Destination interpacket arrival time (mSec)  \n",
       "32  TCP connection setup round-trip time, the sum ...  \n",
       "33  TCP connection setup time, the time between th...  \n",
       "34  TCP connection setup time, the time between th...  \n",
       "35  If source (1) and destination (3)IP addresses ...  \n",
       "36  No. for each state (6) according to specific r...  \n",
       "37  No. of flows that has methods such as Get and ...  \n",
       "38  If the ftp session is accessed by user and pas...  \n",
       "39     No of flows that has a command in ftp session.  \n",
       "40  No. of connections that contain the same servi...  \n",
       "41  No. of connections that contain the same servi...  \n",
       "42  No. of connections of the same destination add...  \n",
       "43  No. of connections of the same source address ...  \n",
       "44  No of connections of the same source address (...  \n",
       "45  No of connections of the same destination addr...  \n",
       "46  No of connections of the same source (1) and t...  \n",
       "47  The name of each attack category. In this data...  \n",
       "48              0 for normal and 1 for attack records  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28035e2",
   "metadata": {},
   "source": [
    "#### The features document lists 49 features, but the original dataset has 45 columns. Let us analyze the mismatch between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75fa9203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'service', 'sload', 'dload', 'spkts', 'dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'sjit', 'djit', 'stime', 'ltime', 'sintpkt', 'dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat', 'label']\n",
      "\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "features_list = list(features_df.Name.str.lower())\n",
    "print(features_list)\n",
    "print()\n",
    "print(len(features_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbe0325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = list(orig_train_df.columns.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa0e9673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'rate',\n",
       " 'sinpkt',\n",
       " 'dinpkt',\n",
       " 'smean',\n",
       " 'dmean',\n",
       " 'response_body_len',\n",
       " 'ct_src_ltm']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns in original data set, that are not in features doc\n",
    "df_extra_columns = [i for i in df_columns if i not in features_list]\n",
    "df_extra_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c37ac7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['srcip',\n",
       " 'sport',\n",
       " 'dstip',\n",
       " 'dsport',\n",
       " 'smeansz',\n",
       " 'dmeansz',\n",
       " 'res_bdy_len',\n",
       " 'stime',\n",
       " 'ltime',\n",
       " 'sintpkt',\n",
       " 'dintpkt',\n",
       " 'ct_src_ ltm']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features that are in features doc, but not in original data set\n",
    "features_list_extra = [j for j in features_list if j not in df_columns]\n",
    "features_list_extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1331b5b3",
   "metadata": {},
   "source": [
    "From above two output, the following columns differ only in how they are named. \n",
    "\n",
    "smeansz = smean <br>\n",
    "dmeansz = dmean <br>\n",
    "res_bdy_len = response_body_len <br>\n",
    "ct_src_ ltm = ct_src_ltm <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bfe7d8",
   "metadata": {},
   "source": [
    "\n",
    "### Analysis Results of features available in the dataset\n",
    "\n",
    "Features that are not in training set :\n",
    "\n",
    "1. srcip\n",
    "2. sport\n",
    "3. dstip\n",
    "4. dsport\n",
    "5. stime\n",
    "6. ltime\n",
    "\n",
    "49 feautures - 6 = 43\n",
    "\n",
    "Extra features that are in training set :\n",
    "\n",
    "1. id\n",
    "2. rate - This term is in the training/test sets but not defined anywhere (Refer the paper 1 below)\n",
    "\n",
    "43 + 2 = 45 total number of columns in the train and test files available.\n",
    "\n",
    "Features not needed for training:\n",
    "\n",
    "1. id => Serial Id of record which is not useful for training.\n",
    "2. attack_cat => Category of attack, which is not a training feature.\n",
    "3. label => Target label (\"Normal\"= 0, \"Attack\"= 1)\n",
    "\n",
    "45 - 3 = 42 useful features which can be used for training (Refer the paper 2 below)\n",
    "\n",
    "1. Development of an Efficient Network Intrusion Detection Model Using Extreme Gradient Boosting (XGBoost) on the UNSW-NB15 Dataset\n",
    "\n",
    "2. A Framework for Efficient Network Anomaly Intrusion Detection with Features Selection\n",
    "\n",
    "3. Benchmarking datasets for Anomaly-based Network Intrusion Detection: KDD CUP 99 alternatives\n",
    "\n",
    "4. UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b866ebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports']\n",
      "\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "final_features = [ i for i in list(orig_train_df.columns) if i not in ['id', 'attack_cat', 'label'] ]\n",
    "\n",
    "print(final_features)\n",
    "print()\n",
    "print(len(final_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46048e3",
   "metadata": {},
   "source": [
    "#### So, 42 features available in the original data set, for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c763c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>No.</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>proto</td>\n",
       "      <td>nominal</td>\n",
       "      <td>Transaction protocol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>state</td>\n",
       "      <td>nominal</td>\n",
       "      <td>Indicates to the state and its dependent proto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>dur</td>\n",
       "      <td>Float</td>\n",
       "      <td>Record total duration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>sbytes</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Source to destination transaction bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>dbytes</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Destination to source transaction bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>sttl</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Source to destination time to live value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>dttl</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Destination to source time to live value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>sloss</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Source packets retransmitted or dropped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>dloss</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Destination packets retransmitted or dropped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>service</td>\n",
       "      <td>nominal</td>\n",
       "      <td>http, ftp, smtp, ssh, dns, ftp-data ,irc  and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>sload</td>\n",
       "      <td>Float</td>\n",
       "      <td>Source bits per second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>dload</td>\n",
       "      <td>Float</td>\n",
       "      <td>Destination bits per second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>spkts</td>\n",
       "      <td>integer</td>\n",
       "      <td>Source to destination packet count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>dpkts</td>\n",
       "      <td>integer</td>\n",
       "      <td>Destination to source packet count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>swin</td>\n",
       "      <td>integer</td>\n",
       "      <td>Source TCP window advertisement value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>dwin</td>\n",
       "      <td>integer</td>\n",
       "      <td>Destination TCP window advertisement value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>stcpb</td>\n",
       "      <td>integer</td>\n",
       "      <td>Source TCP base sequence number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>dtcpb</td>\n",
       "      <td>integer</td>\n",
       "      <td>Destination TCP base sequence number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>smeansz</td>\n",
       "      <td>integer</td>\n",
       "      <td>Mean of the ?ow packet size transmitted by the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>dmeansz</td>\n",
       "      <td>integer</td>\n",
       "      <td>Mean of the ?ow packet size transmitted by the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>trans_depth</td>\n",
       "      <td>integer</td>\n",
       "      <td>Represents the pipelined depth into the connec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>res_bdy_len</td>\n",
       "      <td>integer</td>\n",
       "      <td>Actual uncompressed content size of the data t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>sjit</td>\n",
       "      <td>Float</td>\n",
       "      <td>Source jitter (mSec)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>djit</td>\n",
       "      <td>Float</td>\n",
       "      <td>Destination jitter (mSec)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>sintpkt</td>\n",
       "      <td>Float</td>\n",
       "      <td>Source interpacket arrival time (mSec)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>dintpkt</td>\n",
       "      <td>Float</td>\n",
       "      <td>Destination interpacket arrival time (mSec)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>tcprtt</td>\n",
       "      <td>Float</td>\n",
       "      <td>TCP connection setup round-trip time, the sum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>synack</td>\n",
       "      <td>Float</td>\n",
       "      <td>TCP connection setup time, the time between th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>ackdat</td>\n",
       "      <td>Float</td>\n",
       "      <td>TCP connection setup time, the time between th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>is_sm_ips_ports</td>\n",
       "      <td>Binary</td>\n",
       "      <td>If source (1) and destination (3)IP addresses ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>ct_state_ttl</td>\n",
       "      <td>Integer</td>\n",
       "      <td>No. for each state (6) according to specific r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>ct_flw_http_mthd</td>\n",
       "      <td>Integer</td>\n",
       "      <td>No. of flows that has methods such as Get and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>is_ftp_login</td>\n",
       "      <td>Binary</td>\n",
       "      <td>If the ftp session is accessed by user and pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>ct_ftp_cmd</td>\n",
       "      <td>integer</td>\n",
       "      <td>No of flows that has a command in ftp session.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>ct_srv_src</td>\n",
       "      <td>integer</td>\n",
       "      <td>No. of connections that contain the same servi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>ct_srv_dst</td>\n",
       "      <td>integer</td>\n",
       "      <td>No. of connections that contain the same servi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>ct_dst_ltm</td>\n",
       "      <td>integer</td>\n",
       "      <td>No. of connections of the same destination add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>ct_src_ ltm</td>\n",
       "      <td>integer</td>\n",
       "      <td>No. of connections of the same source address ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "      <td>ct_src_dport_ltm</td>\n",
       "      <td>integer</td>\n",
       "      <td>No of connections of the same source address (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>ct_dst_sport_ltm</td>\n",
       "      <td>integer</td>\n",
       "      <td>No of connections of the same destination addr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "      <td>ct_dst_src_ltm</td>\n",
       "      <td>integer</td>\n",
       "      <td>No of connections of the same source (1) and t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  No.              Name    Type   \\\n",
       "0       4    5             proto  nominal   \n",
       "1       5    6             state  nominal   \n",
       "2       6    7               dur    Float   \n",
       "3       7    8            sbytes  Integer   \n",
       "4       8    9            dbytes  Integer   \n",
       "5       9   10              sttl  Integer   \n",
       "6      10   11              dttl  Integer   \n",
       "7      11   12             sloss  Integer   \n",
       "8      12   13             dloss  Integer   \n",
       "9      13   14           service  nominal   \n",
       "10     14   15             sload    Float   \n",
       "11     15   16             dload    Float   \n",
       "12     16   17             spkts  integer   \n",
       "13     17   18             dpkts  integer   \n",
       "14     18   19              swin  integer   \n",
       "15     19   20              dwin  integer   \n",
       "16     20   21             stcpb  integer   \n",
       "17     21   22             dtcpb  integer   \n",
       "18     22   23           smeansz  integer   \n",
       "19     23   24           dmeansz  integer   \n",
       "20     24   25       trans_depth  integer   \n",
       "21     25   26       res_bdy_len  integer   \n",
       "22     26   27              sjit    Float   \n",
       "23     27   28              djit    Float   \n",
       "24     30   31           sintpkt    Float   \n",
       "25     31   32           dintpkt    Float   \n",
       "26     32   33            tcprtt    Float   \n",
       "27     33   34            synack    Float   \n",
       "28     34   35            ackdat    Float   \n",
       "29     35   36   is_sm_ips_ports   Binary   \n",
       "30     36   37      ct_state_ttl  Integer   \n",
       "31     37   38  ct_flw_http_mthd  Integer   \n",
       "32     38   39      is_ftp_login   Binary   \n",
       "33     39   40        ct_ftp_cmd  integer   \n",
       "34     40   41        ct_srv_src  integer   \n",
       "35     41   42        ct_srv_dst  integer   \n",
       "36     42   43        ct_dst_ltm  integer   \n",
       "37     43   44       ct_src_ ltm  integer   \n",
       "38     44   45  ct_src_dport_ltm  integer   \n",
       "39     45   46  ct_dst_sport_ltm  integer   \n",
       "40     46   47    ct_dst_src_ltm  integer   \n",
       "\n",
       "                                          Description  \n",
       "0                                Transaction protocol  \n",
       "1   Indicates to the state and its dependent proto...  \n",
       "2                               Record total duration  \n",
       "3            Source to destination transaction bytes   \n",
       "4             Destination to source transaction bytes  \n",
       "5           Source to destination time to live value   \n",
       "6            Destination to source time to live value  \n",
       "7            Source packets retransmitted or dropped   \n",
       "8        Destination packets retransmitted or dropped  \n",
       "9   http, ftp, smtp, ssh, dns, ftp-data ,irc  and ...  \n",
       "10                             Source bits per second  \n",
       "11                        Destination bits per second  \n",
       "12                Source to destination packet count   \n",
       "13                 Destination to source packet count  \n",
       "14              Source TCP window advertisement value  \n",
       "15         Destination TCP window advertisement value  \n",
       "16                    Source TCP base sequence number  \n",
       "17               Destination TCP base sequence number  \n",
       "18  Mean of the ?ow packet size transmitted by the...  \n",
       "19  Mean of the ?ow packet size transmitted by the...  \n",
       "20  Represents the pipelined depth into the connec...  \n",
       "21  Actual uncompressed content size of the data t...  \n",
       "22                               Source jitter (mSec)  \n",
       "23                          Destination jitter (mSec)  \n",
       "24             Source interpacket arrival time (mSec)  \n",
       "25        Destination interpacket arrival time (mSec)  \n",
       "26  TCP connection setup round-trip time, the sum ...  \n",
       "27  TCP connection setup time, the time between th...  \n",
       "28  TCP connection setup time, the time between th...  \n",
       "29  If source (1) and destination (3)IP addresses ...  \n",
       "30  No. for each state (6) according to specific r...  \n",
       "31  No. of flows that has methods such as Get and ...  \n",
       "32  If the ftp session is accessed by user and pas...  \n",
       "33     No of flows that has a command in ftp session.  \n",
       "34  No. of connections that contain the same servi...  \n",
       "35  No. of connections that contain the same servi...  \n",
       "36  No. of connections of the same destination add...  \n",
       "37  No. of connections of the same source address ...  \n",
       "38  No of connections of the same source address (...  \n",
       "39  No of connections of the same destination addr...  \n",
       "40  No of connections of the same source (1) and t...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description of the features (except for \"rate\" for which the description is not available )\n",
    "features_df.Name = features_df.Name.str.lower()\n",
    "features_df.loc[~features_df['Name'].isin(['srcip', 'sport', 'dstip', 'dsport', 'attack_cat', 'label', 'stime', 'ltime'])].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7523b1c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 2. Basic Pre-processing - Missing Values, Outliers Treatment etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b242497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to list numeric vars and categorical vars\n",
    "def sep_num_cat(df):\n",
    "    numeric_vars = list(df.select_dtypes(include=['float64', 'int64', 'uint8', 'int32']).columns)\n",
    "    categorical_vars = list(df.select_dtypes(include=['object', 'category']).columns)\n",
    "    return numeric_vars, categorical_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99ecf266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175341 entries, 0 to 175340\n",
      "Data columns (total 45 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   id                 175341 non-null  int64  \n",
      " 1   dur                175341 non-null  float64\n",
      " 2   proto              175341 non-null  object \n",
      " 3   service            175341 non-null  object \n",
      " 4   state              175341 non-null  object \n",
      " 5   spkts              175341 non-null  int64  \n",
      " 6   dpkts              175341 non-null  int64  \n",
      " 7   sbytes             175341 non-null  int64  \n",
      " 8   dbytes             175341 non-null  int64  \n",
      " 9   rate               175341 non-null  float64\n",
      " 10  sttl               175341 non-null  int64  \n",
      " 11  dttl               175341 non-null  int64  \n",
      " 12  sload              175341 non-null  float64\n",
      " 13  dload              175341 non-null  float64\n",
      " 14  sloss              175341 non-null  int64  \n",
      " 15  dloss              175341 non-null  int64  \n",
      " 16  sinpkt             175341 non-null  float64\n",
      " 17  dinpkt             175341 non-null  float64\n",
      " 18  sjit               175341 non-null  float64\n",
      " 19  djit               175341 non-null  float64\n",
      " 20  swin               175341 non-null  int64  \n",
      " 21  stcpb              175341 non-null  int64  \n",
      " 22  dtcpb              175341 non-null  int64  \n",
      " 23  dwin               175341 non-null  int64  \n",
      " 24  tcprtt             175341 non-null  float64\n",
      " 25  synack             175341 non-null  float64\n",
      " 26  ackdat             175341 non-null  float64\n",
      " 27  smean              175341 non-null  int64  \n",
      " 28  dmean              175341 non-null  int64  \n",
      " 29  trans_depth        175341 non-null  int64  \n",
      " 30  response_body_len  175341 non-null  int64  \n",
      " 31  ct_srv_src         175341 non-null  int64  \n",
      " 32  ct_state_ttl       175341 non-null  int64  \n",
      " 33  ct_dst_ltm         175341 non-null  int64  \n",
      " 34  ct_src_dport_ltm   175341 non-null  int64  \n",
      " 35  ct_dst_sport_ltm   175341 non-null  int64  \n",
      " 36  ct_dst_src_ltm     175341 non-null  int64  \n",
      " 37  is_ftp_login       175341 non-null  int64  \n",
      " 38  ct_ftp_cmd         175341 non-null  int64  \n",
      " 39  ct_flw_http_mthd   175341 non-null  int64  \n",
      " 40  ct_src_ltm         175341 non-null  int64  \n",
      " 41  ct_srv_dst         175341 non-null  int64  \n",
      " 42  is_sm_ips_ports    175341 non-null  int64  \n",
      " 43  attack_cat         175341 non-null  object \n",
      " 44  label              175341 non-null  int64  \n",
      "dtypes: float64(11), int64(30), object(4)\n",
      "memory usage: 60.2+ MB\n"
     ]
    }
   ],
   "source": [
    "orig_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "271fe739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82332 entries, 0 to 82331\n",
      "Data columns (total 45 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 82332 non-null  int64  \n",
      " 1   dur                82332 non-null  float64\n",
      " 2   proto              82332 non-null  object \n",
      " 3   service            82332 non-null  object \n",
      " 4   state              82332 non-null  object \n",
      " 5   spkts              82332 non-null  int64  \n",
      " 6   dpkts              82332 non-null  int64  \n",
      " 7   sbytes             82332 non-null  int64  \n",
      " 8   dbytes             82332 non-null  int64  \n",
      " 9   rate               82332 non-null  float64\n",
      " 10  sttl               82332 non-null  int64  \n",
      " 11  dttl               82332 non-null  int64  \n",
      " 12  sload              82332 non-null  float64\n",
      " 13  dload              82332 non-null  float64\n",
      " 14  sloss              82332 non-null  int64  \n",
      " 15  dloss              82332 non-null  int64  \n",
      " 16  sinpkt             82332 non-null  float64\n",
      " 17  dinpkt             82332 non-null  float64\n",
      " 18  sjit               82332 non-null  float64\n",
      " 19  djit               82332 non-null  float64\n",
      " 20  swin               82332 non-null  int64  \n",
      " 21  stcpb              82332 non-null  int64  \n",
      " 22  dtcpb              82332 non-null  int64  \n",
      " 23  dwin               82332 non-null  int64  \n",
      " 24  tcprtt             82332 non-null  float64\n",
      " 25  synack             82332 non-null  float64\n",
      " 26  ackdat             82332 non-null  float64\n",
      " 27  smean              82332 non-null  int64  \n",
      " 28  dmean              82332 non-null  int64  \n",
      " 29  trans_depth        82332 non-null  int64  \n",
      " 30  response_body_len  82332 non-null  int64  \n",
      " 31  ct_srv_src         82332 non-null  int64  \n",
      " 32  ct_state_ttl       82332 non-null  int64  \n",
      " 33  ct_dst_ltm         82332 non-null  int64  \n",
      " 34  ct_src_dport_ltm   82332 non-null  int64  \n",
      " 35  ct_dst_sport_ltm   82332 non-null  int64  \n",
      " 36  ct_dst_src_ltm     82332 non-null  int64  \n",
      " 37  is_ftp_login       82332 non-null  int64  \n",
      " 38  ct_ftp_cmd         82332 non-null  int64  \n",
      " 39  ct_flw_http_mthd   82332 non-null  int64  \n",
      " 40  ct_src_ltm         82332 non-null  int64  \n",
      " 41  ct_srv_dst         82332 non-null  int64  \n",
      " 42  is_sm_ips_ports    82332 non-null  int64  \n",
      " 43  attack_cat         82332 non-null  object \n",
      " 44  label              82332 non-null  int64  \n",
      "dtypes: float64(11), int64(30), object(4)\n",
      "memory usage: 28.3+ MB\n"
     ]
    }
   ],
   "source": [
    "orig_test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5d8590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175341, 45)\n",
      "(82332, 45)\n"
     ]
    }
   ],
   "source": [
    "print(orig_train_df.shape)\n",
    "print(orig_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c1546bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = orig_train_df.columns\n",
    "test_columns = orig_test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8110893e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if columns in train and test files are same\n",
    "train_columns == test_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a074f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['proto', 'service', 'state', 'attack_cat']\n",
      "['proto', 'service', 'state', 'attack_cat']\n"
     ]
    }
   ],
   "source": [
    "train_num_vars, train_cat_vars = sep_num_cat(orig_train_df)\n",
    "test_num_vars, test_cat_vars = sep_num_cat(orig_test_df)\n",
    "print(train_cat_vars)\n",
    "print(test_cat_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba585d",
   "metadata": {},
   "source": [
    "#### Four categorical variables are seen in the data set - proto, service, state and atack_cat.\n",
    "Let's analyse them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01845e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train_df.proto = orig_train_df.proto.str.lower()\n",
    "orig_train_df.service = orig_train_df.service.str.lower()\n",
    "orig_train_df.state = orig_train_df.state.str.lower()\n",
    "orig_train_df.attack_cat = orig_train_df.attack_cat.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52ad109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_test_df.proto = orig_test_df.proto.str.lower()\n",
    "orig_test_df.service = orig_test_df.service.str.lower()\n",
    "orig_test_df.state = orig_test_df.state.str.lower()\n",
    "orig_test_df.attack_cat = orig_test_df.attack_cat.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42321fd2",
   "metadata": {},
   "source": [
    "#### We compare the values of 'state' variable between the train and test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16399bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int    82275\n",
       "fin    77825\n",
       "con    13152\n",
       "req     1991\n",
       "rst       83\n",
       "eco       12\n",
       "par        1\n",
       "no         1\n",
       "urn        1\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state variable\n",
    "orig_train_df.state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cbea3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fin    39339\n",
       "int    34163\n",
       "con     6982\n",
       "req     1842\n",
       "acc        4\n",
       "clo        1\n",
       "rst        1\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_test_df.state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36b9b1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'req', 'con', 'eco', 'acc', 'rst', 'no', 'fin', 'clo', 'int', 'par', 'urn'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 11 values which are common between the two files\n",
    "train_state_set = set(orig_train_df.state.values)\n",
    "test_state_set = set(orig_test_df.state.values)\n",
    "print(train_state_set | test_state_set)\n",
    "len(train_state_set | test_state_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcd7f615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc', 'clo', 'eco', 'no', 'par', 'urn'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some values are present in train set, but not in test set and vice versa.\n",
    "train_state_set ^ test_state_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec1805",
   "metadata": {},
   "source": [
    "#### We compare the values of 'proto' variable between the train and test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7729d5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tcp     79946\n",
       "udp     63283\n",
       "unas    12084\n",
       "arp      2859\n",
       "ospf     2595\n",
       "        ...  \n",
       "egp        98\n",
       "hmp        98\n",
       "igmp       18\n",
       "icmp       15\n",
       "rtp         1\n",
       "Name: proto, Length: 133, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_train_df.proto.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9957ab16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tcp           43095\n",
       "udp           29418\n",
       "unas           3515\n",
       "arp             987\n",
       "ospf            676\n",
       "              ...  \n",
       "br-sat-mon       32\n",
       "tcf              32\n",
       "isis             32\n",
       "ib               31\n",
       "igmp             30\n",
       "Name: proto, Length: 131, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_test_df.proto.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1572e918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 133 values which are common between the two files\n",
    "train_proto_set = set(orig_train_df.proto.values)\n",
    "test_proto_set = set(orig_test_df.proto.values)\n",
    "len(train_proto_set | test_proto_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b14b43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'icmp', 'rtp'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two values of 'proto' are in train set, but not in test set.\n",
    "train_proto_set - test_proto_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62b30a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All values of 'proto' in test set are in train set as well\n",
    "test_proto_set - train_proto_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3b80a",
   "metadata": {},
   "source": [
    "#### We compare the values of 'service' variable between the train and test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58b58734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-           94168\n",
       "dns         47294\n",
       "http        18724\n",
       "smtp         5058\n",
       "ftp-data     3995\n",
       "ftp          3428\n",
       "ssh          1302\n",
       "pop3         1105\n",
       "dhcp           94\n",
       "snmp           80\n",
       "ssl            56\n",
       "irc            25\n",
       "radius         12\n",
       "Name: service, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_train_df.service.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7418b417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-           47153\n",
       "dns         21367\n",
       "http         8287\n",
       "smtp         1851\n",
       "ftp          1552\n",
       "ftp-data     1396\n",
       "pop3          423\n",
       "ssh           204\n",
       "ssl            30\n",
       "snmp           29\n",
       "dhcp           26\n",
       "radius          9\n",
       "irc             5\n",
       "Name: service, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_test_df.service.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9d7c64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 13 values which are common between the two files\n",
    "train_service_set = set(orig_train_df.service.values)\n",
    "test_service_set = set(orig_test_df.service.values)\n",
    "len(train_service_set | test_service_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7000905f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All values are there both in train set and testing set\n",
    "train_service_set - test_service_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c68a8c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_service_set - train_service_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0617c7",
   "metadata": {},
   "source": [
    "#### As we saw above, some categorical features have different values in train and test set files. \n",
    "\n",
    "If we have these two files separately ( for training and testing respectively), it would result in a\n",
    "different number of final features between both sets, when we extract one-hot encoded features from these categorical variables.\n",
    "So let us merge both files (train and test in the original dataset) and create a full df from which we can later split train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b045761",
   "metadata": {},
   "source": [
    "#### Combine both files in to one. We will use this whole dataset to split in to train and test ourselves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb3887e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save a backup of the original train and test df\n",
    "orig_train_df_copy = orig_train_df.copy()\n",
    "orig_test_df_copy = orig_test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "caff70df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257673, 45)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "full_df = pd.concat([orig_train_df, orig_test_df], axis=0)\n",
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "50a689b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffle the full dataframe\n",
    "full_df = full_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e50caea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122638</td>\n",
       "      <td>0.000</td>\n",
       "      <td>udp</td>\n",
       "      <td>dns</td>\n",
       "      <td>int</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>200000.005</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>generic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>udp</td>\n",
       "      <td>dns</td>\n",
       "      <td>int</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>333333.322</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>generic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69949</td>\n",
       "      <td>1.168</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>con</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1012</td>\n",
       "      <td>86</td>\n",
       "      <td>5.995</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45922</td>\n",
       "      <td>0.486</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>fin</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>798</td>\n",
       "      <td>1568</td>\n",
       "      <td>34.945</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>exploits</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31409</td>\n",
       "      <td>0.072</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>con</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>568</td>\n",
       "      <td>312</td>\n",
       "      <td>97.024</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>92651</td>\n",
       "      <td>0.780</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>fin</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>12598</td>\n",
       "      <td>268</td>\n",
       "      <td>29.503</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66840</td>\n",
       "      <td>0.995</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>fin</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>590</td>\n",
       "      <td>268</td>\n",
       "      <td>15.076</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>123192</td>\n",
       "      <td>0.000</td>\n",
       "      <td>udp</td>\n",
       "      <td>dns</td>\n",
       "      <td>int</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>111111.107</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>generic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9755</td>\n",
       "      <td>0.737</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>fin</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>874</td>\n",
       "      <td>268</td>\n",
       "      <td>20.363</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>reconnaissance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>169314</td>\n",
       "      <td>0.000</td>\n",
       "      <td>udp</td>\n",
       "      <td>dns</td>\n",
       "      <td>int</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>125000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>generic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   dur proto service state  spkts  dpkts  sbytes  dbytes       rate  \\\n",
       "0  122638 0.000   udp     dns   int      2      0     114       0 200000.005   \n",
       "1  137119 0.000   udp     dns   int      2      0     114       0 333333.322   \n",
       "2   69949 1.168   tcp       -   con      6      2    1012      86      5.995   \n",
       "3   45922 0.486   tcp    http   fin     10      8     798    1568     34.945   \n",
       "4   31409 0.072   udp       -   con      4      4     568     312     97.024   \n",
       "5   92651 0.780   tcp       -   fin     18      6   12598     268     29.503   \n",
       "6   66840 0.995   tcp       -   fin     10      6     590     268     15.076   \n",
       "7  123192 0.000   udp     dns   int      2      0     114       0 111111.107   \n",
       "8    9755 0.737   tcp    http   fin     10      6     874     268     20.363   \n",
       "9  169314 0.000   udp     dns   int      2      0     114       0 125000.000   \n",
       "\n",
       "   ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  ct_ftp_cmd  \\\n",
       "0  ...                 9              13             0           0   \n",
       "1  ...                16              16             0           0   \n",
       "2  ...                 1               2             0           0   \n",
       "3  ...                 1               1             0           0   \n",
       "4  ...                 1               5             0           0   \n",
       "5  ...                 1               1             0           0   \n",
       "6  ...                 1               3             0           0   \n",
       "7  ...                18              18             0           0   \n",
       "8  ...                 1               1             0           0   \n",
       "9  ...                17              25             0           0   \n",
       "\n",
       "   ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports      attack_cat  \\\n",
       "0                 0           9          13                0         generic   \n",
       "1                 0          17          16                0         generic   \n",
       "2                 0           2           2                0          normal   \n",
       "3                 1           1           1                0        exploits   \n",
       "4                 0           2           8                0          normal   \n",
       "5                 0           2           1                0             dos   \n",
       "6                 0           2           6                0          normal   \n",
       "7                 0          18          18                0         generic   \n",
       "8                 1           1           1                0  reconnaissance   \n",
       "9                 0          17          25                0         generic   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  \n",
       "5      1  \n",
       "6      0  \n",
       "7      1  \n",
       "8      1  \n",
       "9      1  \n",
       "\n",
       "[10 rows x 45 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "546d7ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>udp</td>\n",
       "      <td>dns</td>\n",
       "      <td>int</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>200000.005</td>\n",
       "      <td>254</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>udp</td>\n",
       "      <td>dns</td>\n",
       "      <td>int</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>333333.322</td>\n",
       "      <td>254</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.168</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>con</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1012</td>\n",
       "      <td>86</td>\n",
       "      <td>5.995</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.486</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>fin</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>798</td>\n",
       "      <td>1568</td>\n",
       "      <td>34.945</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>con</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>568</td>\n",
       "      <td>312</td>\n",
       "      <td>97.024</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dur proto service state  spkts  dpkts  sbytes  dbytes       rate  sttl  \\\n",
       "0 0.000   udp     dns   int      2      0     114       0 200000.005   254   \n",
       "1 0.000   udp     dns   int      2      0     114       0 333333.322   254   \n",
       "2 1.168   tcp       -   con      6      2    1012      86      5.995    62   \n",
       "3 0.486   tcp    http   fin     10      8     798    1568     34.945    62   \n",
       "4 0.072   udp       -   con      4      4     568     312     97.024    31   \n",
       "\n",
       "   ...  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  \\\n",
       "0  ...                 9                 9              13             0   \n",
       "1  ...                16                16              16             0   \n",
       "2  ...                 1                 1               2             0   \n",
       "3  ...                 1                 1               1             0   \n",
       "4  ...                 1                 1               5             0   \n",
       "\n",
       "   ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  \\\n",
       "0           0                 0           9          13                0   \n",
       "1           0                 0          17          16                0   \n",
       "2           0                 0           2           2                0   \n",
       "3           0                 1           1           1                0   \n",
       "4           0                 0           2           8                0   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ## Drop Unnecessary variables - id, attack_cat\n",
    "full_df = full_df.drop(['id', 'attack_cat'], axis=1)\n",
    "full_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fab2c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports', 'label']\n",
      "\n",
      "['proto', 'service', 'state']\n"
     ]
    }
   ],
   "source": [
    "# Examine the numerical and categorical features\n",
    "numeric_vars, categorical_vars = sep_num_cat(full_df)\n",
    "print(numeric_vars)\n",
    "print()\n",
    "print(categorical_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785ec0d",
   "metadata": {},
   "source": [
    "## Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "952bd39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, False), (1, False), (2, False), (3, False), (4, False), (5, False), (6, False), (7, False), (8, False), (9, False), (10, False), (11, False), (12, False), (13, False), (14, False), (15, False), (16, False), (17, False), (18, False), (19, False), (20, False), (21, False), (22, False), (23, False), (24, False), (25, False), (26, False), (27, False), (28, False), (29, False), (30, False), (31, False), (32, False), (33, False), (34, False), (35, False), (36, False), (37, False), (38, False), (39, False), (40, False), (41, False), (42, False)]\n"
     ]
    }
   ],
   "source": [
    "print(list(enumerate(full_df.isnull().any())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e8c49fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.columns[full_df.isnull().any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f45e66",
   "metadata": {},
   "source": [
    "#### No missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a567fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19154185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     257673\n",
       "unique        13\n",
       "top            -\n",
       "freq      141321\n",
       "Name: service, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at categorical features\n",
    "\n",
    "#service\n",
    "full_df.service.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "11bb41b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-           141321\n",
       "dns          68661\n",
       "http         27011\n",
       "smtp          6909\n",
       "ftp-data      5391\n",
       "ftp           4980\n",
       "pop3          1528\n",
       "ssh           1506\n",
       "dhcp           120\n",
       "snmp           109\n",
       "ssl             86\n",
       "irc             30\n",
       "radius          21\n",
       "Name: service, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.service.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53d32244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert '-' value in to 'unpopular' according to feature description file\n",
    "full_df['service'].loc[full_df.service == '-'] = 'unpopular'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6fa8ed42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unpopular    141321\n",
       "dns           68661\n",
       "http          27011\n",
       "smtp           6909\n",
       "ftp-data       5391\n",
       "ftp            4980\n",
       "pop3           1528\n",
       "ssh            1506\n",
       "dhcp            120\n",
       "snmp            109\n",
       "ssl              86\n",
       "irc              30\n",
       "radius           21\n",
       "Name: service, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.service.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a400a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tcp     123041\n",
       "udp      92701\n",
       "unas     15599\n",
       "arp       3846\n",
       "ospf      3271\n",
       "         ...  \n",
       "egp        131\n",
       "rdp        131\n",
       "igmp        48\n",
       "icmp        15\n",
       "rtp          1\n",
       "Name: proto, Length: 133, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proto\n",
    "full_df.proto.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed196606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fin    117164\n",
       "int    116438\n",
       "con     20134\n",
       "req      3833\n",
       "rst        84\n",
       "eco        12\n",
       "acc         4\n",
       "par         1\n",
       "no          1\n",
       "clo         1\n",
       "urn         1\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state\n",
    "full_df.state.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08d377",
   "metadata": {},
   "source": [
    "## Analyse and Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1c1529c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dur</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sload</th>\n",
       "      <th>dload</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>...</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "      <td>257673.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.247</td>\n",
       "      <td>19.777</td>\n",
       "      <td>18.515</td>\n",
       "      <td>8572.952</td>\n",
       "      <td>14387.288</td>\n",
       "      <td>91253.912</td>\n",
       "      <td>180.001</td>\n",
       "      <td>84.755</td>\n",
       "      <td>70608691.228</td>\n",
       "      <td>658214.282</td>\n",
       "      <td>...</td>\n",
       "      <td>5.238</td>\n",
       "      <td>4.033</td>\n",
       "      <td>8.323</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.132</td>\n",
       "      <td>6.800</td>\n",
       "      <td>9.121</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.974</td>\n",
       "      <td>135.947</td>\n",
       "      <td>111.986</td>\n",
       "      <td>173773.881</td>\n",
       "      <td>146199.282</td>\n",
       "      <td>160344.637</td>\n",
       "      <td>102.488</td>\n",
       "      <td>112.762</td>\n",
       "      <td>185731252.842</td>\n",
       "      <td>2412372.138</td>\n",
       "      <td>...</td>\n",
       "      <td>8.161</td>\n",
       "      <td>5.832</td>\n",
       "      <td>11.121</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.682</td>\n",
       "      <td>8.396</td>\n",
       "      <td>10.875</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>114.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30.789</td>\n",
       "      <td>62.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12318.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.004</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>528.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>2955.665</td>\n",
       "      <td>254.000</td>\n",
       "      <td>29.000</td>\n",
       "      <td>743942.312</td>\n",
       "      <td>1747.441</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.686</td>\n",
       "      <td>12.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>1362.000</td>\n",
       "      <td>1064.000</td>\n",
       "      <td>125000.000</td>\n",
       "      <td>254.000</td>\n",
       "      <td>252.000</td>\n",
       "      <td>80000000.000</td>\n",
       "      <td>22105.385</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>60.000</td>\n",
       "      <td>10646.000</td>\n",
       "      <td>11018.000</td>\n",
       "      <td>14355774.000</td>\n",
       "      <td>14657531.000</td>\n",
       "      <td>1000000.003</td>\n",
       "      <td>255.000</td>\n",
       "      <td>254.000</td>\n",
       "      <td>5988000256.000</td>\n",
       "      <td>22422730.000</td>\n",
       "      <td>...</td>\n",
       "      <td>59.000</td>\n",
       "      <td>46.000</td>\n",
       "      <td>65.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>62.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             dur      spkts      dpkts       sbytes       dbytes        rate  \\\n",
       "count 257673.000 257673.000 257673.000   257673.000   257673.000  257673.000   \n",
       "mean       1.247     19.777     18.515     8572.952    14387.288   91253.912   \n",
       "std        5.974    135.947    111.986   173773.881   146199.282  160344.637   \n",
       "min        0.000      1.000      0.000       24.000        0.000       0.000   \n",
       "25%        0.000      2.000      0.000      114.000        0.000      30.789   \n",
       "50%        0.004      4.000      2.000      528.000      178.000    2955.665   \n",
       "75%        0.686     12.000     10.000     1362.000     1064.000  125000.000   \n",
       "max       60.000  10646.000  11018.000 14355774.000 14657531.000 1000000.003   \n",
       "\n",
       "            sttl       dttl          sload        dload  ...  \\\n",
       "count 257673.000 257673.000     257673.000   257673.000  ...   \n",
       "mean     180.001     84.755   70608691.228   658214.282  ...   \n",
       "std      102.488    112.762  185731252.842  2412372.138  ...   \n",
       "min        0.000      0.000          0.000        0.000  ...   \n",
       "25%       62.000      0.000      12318.005        0.000  ...   \n",
       "50%      254.000     29.000     743942.312     1747.441  ...   \n",
       "75%      254.000    252.000   80000000.000    22105.385  ...   \n",
       "max      255.000    254.000 5988000256.000 22422730.000  ...   \n",
       "\n",
       "       ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  \\\n",
       "count        257673.000        257673.000      257673.000    257673.000   \n",
       "mean              5.238             4.033           8.323         0.013   \n",
       "std               8.161             5.832          11.121         0.116   \n",
       "min               1.000             1.000           1.000         0.000   \n",
       "25%               1.000             1.000           1.000         0.000   \n",
       "50%               1.000             1.000           3.000         0.000   \n",
       "75%               4.000             3.000           8.000         0.000   \n",
       "max              59.000            46.000          65.000         4.000   \n",
       "\n",
       "       ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  \\\n",
       "count  257673.000        257673.000  257673.000  257673.000       257673.000   \n",
       "mean        0.013             0.132       6.800       9.121            0.014   \n",
       "std         0.116             0.682       8.396      10.875            0.119   \n",
       "min         0.000             0.000       1.000       1.000            0.000   \n",
       "25%         0.000             0.000       2.000       2.000            0.000   \n",
       "50%         0.000             0.000       3.000       4.000            0.000   \n",
       "75%         0.000             0.000       8.000      11.000            0.000   \n",
       "max         4.000            30.000      60.000      62.000            1.000   \n",
       "\n",
       "           label  \n",
       "count 257673.000  \n",
       "mean       0.639  \n",
       "std        0.480  \n",
       "min        0.000  \n",
       "25%        0.000  \n",
       "50%        1.000  \n",
       "75%        1.000  \n",
       "max        1.000  \n",
       "\n",
       "[8 rows x 40 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df[numeric_vars].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "921d715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   257673.000\n",
      "mean         1.247\n",
      "std          5.974\n",
      "min          0.000\n",
      "25%          0.000\n",
      "50%          0.004\n",
      "75%          0.686\n",
      "max         60.000\n",
      "Name: dur, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "univariate(full_df['dur'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "22e73f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0000000e+00 4.2850000e-03 6.8577700e-01 1.0857574e+00 1.4704954e+00\n",
      " 2.8114142e+00 1.1758444e+01 3.7853725e+01]\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(full_df['dur'], [1, 50, 75, 85, 90, 95, 98, 99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11ccfcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   257673.000\n",
      "mean        19.777\n",
      "std        135.947\n",
      "min          1.000\n",
      "25%          2.000\n",
      "50%          4.000\n",
      "75%         12.000\n",
      "max      10646.000\n",
      "Name: spkts, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "univariate(full_df['spkts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f06e594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.   4.  12.  22.  42.  62. 122. 230.]\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(full_df['spkts'], [1, 50, 75, 85, 90, 95, 98, 99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97366002",
   "metadata": {},
   "source": [
    "#### The data seems to be right skewed with extreme outliers on the higher side of the data.\n",
    "#### Let us remove the higher side outliers by fixing the cutoff threshold as 90 percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8be45d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4704954000000017"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc_90 = np.percentile(full_df['dur'], 90)\n",
    "perc_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5713e2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231905, 43)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_wo_outliers = full_df.loc[full_df['dur'] < perc_90]\n",
    "full_df_wo_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "47367cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   231905.000\n",
      "mean         0.242\n",
      "std          0.388\n",
      "min          0.000\n",
      "25%          0.000\n",
      "50%          0.001\n",
      "75%          0.434\n",
      "max          1.470\n",
      "Name: dur, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "univariate(full_df_wo_outliers['dur'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c6897566",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_orig = full_df.copy()\n",
    "full_df = full_df_wo_outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d275793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88297b65",
   "metadata": {},
   "source": [
    "# 3. Data Preparation For Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d7b48",
   "metadata": {},
   "source": [
    "## 3.1 Check class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33b238ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    146771\n",
       "0     85134\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "full_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bcf5a9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Temporarily convert 'label' to category type, to aid visualization\n",
    "full_df.label = full_df.label.astype('category')\n",
    "univariate(full_df['label'], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6653322",
   "metadata": {},
   "source": [
    "#### We have a class imbalance in the dataset. The attack traffic is majority with 146771 records and normal traffic has only 85134 records. We have to address this class imbalance accordingly during modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d674c716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check if any feature has only values as 0\n",
    "num_vars, cat_vars = sep_num_cat(full_df)\n",
    "\n",
    "zero_cols = [col for col in num_vars if full_df[col].sum()==0]\n",
    "zero_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "078c4284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['proto', 'service', 'state', 'label']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a2481708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert label dtype back to int\n",
    "full_df.label = full_df.label.astype('int')\n",
    "full_df.label.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3298b746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231905, 43)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e5ec7",
   "metadata": {},
   "source": [
    "## 3.2 Transformation (One-hot encoding) of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2ef04baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['proto', 'service', 'state']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_vars, cat_vars = sep_num_cat(full_df)\n",
    "cat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d7b52dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ospf', 'igmp', 'larp', 'ipip', 'isis', 'a/n', 'etherip', 'nsfnet-igp', 'pipe', 'crudp', 'st2', 'l2tp', 'br-sat-mon', 'pvp', 'nvp', 'iso-tp4', 'irtp', 'emcon', 'il', 'prm', 'visa', 'encap', 'idrp', 'tlsp', 'kryptolan', 'sps', 'micp', 'mfe-nsp', 'smp', 'ipnip', 'bbn-rcc', 'any', 'argus', 'cphb', 'vmtp', 'pri-enc', 'bna', 'cbt', 'skip', 'sat-expak', 'narp', 'tcf', 'egp', 'aris', 'qnx', 'chaos', 'ipcv', 'vrrp', 'trunk-1', 'ptp', 'sun-nd', 'swipe', 'fire', 'sat-mon', 'gmtp', 'aes-sp3-d', 'merit-inp', 'ggp', 'ib', 'ipv6-opts', 'udp', 'rsvp', 'idpr-cmtp', 'sm', 'ifmp', 'ippc', 'unas', 'cftp', 'crtp', 'compaq-peer', 'wb-mon', 'secure-vmtp', 'uti', 'xtp', 'idpr', 'hmp', 'pim', 'tp++', 'ipv6-no', 'xnet', 'ddx', 'ipv6-frag', 'iso-ip', 'pnni', 'sep', 'dcn', 'ipv6', 'igp', 'arp', 'trunk-2', 'sctp', 'sprite-rpc', 'ipx-n-ip', 'snp', 'vines', 'dgp', 'mhrp', 'pup', 'rtp', 'leaf-1', '3pc', 'mtp', 'mux', 'gre', 'ipcomp', 'ipv6-route', 'sdrp', 'tcp', 'leaf-2', 'i-nlsp', 'xns-idp', 'ip', 'scps', 'pgm', 'sccopmce', 'iplt', 'rdp', 'zero', 'iatp', 'wb-expak', 'fc', 'icmp', 'stp', 'mobile', 'srp', 'ddp', 'eigrp', 'cpnx', 'wsn', 'ax.25', 'netblt', 'ttp'}\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "proto_list = set(full_df.proto.values)\n",
    "print(proto_list)\n",
    "print(len(proto_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1c1476e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pop3', 'unpopular', 'ftp', 'ftp-data', 'ssl', 'ssh', 'snmp', 'radius', 'http', 'dhcp', 'smtp', 'irc', 'dns'}\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "service_list = set(full_df.service.values)\n",
    "print(service_list)\n",
    "print(len(service_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9890adf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'req', 'con', 'eco', 'acc', 'rst', 'no', 'fin', 'clo', 'int', 'par', 'urn'}\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "state_list = set(full_df.state.values)\n",
    "print(state_list)\n",
    "print(len(state_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d96e0",
   "metadata": {},
   "source": [
    "#### There are three categorical variables - 'proto', 'service', 'state'. \n",
    "#### Let us apply one-hot encoding to these features to convert them to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "73080156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proto_3pc</th>\n",
       "      <th>proto_a/n</th>\n",
       "      <th>proto_aes-sp3-d</th>\n",
       "      <th>proto_any</th>\n",
       "      <th>proto_argus</th>\n",
       "      <th>proto_aris</th>\n",
       "      <th>proto_arp</th>\n",
       "      <th>proto_ax.25</th>\n",
       "      <th>proto_bbn-rcc</th>\n",
       "      <th>proto_bna</th>\n",
       "      <th>...</th>\n",
       "      <th>state_clo</th>\n",
       "      <th>state_con</th>\n",
       "      <th>state_eco</th>\n",
       "      <th>state_fin</th>\n",
       "      <th>state_int</th>\n",
       "      <th>state_no</th>\n",
       "      <th>state_par</th>\n",
       "      <th>state_req</th>\n",
       "      <th>state_rst</th>\n",
       "      <th>state_urn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257667</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257668</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257670</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257671</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257672</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231905 rows Ã— 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        proto_3pc  proto_a/n  proto_aes-sp3-d  proto_any  proto_argus  \\\n",
       "0               0          0                0          0            0   \n",
       "1               0          0                0          0            0   \n",
       "2               0          0                0          0            0   \n",
       "3               0          0                0          0            0   \n",
       "4               0          0                0          0            0   \n",
       "...           ...        ...              ...        ...          ...   \n",
       "257667          0          0                0          0            0   \n",
       "257668          0          0                0          0            0   \n",
       "257670          0          0                0          0            0   \n",
       "257671          0          0                0          0            0   \n",
       "257672          0          0                0          0            0   \n",
       "\n",
       "        proto_aris  proto_arp  proto_ax.25  proto_bbn-rcc  proto_bna  ...  \\\n",
       "0                0          0            0              0          0  ...   \n",
       "1                0          0            0              0          0  ...   \n",
       "2                0          0            0              0          0  ...   \n",
       "3                0          0            0              0          0  ...   \n",
       "4                0          0            0              0          0  ...   \n",
       "...            ...        ...          ...            ...        ...  ...   \n",
       "257667           0          1            0              0          0  ...   \n",
       "257668           0          0            0              0          0  ...   \n",
       "257670           0          0            0              0          0  ...   \n",
       "257671           0          0            0              0          0  ...   \n",
       "257672           0          0            0              0          0  ...   \n",
       "\n",
       "        state_clo  state_con  state_eco  state_fin  state_int  state_no  \\\n",
       "0               0          0          0          0          1         0   \n",
       "1               0          0          0          0          1         0   \n",
       "2               0          1          0          0          0         0   \n",
       "3               0          0          0          1          0         0   \n",
       "4               0          1          0          0          0         0   \n",
       "...           ...        ...        ...        ...        ...       ...   \n",
       "257667          0          0          0          0          1         0   \n",
       "257668          0          0          0          0          1         0   \n",
       "257670          0          0          0          0          1         0   \n",
       "257671          0          0          0          0          1         0   \n",
       "257672          0          0          0          0          1         0   \n",
       "\n",
       "        state_par  state_req  state_rst  state_urn  \n",
       "0               0          0          0          0  \n",
       "1               0          0          0          0  \n",
       "2               0          0          0          0  \n",
       "3               0          0          0          0  \n",
       "4               0          0          0          0  \n",
       "...           ...        ...        ...        ...  \n",
       "257667          0          0          0          0  \n",
       "257668          0          0          0          0  \n",
       "257670          0          0          0          0  \n",
       "257671          0          0          0          0  \n",
       "257672          0          0          0          0  \n",
       "\n",
       "[231905 rows x 156 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy1 = pd.get_dummies(full_df[['proto', 'service', 'state']])\n",
    "dummy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4db2719f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_any', 'proto_argus',\n",
       "       'proto_aris', 'proto_arp', 'proto_ax.25', 'proto_bbn-rcc', 'proto_bna',\n",
       "       ...\n",
       "       'state_clo', 'state_con', 'state_eco', 'state_fin', 'state_int',\n",
       "       'state_no', 'state_par', 'state_req', 'state_rst', 'state_urn'],\n",
       "      dtype='object', length=156)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2e510331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proto_3pc</th>\n",
       "      <th>proto_aes-sp3-d</th>\n",
       "      <th>proto_any</th>\n",
       "      <th>proto_argus</th>\n",
       "      <th>proto_aris</th>\n",
       "      <th>proto_arp</th>\n",
       "      <th>proto_ax.25</th>\n",
       "      <th>proto_bbn-rcc</th>\n",
       "      <th>proto_bna</th>\n",
       "      <th>proto_br-sat-mon</th>\n",
       "      <th>...</th>\n",
       "      <th>state_acc</th>\n",
       "      <th>state_clo</th>\n",
       "      <th>state_con</th>\n",
       "      <th>state_eco</th>\n",
       "      <th>state_fin</th>\n",
       "      <th>state_int</th>\n",
       "      <th>state_par</th>\n",
       "      <th>state_req</th>\n",
       "      <th>state_rst</th>\n",
       "      <th>state_urn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257667</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257668</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257670</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257671</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257672</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231905 rows Ã— 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        proto_3pc  proto_aes-sp3-d  proto_any  proto_argus  proto_aris  \\\n",
       "0               0                0          0            0           0   \n",
       "1               0                0          0            0           0   \n",
       "2               0                0          0            0           0   \n",
       "3               0                0          0            0           0   \n",
       "4               0                0          0            0           0   \n",
       "...           ...              ...        ...          ...         ...   \n",
       "257667          0                0          0            0           0   \n",
       "257668          0                0          0            0           0   \n",
       "257670          0                0          0            0           0   \n",
       "257671          0                0          0            0           0   \n",
       "257672          0                0          0            0           0   \n",
       "\n",
       "        proto_arp  proto_ax.25  proto_bbn-rcc  proto_bna  proto_br-sat-mon  \\\n",
       "0               0            0              0          0                 0   \n",
       "1               0            0              0          0                 0   \n",
       "2               0            0              0          0                 0   \n",
       "3               0            0              0          0                 0   \n",
       "4               0            0              0          0                 0   \n",
       "...           ...          ...            ...        ...               ...   \n",
       "257667          1            0              0          0                 0   \n",
       "257668          0            0              0          0                 0   \n",
       "257670          0            0              0          0                 0   \n",
       "257671          0            0              0          0                 0   \n",
       "257672          0            0              0          0                 0   \n",
       "\n",
       "        ...  state_acc  state_clo  state_con  state_eco  state_fin  state_int  \\\n",
       "0       ...          0          0          0          0          0          1   \n",
       "1       ...          0          0          0          0          0          1   \n",
       "2       ...          0          0          1          0          0          0   \n",
       "3       ...          0          0          0          0          1          0   \n",
       "4       ...          0          0          1          0          0          0   \n",
       "...     ...        ...        ...        ...        ...        ...        ...   \n",
       "257667  ...          0          0          0          0          0          1   \n",
       "257668  ...          0          0          0          0          0          1   \n",
       "257670  ...          0          0          0          0          0          1   \n",
       "257671  ...          0          0          0          0          0          1   \n",
       "257672  ...          0          0          0          0          0          1   \n",
       "\n",
       "        state_par  state_req  state_rst  state_urn  \n",
       "0               0          0          0          0  \n",
       "1               0          0          0          0  \n",
       "2               0          0          0          0  \n",
       "3               0          0          0          0  \n",
       "4               0          0          0          0  \n",
       "...           ...        ...        ...        ...  \n",
       "257667          0          0          0          0  \n",
       "257668          0          0          0          0  \n",
       "257670          0          0          0          0  \n",
       "257671          0          0          0          0  \n",
       "257672          0          0          0          0  \n",
       "\n",
       "[231905 rows x 153 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy2 = dummy1.drop(['proto_a/n', 'state_no', 'service_unpopular' ], 1)\n",
    "dummy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7ee8ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1e9b7b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231905, 196)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_new = pd.concat([full_df, dummy2], axis=1)\n",
    "full_df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2bc7d274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231905, 193)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_new = full_df_new.drop(['proto', 'state', 'service'], 1)\n",
    "full_df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a55f7b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 231905 entries, 0 to 257672\n",
      "Columns: 193 entries, dur to state_urn\n",
      "dtypes: float64(11), int32(1), int64(28), uint8(153)\n",
      "memory usage: 115.5 MB\n"
     ]
    }
   ],
   "source": [
    "#full_df_new.info(verbose=1)\n",
    "full_df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934240e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "026273b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "num_vars, cat_vars = sep_num_cat(full_df_new)\n",
    "print(len(num_vars))\n",
    "print(len(cat_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b63aa",
   "metadata": {},
   "source": [
    "## 3.3 Split original dataset in to Train-Test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "951a3831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231905, 192)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = full_df_new.pop('label')\n",
    "X = full_df_new\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4de2bc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231905,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e9039f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Main set -> Train1, Test1\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=30000, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d316e",
   "metadata": {},
   "source": [
    "## 3.4 Standard Scaling of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "19b0f6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.62210931, -0.43169889, -0.3222141 , ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       [-0.62209385, -0.43169889, -0.3222141 , ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       [ 1.46132753, -0.06667776, -0.13549   , ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       ...,\n",
       "       [-0.50252632, -0.24918833, -0.07324863, ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       [-0.62209643, -0.43169889, -0.3222141 , ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       [-0.40745143,  2.21470435,  1.60726829, ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Scale the training set\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "X_train1_std_scaled = std_scaler.fit_transform(X_train1)\n",
    "X_train1_std_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "45dbe6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.62209643, -0.43169889, -0.3222141 , ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       [-0.62209643, -0.43169889, -0.3222141 , ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       [-0.4084536 ,  0.48085395,  0.11347547, ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       ...,\n",
       "       [-0.62210931, -0.43169889, -0.3222141 , ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       [-0.62210416, -0.43169889, -0.3222141 , ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       [-0.61935012, -0.43169889, -0.25997273, ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale transform the testing set\n",
    "X_test1_std_scaled = std_scaler.transform(X_test1)\n",
    "X_test1_std_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "96796b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train1_std_scaled\n",
    "X_test1 = X_test1_std_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "78300512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201905, 192) (201905,) (30000, 192) (30000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train1.shape, y_train1.shape, X_test1.shape, y_test1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab641efb",
   "metadata": {},
   "source": [
    "#### Split further for Autoencoder training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f603770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train1 -> Train2, Test2  ( Train and test Auto-encoder using this )\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_train1, y_train1, test_size=30000, random_state=42, stratify=y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "52c7fa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171905, 192) (171905,) (30000, 192) (30000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train2.shape, y_train2.shape, X_test2.shape, y_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1d9ee78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6328902591547657\n"
     ]
    }
   ],
   "source": [
    "### Check for similar distribution of classes in both train and test sets\n",
    "print(y_train2.sum()/len(y_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2070d90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6329\n"
     ]
    }
   ],
   "source": [
    "print(y_test2.sum()/len(y_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c63e37",
   "metadata": {},
   "source": [
    "# 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b04c1",
   "metadata": {},
   "source": [
    "## 4.1 Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82472444",
   "metadata": {},
   "source": [
    "### We will train and use a Symmetric Stacked Auto Encoder (SAE) with tied weights and gaussian denoising technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9dc5c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "83ab0f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171905, 192)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5cfa4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When an autoencoder is symmetrical, a common technique is to tie the weights of \n",
    "# the decoder layers to that of the encoder layers. This limits the risk of overfitting,\n",
    "# halves the number of weights in the model, speeding up training. This is done by transposing\n",
    "# the weights of encoder for the decoder.\n",
    "\n",
    "# Code to tie the weights of encoder and decoder\n",
    "\n",
    "class DenseTranspose(keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        self.dense = dense\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name=\"bias\",\n",
    "                                      shape=[self.dense.input_shape[-1]],\n",
    "                                      initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
    "        return self.activation(z + self.biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a31d5f4",
   "metadata": {},
   "source": [
    "### 4.1.1 Training AutoEncoder\n",
    "\n",
    "The auto-encoder will have two hidden encoder layers to extract latent features from the input features\n",
    "and two decoder layers ( one hidden and another output layer ) to reconstruct the input features back from the latent features provided by the encoder layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae1e93",
   "metadata": {},
   "source": [
    "#### 4.1.1.1 Hyper-Parameter Tuning of Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7fadb153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to build a AE with the given parameters, for grid search hyper-parameter tuning.\n",
    "\n",
    "# We use \n",
    "# layer_sizes - Number of neurons in first encoder layer and second encoder layer, respectively.\n",
    "# Learn rate - Learning rate for Stochastic Gradient Descent Learning.\n",
    "# gnoise - Gaussian noise fraction to be tried\n",
    "# input shape - input shape of data\n",
    "\n",
    "def tuneModel(layer_sizes, learn_rate, gnoise, input_shape = X_train2.shape[1]): \n",
    "    \n",
    "    print(layer_sizes)\n",
    "    print(learn_rate)\n",
    "    print(gnoise)\n",
    "    \n",
    "    dense_1 = keras.layers.Dense(layer_sizes[0], activation=\"selu\")\n",
    "    dense_2 = keras.layers.Dense(layer_sizes[1], activation=\"selu\")\n",
    "    \n",
    "    #print(\"here\")\n",
    "    \n",
    "    tied_encoder = keras.models.Sequential([\n",
    "      keras.layers.Flatten(input_shape=[X_train2.shape[1]]),\n",
    "      keras.layers.GaussianNoise(gnoise),\n",
    "      dense_1,\n",
    "      dense_2\n",
    "    ])\n",
    "\n",
    "    tied_decoder = keras.models.Sequential([\n",
    "      DenseTranspose(dense_2, activation=\"selu\"),\n",
    "      DenseTranspose(dense_1, activation=\"selu\"),\n",
    "    ])\n",
    "\n",
    "    stacked_ae = keras.models.Sequential([tied_encoder, tied_decoder])\n",
    "    stacked_ae.compile(loss = \"mse\", optimizer=keras.optimizers.SGD(lr=learn_rate))\n",
    "    \n",
    "    print(tied_encoder.summary())\n",
    "    print()\n",
    "    print(tied_decoder.summary())\n",
    "    print()\n",
    "    print(stacked_ae.summary())\n",
    "    \n",
    "    return stacked_ae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b24f43",
   "metadata": {},
   "source": [
    "#### Use RandomizedSearchCV search for best hyperparameter combination \n",
    "\n",
    "Hidden layer sizes (First encoder layer size, Second encoder layer size)  = [(150,75), (120,60), (100,50)] <br>\n",
    "learn_rate=[0.001, 0.002, 0.005], <br>\n",
    "gnoise=[0.1, 0.2, 0.3] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "048f21e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "(120, 60)\n",
      "0.001\n",
      "0.2\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose (DenseTransp (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_1 (DenseTran (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.9732\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.8125\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7633\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7326\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.7082\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6873\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6693\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6535\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6395\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6265\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6143\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6026\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5914\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5803\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5696\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5589\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5483\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5380\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5277\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5175\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5072\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4972\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4873\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4774\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4679\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4582\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4490\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4396\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4307\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4217\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4133\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4048\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3967\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3889\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3813\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3742\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3672\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3604\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.3539\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3479\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3420\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3364\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3311\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3261\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3211\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3164\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3122\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3076\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3040\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2999\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2965\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2929\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2893\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2863\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2830\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2801\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2773\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2744\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2719\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2693\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2669\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2645\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2624\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2602\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2581\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2561\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2539\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2522\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2504\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2484\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2467\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2450\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2434\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2416\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2403\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2388\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2370\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2356\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2350\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2328\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2316\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2302\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2289\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2275\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2262\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2251\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2236\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2226\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2213\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2202\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2192\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2180\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2170\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2158\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2148\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2141\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2128\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2117\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2108\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2099\n",
      "(120, 60)\n",
      "0.001\n",
      "0.2\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_2 (DenseTran (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_3 (DenseTran (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_3 (Sequential)    (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.9947\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.8203\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7649\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7321\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7072\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6868\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6693\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6537\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6395\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6263\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6137\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6015\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5898\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5784\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5674\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5563\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5456\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5349\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5245\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5141\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5038\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4937\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4840\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4741\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4647\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4550\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4464\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4373\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4287\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4203\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4119\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4043\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3964\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3889\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3819\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3749\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3683\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3618\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3554\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3496\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3439\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3383\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3328\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3277\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3226\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3178\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3132\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3088\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3043\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3004\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2960\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2923\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2884\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2845\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2814\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2777\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2741\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2710\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2680\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2646\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2619\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2589\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2560\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2533\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2506\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2481\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2457\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2433\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2408\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2387\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2364\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2344\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2323\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2303\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2284\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2267\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2249\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2231\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2212\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2195\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2181\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2163\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2148\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2133\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2119\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2104\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2089\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2076\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2063\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2049\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2036\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2024\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2012\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2001\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1989\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1980\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1967\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1955\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1945\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1935\n",
      "(120, 60)\n",
      "0.001\n",
      "0.2\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_2 (GaussianNo (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_4 (DenseTran (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_5 (DenseTran (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_6 (Sequential)    (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_7 (Sequential)    (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.9692\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.8124\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7596\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7279\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7034\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.6828\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6651\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6489\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6345\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6208\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6080\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5956\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5835\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5721\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5607\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.5496\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5388\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5278\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5173\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5068\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4965\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4864\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4764\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4666\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4570\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4474\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4386\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4293\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4207\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4122\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4040\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3959\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3882\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3805\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3729\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3659\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3589\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3519\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3453\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3391\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3327\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3268\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3208\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3151\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3098\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3046\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2997\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2946\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2901\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2857\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2816\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2775\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2737\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2699\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2666\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2632\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2602\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2570\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2541\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2513\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2484\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2458\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2432\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2407\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2382\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2360\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2335\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2312\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2287\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2267\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2245\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2222\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2200\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2180\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2160\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2139\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2120\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2100\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2083\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2064\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2048\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2030\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2014\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1998\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1983\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1968\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1953\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1939\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1926\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1912\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1901\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1888\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1876\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1865\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1855\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1844\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1834\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1824\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1816\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1805\n",
      "(120, 60)\n",
      "0.005\n",
      "0.1\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_3 (GaussianNo (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_6 (DenseTran (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_7 (DenseTran (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_9 (Sequential)    (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_10 (Sequential)   (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7566\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6173\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5435\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4730\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4022\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3455\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3079\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2817\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2722\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2474\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2367\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2292\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2218\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2191\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2327\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2240\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2115\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2145\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2087\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2052\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2030\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1990\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1957\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1956\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1980\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1877\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1812\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1702\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1671\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1672\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1732\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1804\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1814\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1793\n",
      "Epoch 00034: early stopping\n",
      "(120, 60)\n",
      "0.005\n",
      "0.1\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_4 (GaussianNo (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_8 (DenseTran (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_9 (DenseTran (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_12 (Sequential)   (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_13 (Sequential)   (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.7720\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.6330\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5581\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4859\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4105\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3490\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3074\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2863\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2666\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2592\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2475\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2500\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2415\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2287\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2185\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2140\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2259\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2186\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2158\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2032\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1952\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2016\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2079\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2027\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1957\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1917\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1907\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1874\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1932\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1997\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1908\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1945\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2035\n",
      "Epoch 00033: early stopping\n",
      "(120, 60)\n",
      "0.005\n",
      "0.1\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_5 (GaussianNo (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_10 (DenseTra (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_11 (DenseTra (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_15 (Sequential)   (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_16 (Sequential)   (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7642\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6207\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5420\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4673\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3964\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3407\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3023\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2843\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2702\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2562\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2434\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2340\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2253\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2181\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2180\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2055\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1965\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1907\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1868\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1831\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1801\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1774\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1751\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1731\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1741\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1706\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1872\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1869\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1879\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1778\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1732\n",
      "Epoch 00031: early stopping\n",
      "(100, 50)\n",
      "0.002\n",
      "0.2\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_6 (GaussianNo (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_12 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_13 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_18 (Sequential)   (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_19 (Sequential)   (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.9371\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7860\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7409\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7091\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6842\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.6632\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6448\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.6278\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.6120\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.5966\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5814\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5664\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5510\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5354\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.5194\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.5030\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.4879\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4714\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4554\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4405\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4254\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4112\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3980\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3851\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3737\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3626\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3534\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3439\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3358\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3285\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3209\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3152\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3089\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3034\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2982\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2952\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2896\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2850\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2813\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2743\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2711\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2679\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2647\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2616\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2592\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2563\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2537A: 0s - loss:\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2514\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2491\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2468\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2445\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2423\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2405\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2385\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2366\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2348\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2333\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2312\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2297\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2281\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2266\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2250\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2237\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2222\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2208\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2197\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2182\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2171\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2157\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2145\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2134\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2123\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2111\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2100\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2089\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2078\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2071\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2058\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2048\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2041\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2032\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2020\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2013\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2000\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1996\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1985\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1979\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.1969\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1962\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1952\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1948\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1939\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1930\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1942\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1919\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1909\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1903\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1895\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1891\n",
      "(100, 50)\n",
      "0.002\n",
      "0.2\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_7 (GaussianNo (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_14 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_15 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_21 (Sequential)   (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_22 (Sequential)   (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.9269\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7919\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7474\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7154\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6898\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6686\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.6503\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.6335\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.6179\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.6025\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.5878\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.5725\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.5571\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.5410\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.5258\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.5087\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.4949\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.4786\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.4642\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.4488\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.4355\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.4219\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.4088\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.3967\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.3850\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.3744\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3641\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.3546\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3468\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3381\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3306\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3236\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3177\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3113\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.3054\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3005\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2959\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2911\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2873\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2834\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2790\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2796\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2759\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2695\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2664\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2630\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2602\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2588\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2553\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2538\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2506\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2475\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2453\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2434\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2411\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2386\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2375\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2350\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2329\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2318\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2298\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2280\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2262\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2253\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2233\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2218\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2199\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2192\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2180\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2165\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2146\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2134\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2122\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2106\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2097\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2091\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2071\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2059\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2053\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2043\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2030\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2017\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2012\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2005\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1995\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1981\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1977\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1965\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1958\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1950A: 0s -\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1938\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1931\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1929\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1916\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1909\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.1896\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.1893\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1881\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.1877\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1867\n",
      "(100, 50)\n",
      "0.002\n",
      "0.2\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_8 (GaussianNo (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_16 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_17 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_24 (Sequential)   (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_25 (Sequential)   (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.9341\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.7803\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.7335\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.7016\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.6768\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.6560\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.6377\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.6208\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6049\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5895\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.5744\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5596\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.5446\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5295\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.5139\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.4986\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4831\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4671\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4523\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4375\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4227\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.4090\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3957\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.3833\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.3716\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.3602\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.3506\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3409\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.3329\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.3244\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.3177\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.3109\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.3046\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2988\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2939\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2885\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2841\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2796\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2754\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2713\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2685\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2645\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2613\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2580\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2550\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - ETA: 0s - loss: 0.252 - 6s 2ms/step - loss: 0.2523\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2496\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2466\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2447\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2418\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2395\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2371\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2352\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2332\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2308\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2294\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2271\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2255\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2236\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2219\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2202\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2186\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2174\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2156\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2137\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2127\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2110\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2100\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2084\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2071\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2059\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2048\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2035\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2023\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.2013\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2002\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1992\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1981\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1971\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1961\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1953\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1944\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1938\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1925\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1919\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1909\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.1903\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1894\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.1885A: 0s - \n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1879\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.1871\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.1864\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.1856\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1851\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.1841\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1840\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1831\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1822\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1817\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.1811\n",
      "(100, 50)\n",
      "0.001\n",
      "0.1\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_9 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_9 (GaussianNo (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_18 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_19 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_27 (Sequential)   (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_28 (Sequential)   (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.9551\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.8181\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.7692\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.7438\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 5s 2ms/step - loss: 0.7246\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.7079\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6929\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.6790\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 31233s 9s/step - loss: 0.6661\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6539\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6423A: \n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.6312\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.6203\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6095\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.5988\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.5882\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.5773\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 916us/step - loss: 0.5663\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.5551\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 964us/step - loss: 0.5437\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.5322\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.5205\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 919us/step - loss: 0.5084\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.4965\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.4843\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.4718\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 924us/step - loss: 0.4595\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.4472\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 920us/step - loss: 0.4353\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.4235\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 924us/step - loss: 0.4118\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 920us/step - loss: 0.4007\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.3901\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.3799\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.3702\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.3610\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.3525\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.3446\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.3370\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.3302\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.3237\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.3177\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 924us/step - loss: 0.3121\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.3069\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 4s 983us/step - loss: 0.3023\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2975\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2934\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.2893\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.2857\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.2821\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.2787\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.2758\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 944us/step - loss: 0.2727\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 964us/step - loss: 0.2699\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.2672\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.2646\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.2622\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.2600\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2577\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 6s 2ms/step - loss: 0.2556\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2536\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2516\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2498\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2480\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2464\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2447\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.2431\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.2415\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2400\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.2385\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.2372\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 920us/step - loss: 0.2358\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.2345\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.2332\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.2321\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 962us/step - loss: 0.2308\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2296\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2283\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.2276\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2263\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2251\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 916us/step - loss: 0.2241\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.2231\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 976us/step - loss: 0.2222\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2213\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 4s 989us/step - loss: 0.2204\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 910us/step - loss: 0.2195\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 906us/step - loss: 0.2186\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 4s 993us/step - loss: 0.2178\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 915us/step - loss: 0.2168\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 905us/step - loss: 0.2160\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.2151\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 913us/step - loss: 0.2144\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.2136\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 905us/step - loss: 0.2129\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.2122\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 903us/step - loss: 0.2114\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 907us/step - loss: 0.2108\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.2100\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 905us/step - loss: 0.2093\n",
      "(100, 50)\n",
      "0.001\n",
      "0.1\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_10 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_10 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_20 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_21 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_30 (Sequential)   (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_31 (Sequential)   (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.9657\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 859us/step - loss: 0.8288\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 858us/step - loss: 0.7828\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 882us/step - loss: 0.7564\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 853us/step - loss: 0.7349\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 854us/step - loss: 0.7153\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.6972\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 855us/step - loss: 0.6806\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 877us/step - loss: 0.6653\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 855us/step - loss: 0.6511\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 857us/step - loss: 0.6377\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.6248\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 858us/step - loss: 0.6123\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 870us/step - loss: 0.5998\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 861us/step - loss: 0.5876\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 857us/step - loss: 0.5751\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 857us/step - loss: 0.5630\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 858us/step - loss: 0.5504\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.5378\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 875us/step - loss: 0.5253\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 857us/step - loss: 0.5125\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 853us/step - loss: 0.4994\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.4866\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 855us/step - loss: 0.4735\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 877us/step - loss: 0.4606\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 855us/step - loss: 0.4479\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 852us/step - loss: 0.4351\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 863us/step - loss: 0.4231\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 857us/step - loss: 0.4113\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 879us/step - loss: 0.3997\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.3890\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.3788\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.3690\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 857us/step - loss: 0.3602\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 881us/step - loss: 0.3516\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 853us/step - loss: 0.3441\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 855us/step - loss: 0.3369\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.3302\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.3239\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 878us/step - loss: 0.3183\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 860us/step - loss: 0.3128\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 854us/step - loss: 0.3080\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.3035\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 858us/step - loss: 0.2990\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 872us/step - loss: 0.2949\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 866us/step - loss: 0.2913\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2877\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2843\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 864us/step - loss: 0.2810\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.2781\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.2752\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 923us/step - loss: 0.2725\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 880us/step - loss: 0.2699\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.2672\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 961us/step - loss: 0.2648\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2625\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.2605\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2583\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 862us/step - loss: 0.2564\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 863us/step - loss: 0.2545\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 873us/step - loss: 0.2526\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2508\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 859us/step - loss: 0.2491\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 3s 857us/step - loss: 0.2475\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2459\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 881us/step - loss: 0.2442\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 3s 858us/step - loss: 0.2430\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 854us/step - loss: 0.2415\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 857us/step - loss: 0.2400\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2387\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 3s 878us/step - loss: 0.2374\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 855us/step - loss: 0.2362\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 859us/step - loss: 0.2350\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 859us/step - loss: 0.2338\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 858us/step - loss: 0.2327\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 882us/step - loss: 0.2316\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 3s 858us/step - loss: 0.2304\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 867us/step - loss: 0.2294\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 857us/step - loss: 0.2283\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2273\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 3s 875us/step - loss: 0.2264\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 855us/step - loss: 0.2253\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2244\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 853us/step - loss: 0.2235\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 3s 858us/step - loss: 0.2227\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 872us/step - loss: 0.2217\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 861us/step - loss: 0.2208\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2199\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 865us/step - loss: 0.2191\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2183\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 854us/step - loss: 0.2176\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.2168\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2160\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 854us/step - loss: 0.2153\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 859us/step - loss: 0.2146\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2138\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.2132\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2124\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 856us/step - loss: 0.2118\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 859us/step - loss: 0.2112\n",
      "(100, 50)\n",
      "0.001\n",
      "0.1\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_11 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_11 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_22 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_23 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_33 (Sequential)   (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_34 (Sequential)   (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 879us/step - loss: 0.9612\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.8134\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 851us/step - loss: 0.7673\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 852us/step - loss: 0.7404\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 3s 851us/step - loss: 0.7184\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 852us/step - loss: 0.6990\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 924us/step - loss: 0.6817\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.6660\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 860us/step - loss: 0.6515\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 850us/step - loss: 0.6378\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 851us/step - loss: 0.6247\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 879us/step - loss: 0.6119\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 853us/step - loss: 0.5992\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 854us/step - loss: 0.5866\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 852us/step - loss: 0.5740\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 850us/step - loss: 0.5614\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 877us/step - loss: 0.5486\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 858us/step - loss: 0.5359\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 848us/step - loss: 0.5229\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 850us/step - loss: 0.5098\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 851us/step - loss: 0.4964\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 869us/step - loss: 0.4829\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 858us/step - loss: 0.4694\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 851us/step - loss: 0.4557\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 851us/step - loss: 0.4421\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 854us/step - loss: 0.4289\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 862us/step - loss: 0.4160\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 865us/step - loss: 0.4033\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 854us/step - loss: 0.3914\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 850us/step - loss: 0.3800\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 855us/step - loss: 0.3692\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 853us/step - loss: 0.3591\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 4s 982us/step - loss: 0.3499\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.3413\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 919us/step - loss: 0.3334\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 913us/step - loss: 0.3262\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 918us/step - loss: 0.3195\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.3133\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 918us/step - loss: 0.3078\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 917us/step - loss: 0.3028\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 915us/step - loss: 0.2980\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.2937\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 915us/step - loss: 0.2898\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 906us/step - loss: 0.2861\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.2824\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 918us/step - loss: 0.2793\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.2761\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 912us/step - loss: 0.2732\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 919us/step - loss: 0.2704\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 913us/step - loss: 0.2677\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 923us/step - loss: 0.2653\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.2629\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.2607\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 917us/step - loss: 0.2585\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 919us/step - loss: 0.2564\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.2544\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2525\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.2507\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.2488\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.2472\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.2456\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.2440\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.2426\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 3s 923us/step - loss: 0.2411\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 3s 921us/step - loss: 0.2397\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.2383\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.2369\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 923us/step - loss: 0.2357\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.2344\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.2333\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.2321\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.2309\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.2299\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 923us/step - loss: 0.2287\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2276\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.2267\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.2256\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.2247\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.2237\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.2228\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 3s 961us/step - loss: 0.2219\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.2209\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.2200\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.2192\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.2183\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 963us/step - loss: 0.2175\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2167\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2159\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.2151\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.2144\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.2137\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2128\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2121\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.2114\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 957us/step - loss: 0.2106\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.2100\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.2093\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2087\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.2080\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2073\n",
      "(150, 75)\n",
      "0.001\n",
      "0.3\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_12 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_12 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_24 (DenseTra (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_25 (DenseTra (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_36 (Sequential)   (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_37 (Sequential)   (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 1.0154\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.8159\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.7499\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.7104\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6820\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6595\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6405\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6241\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6096\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5960\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5836\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5718\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5605\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5497\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5394\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5291\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5193\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5099\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5003\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4914\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4821\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4734\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 4s 992us/step - loss: 0.4644\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 4s 989us/step - loss: 0.4557\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4473\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 4s 994us/step - loss: 0.4388\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 4s 988us/step - loss: 0.4305\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4221\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 985us/step - loss: 0.4139\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4057\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 4s 993us/step - loss: 0.3975\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3893\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3815\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3731\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3652\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3566\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3488\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3410\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3328\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3247\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3171\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3094\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3018\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2942\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2867\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2797\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2724\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2655\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2589\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2523\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2485\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2406\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.2354\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2305\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2258\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2217\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2179\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2147\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2109\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2078\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2050\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2023\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1999\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1974\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1956\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1935\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1914\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1896\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1861\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1844\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1829\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1813\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1798\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1784\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1771\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1758\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1745\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1733\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1721\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1710\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1697\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1686\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1677\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1670\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1657\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1648\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1637\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1631\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1620\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1611\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1606\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1595\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1588\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1579\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1571\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1565\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1557\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1550\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1543\n",
      "(150, 75)\n",
      "0.001\n",
      "0.3\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_13 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_13 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_26 (DenseTra (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_27 (DenseTra (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_39 (Sequential)   (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_40 (Sequential)   (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 1.0272\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.8308\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.7652\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.7259\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6975\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6747\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6554\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6382\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6227\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6084\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5950\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5824\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5706\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5591\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5481\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5375\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5272\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5174\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5078\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4980\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4889\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4801\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4711\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4624\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4539\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4456\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4377\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4293\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4214\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4137\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4061\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3983\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3908\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3839\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3765\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3695\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3623\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3561\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3493\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3428\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3363\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 4s 995us/step - loss: 0.3301\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3240\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 4s 990us/step - loss: 0.3179\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 4s 993us/step - loss: 0.3120\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 4s 993us/step - loss: 0.3060\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3002\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2943\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 4s 997us/step - loss: 0.2881\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2818\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2757\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2692\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2628\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 4s 999us/step - loss: 0.2568\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2510\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2456\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2404\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2354\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 4s 999us/step - loss: 0.2310\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2267\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2229\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2194A: \n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2158\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2126\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2094\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2066\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 4s 998us/step - loss: 0.2040\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2013\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1989\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1967\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1945\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1927\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1906\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1886\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 4s 993us/step - loss: 0.1868\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1850\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 4s 997us/step - loss: 0.1834\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 4s 998us/step - loss: 0.1819\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1802\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 4s 996us/step - loss: 0.1787\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 4s 994us/step - loss: 0.1774\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 4s 991us/step - loss: 0.1760\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 4s 1000us/step - loss: 0.1745\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 4s 989us/step - loss: 0.1733\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 4s 992us/step - loss: 0.1722\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 4s 990us/step - loss: 0.1707\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 4s 988us/step - loss: 0.1698\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1686\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 4s 998us/step - loss: 0.1674\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 4s 995us/step - loss: 0.1664\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 4s 986us/step - loss: 0.1655\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1644\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 4s 994us/step - loss: 0.1634\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 4s 999us/step - loss: 0.1626\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 4s 996us/step - loss: 0.1616\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 4s 993us/step - loss: 0.1606\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1597\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 4s 999us/step - loss: 0.1590\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 4s 999us/step - loss: 0.1581\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 4s 994us/step - loss: 0.1574\n",
      "(150, 75)\n",
      "0.001\n",
      "0.3\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_14 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_14 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_28 (DenseTra (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_29 (DenseTra (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_42 (Sequential)   (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_43 (Sequential)   (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.9877\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.8127\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.7515\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.7133\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6848\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6614\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6417\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6243\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6086\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5941\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5807\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5680\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5560\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5446\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5333\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5230\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5126\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5027\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4927\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4836\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4745\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4654\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4565\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4480\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4396\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4313\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4231\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4153\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4073\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3999\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3920\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3843\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3766\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3692\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3617\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3544\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3468\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3396\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3324\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3252\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3185\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3114\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3048\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2979\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2916\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2851\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2784\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2716\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2648\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2579\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2512\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2445\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2384\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2325\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2270\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2221\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2175\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2133\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2096\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2062\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2031\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2002\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1975\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1952\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1928\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1908\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1887\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1869\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1852\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1834\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1818\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1805\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1790\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1776\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1763\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1750\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1737\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1725\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1714\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1703\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1691\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1681\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1671\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1661\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1652\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1642\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1633\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1624\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1616\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1607\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1599\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1591\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1584\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1575\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1568\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1559\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1552\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1545\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1539\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1532\n",
      "(120, 60)\n",
      "0.005\n",
      "0.2\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_15 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_15 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_30 (DenseTra (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_31 (DenseTra (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_45 (Sequential)   (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_46 (Sequential)   (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.8033\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 997us/step - loss: 0.6606\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6008\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 982us/step - loss: 0.5521\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 4s 985us/step - loss: 0.5062\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.4593\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 962us/step - loss: 0.4168\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.3767\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.3524\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.3331\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.3186\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.2905\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.2755\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.2652\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 979us/step - loss: 0.2617\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.2463\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.2405\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.2333\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.2285\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.2276\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 963us/step - loss: 0.2234\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.2199\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.2215\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.2024\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.1995\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 964us/step - loss: 0.1947\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.1914\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.1888\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.1856\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.1835\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 964us/step - loss: 0.1816\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.1794\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.1780\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.1747\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.1742\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.1712\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.1718\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.1707\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.1721\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 969us/step - loss: 0.1681\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.1827\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.2002\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.2347\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.2197\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 969us/step - loss: 0.2034\n",
      "Epoch 00045: early stopping\n",
      "(120, 60)\n",
      "0.005\n",
      "0.2\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_16 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_16 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_32 (DenseTra (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_33 (DenseTra (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_48 (Sequential)   (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_49 (Sequential)   (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.8014\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.6686\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.6088\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.5598\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.5131\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.4643\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.4242\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.3846\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.3517\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.3318\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.3087\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2990\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.2819\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2897\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2619\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.2675\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2539\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2703\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.2602\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2672\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.2365\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.2372\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.2152\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.2126\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 918us/step - loss: 0.2067\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2058\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2058\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.2072\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.2097\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.1939\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.1962\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.1879\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.1965\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.2153\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2323\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2077\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.2007\n",
      "Epoch 00037: early stopping\n",
      "(120, 60)\n",
      "0.005\n",
      "0.2\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_17 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_17 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_34 (DenseTra (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_35 (DenseTra (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_51 (Sequential)   (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_52 (Sequential)   (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.8011\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.6551\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 920us/step - loss: 0.5908\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 924us/step - loss: 0.5382\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.4859\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.4436\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.3983\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.3614\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.3316\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.3074\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.2918\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.2747\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.2643\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 976us/step - loss: 0.2581\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 991us/step - loss: 0.2694\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2407\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.2378\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2254\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.2206\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.2188\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2279\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.2078\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.2117\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2030\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2183\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 970us/step - loss: 0.1977\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2008\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.1890\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 967us/step - loss: 0.1948\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.1859\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.1834\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.1868\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.1854\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 962us/step - loss: 0.1898\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.1747\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.1761\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.1830\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.1687\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.1663\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.1686\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.1723\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.1751\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.1616\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.1599\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.1580\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.1560\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.1545\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.1535\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.1526\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.1516\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.1514\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.1498\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.1495\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.1493\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.1497\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.1495\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.1545\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.1494\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.1559\n",
      "Epoch 00059: early stopping\n",
      "(150, 75)\n",
      "0.005\n",
      "0.3\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_18 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_18 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_36 (DenseTra (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_37 (DenseTra (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_54 (Sequential)   (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_55 (Sequential)   (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.8002\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6374\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5761\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5296\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4864\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4468\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4059\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3784\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3443\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3142\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2905\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2794\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2655\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2398\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2269\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2333\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1998\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1987\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1828\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1840\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1831\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2071\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1616\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1571\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1529\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1511\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1478\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1496\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1462\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1521\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1420\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1521\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1480\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1659\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1376\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1391\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1353\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1431\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1363\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1524\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1305\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1358\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1302\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1435\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1322\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1470\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1299\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1406\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1211\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1185\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1205\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1224\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1388\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1370\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1248\n",
      "Epoch 00055: early stopping\n",
      "(150, 75)\n",
      "0.005\n",
      "0.3\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_19 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_19 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_38 (DenseTra (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_39 (DenseTra (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_57 (Sequential)   (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_58 (Sequential)   (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.8023\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6371\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5755\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5272\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4836\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4424\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4037\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3737\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3434\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3250\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3048\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2773\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2577\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2450\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2358\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2326\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2855\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2263\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1995\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1984\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1874\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1993\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1724\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1769\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 4s 995us/step - loss: 0.1654\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1740\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1596\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1678\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1523\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1610\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1486\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 4s 1000us/step - loss: 0.1504\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1419\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1451\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1389\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1439\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1355\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1418\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1344\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1417\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1351\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1895\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1679\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1586\n",
      "Epoch 00044: early stopping\n",
      "(150, 75)\n",
      "0.005\n",
      "0.3\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_20 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_20 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_40 (DenseTra (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_41 (DenseTra (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_60 (Sequential)   (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_61 (Sequential)   (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.7998\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6353\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5731\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5244\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4803\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4391\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4020\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3663\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3372\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3218\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2870\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2658\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2571\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2318\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2157\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2039\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1919\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1831\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1765\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1707\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1693\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1654\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1734\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1650\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1858\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1541\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1497\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1553\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1554\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1736\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1441\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1410\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1482\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1504\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1620\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1423\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1370\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1479\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1393\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1587\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1295\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1262\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1287\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1397\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1485\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1338\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1268\n",
      "Epoch 00047: early stopping\n",
      "(150, 75)\n",
      "0.001\n",
      "0.1\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_21 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_21 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_42 (DenseTra (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_43 (DenseTra (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_63 (Sequential)   (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_64 (Sequential)   (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.8926\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.7287\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6779\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6447\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6175\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5937\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5722\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5520\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5327\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5137\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4950\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4764\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4580\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4399\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4221\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4048\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3881\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3721\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3568\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3433\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3302\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3183\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3075\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2978\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2889\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2809\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2738\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2672\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2612\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2556\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2508\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2459\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2417\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2378\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2341\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2304\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2272\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2240\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2210\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2183\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2154\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2130\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2105\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2083\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2060\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2039\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2018\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1997\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1979\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1960\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1942\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1925\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1909\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1892\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1877\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1862\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1848\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1834\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1820\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1808\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1795\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1783\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1770\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1759\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1747\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1736\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1725\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1715\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1705\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1695\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1685\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1676\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1666\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1658\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1648\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1640\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1632\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1624\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1616\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1607\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1600\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1592\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1584\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1577\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1570\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1563\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1556\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1549\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1543\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1537\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1530\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1524\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1517\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1511\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1505\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1498\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1493\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1487\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1482\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1476\n",
      "(150, 75)\n",
      "0.001\n",
      "0.1\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_22 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_22 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_44 (DenseTra (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_45 (DenseTra (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_66 (Sequential)   (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_67 (Sequential)   (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.8807\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.7235\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6715\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6368\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6089\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5845\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5620\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5407\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5202\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5002\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4806\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4613\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4424\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4239\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4063\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3892\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3733\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3580\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3441\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3310\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3191\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3083\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2984\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2895\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2815\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2743\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2676\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2615\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2561\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2510\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2463\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2419\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2378\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2340\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2305\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2271\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2239\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2211\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2181\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2153\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2128\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2103\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2079\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2056\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2034\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2014\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1995\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1975\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1956\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1938\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1921\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1905\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1888\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1873\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1859\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1845\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1831\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1818\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1804\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1792\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1780\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1769\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1757\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1747\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1735\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1725\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1715\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1706\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1696\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1687\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1677\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1667\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1659\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1651\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1642\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1635\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1627\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1618\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1611\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1604\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1598\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1590\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1583\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1576\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1570\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1562\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1557\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1550\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1544\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1539\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1532\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1527\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1521\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1515\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1510\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1504\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1499\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1494\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1488\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1483\n",
      "(150, 75)\n",
      "0.001\n",
      "0.1\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_23 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_23 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_46 (DenseTra (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_47 (DenseTra (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_69 (Sequential)   (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_70 (Sequential)   (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.8947\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.7238\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6682\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6317\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6022\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5763\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5526\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5304\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5091\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4885\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4687\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4489\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4300\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4113\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3936\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3768\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3609\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3460\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3325\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3200\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3088\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2987\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2894\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2811\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2737\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2670\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2608\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2552\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2499\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2451\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2407\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2366\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2328\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2292\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2258\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2227\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2196\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2168\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2142\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2117\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2092\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2069\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2047\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2026\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2005\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1986\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1967\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1949\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1931\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1914\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1898\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1882\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1868\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1853\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1839\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1824\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1812\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1799\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1786\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1774\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1762\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1750\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1739\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1729\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1718\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1707\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1697\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1688\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1678\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1669\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1660\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1651\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1643\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1633A: 0s - loss: 0\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1626\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1617\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1609\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1602\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1595\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1587\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1580\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1572\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1566\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1559\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1552\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1546\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1540\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1533\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1527\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1521\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1515\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1510\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1503\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1498\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1492\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1487\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1481\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1476\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1471\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1465\n",
      "(120, 60)\n",
      "0.005\n",
      "0.3\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_24 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_24 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_48 (DenseTra (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_49 (DenseTra (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_72 (Sequential)   (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_73 (Sequential)   (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.8319\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.6823\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.6289\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 964us/step - loss: 0.5903\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.5545\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.5225\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.4872\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.4528\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 967us/step - loss: 0.4214\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.3947\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.3670\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.3432\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.3337\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 964us/step - loss: 0.3264\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.2966\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.2771\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.2704\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.2494\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 959us/step - loss: 0.2377\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2258\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.2245\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.2126\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 4s 988us/step - loss: 0.2344\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2281\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.2100\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.2047\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2221\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.1934\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.1887\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.1935\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.1873\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2177\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 962us/step - loss: 0.1801\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.1675\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.1681\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.1647\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.1727\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.1628\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.1710\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.1581\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.1650\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.1722\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.1601\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.1591\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 924us/step - loss: 0.1618\n",
      "Epoch 00045: early stopping\n",
      "(120, 60)\n",
      "0.005\n",
      "0.3\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_25 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_25 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_50 (DenseTra (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_51 (DenseTra (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_75 (Sequential)   (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_76 (Sequential)   (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.8494\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.7002\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.6463\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.6079\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.5749\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.5442\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.5119\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.4808\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.4481\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.4198\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.3942\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.3661\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.3585\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.3307\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.3154\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 966us/step - loss: 0.2984\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.2875\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.2891\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2642\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.2628\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.2485\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.2473\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.2300\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.2322\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.2290\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.2334\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.2236\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 920us/step - loss: 0.2111\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.1912\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.1897\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.1851\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.1859\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.1828\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.1847\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.1838\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.1964\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.3215\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 921us/step - loss: 0.2755\n",
      "Epoch 00038: early stopping\n",
      "(120, 60)\n",
      "0.005\n",
      "0.3\n",
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_26 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_26 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_52 (DenseTra (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_53 (DenseTra (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_78 (Sequential)   (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_79 (Sequential)   (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 910us/step - loss: 0.8307\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 945us/step - loss: 0.6835\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 920us/step - loss: 0.6289\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 920us/step - loss: 0.5900\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 916us/step - loss: 0.5563\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.5227\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 921us/step - loss: 0.4904\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 918us/step - loss: 0.4560\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 906us/step - loss: 0.4245\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.3926\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.3640\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.3366\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.3159\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.2891\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.2849\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.2568\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.2786\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.2301\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.2212\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 924us/step - loss: 0.2189\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 945us/step - loss: 0.2336\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.2013\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.1911\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.1874\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.1907\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.2037\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.1776\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.1747\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.1794\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 944us/step - loss: 0.1932\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.1672\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 918us/step - loss: 0.1645\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.1637\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 917us/step - loss: 0.1708\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.1704\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.1674\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 917us/step - loss: 0.1845\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 920us/step - loss: 0.1597\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 914us/step - loss: 0.1569\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.1674\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 916us/step - loss: 0.1581\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.1728\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 908us/step - loss: 0.1522\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.1469\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.1449\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 921us/step - loss: 0.1439\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 913us/step - loss: 0.1451\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.1538\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 914us/step - loss: 0.1592\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.1454\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.1493\n",
      "Epoch 00051: early stopping\n",
      "(120, 60)\n",
      "0.002\n",
      "0.2\n",
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_27 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_27 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_54 (DenseTra (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_55 (DenseTra (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_81 (Sequential)   (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_82 (Sequential)   (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.8896\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.7468\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 959us/step - loss: 0.6968\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.6614\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.6331\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.6084\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.5862\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 961us/step - loss: 0.5649\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.5446\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.5250\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.5052\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.4859\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 961us/step - loss: 0.4666\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.4485\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.4306\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.4139\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.3978\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 944us/step - loss: 0.3829\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.3693\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.3570\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.3447\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 960us/step - loss: 0.3344\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.3247\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.3153\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.3072\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.2993\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 975us/step - loss: 0.2924\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.2861\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2796\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.2739\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.2690\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.2647\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.2597\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.2554\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.2514\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 966us/step - loss: 0.2475\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.2437\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.2408\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.2372\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.2342\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.2314\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.2286\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.2262\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.2234\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.2210\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.2188\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.2169\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.2146\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.2125\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 3s 963us/step - loss: 0.2116\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 945us/step - loss: 0.2089\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.2070\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.2054\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.2035\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 966us/step - loss: 0.2020\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.2004\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.1988\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.1976\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.1963\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 968us/step - loss: 0.1949\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.1940\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.1923\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.1911\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 3s 944us/step - loss: 0.1900\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 3s 969us/step - loss: 0.1887\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.1875\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.1866 0s - loss\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.1853\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 965us/step - loss: 0.1843\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 945us/step - loss: 0.1835\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.1823\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.1814\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.1804\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 964us/step - loss: 0.1795\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.1785\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.1776\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.1767\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.1760\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 964us/step - loss: 0.1751\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.1744\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.1735\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.1728\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 960us/step - loss: 0.1719\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.1712\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.1705\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.1698\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.1690\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 965us/step - loss: 0.1683\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.1677\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.1670\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.1664\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.1656\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 969us/step - loss: 0.1651\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.1643\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 944us/step - loss: 0.1637\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.1636\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.1633\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 968us/step - loss: 0.1626\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.1616\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 945us/step - loss: 0.1608\n",
      "(120, 60)\n",
      "0.002\n",
      "0.2\n",
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_28 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_28 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_56 (DenseTra (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_57 (DenseTra (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_84 (Sequential)   (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_85 (Sequential)   (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.9119\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 986us/step - loss: 0.7598\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.7077\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.6713\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.6418\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.6159\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 977us/step - loss: 0.5923\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.5694\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.5491\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.5280\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 974us/step - loss: 0.5082\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 963us/step - loss: 0.4879\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.4691\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.4503\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.4325\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 976us/step - loss: 0.4152\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.4002\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 963us/step - loss: 0.3843\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.3706\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.3577\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.3460\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.3345\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.3245\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.3149\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 967us/step - loss: 0.3065\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2990\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2915\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2852\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.2791\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 970us/step - loss: 0.2733\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2685\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2631\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2591\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2543\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 977us/step - loss: 0.2510\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.2468\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2435\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.2401\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 969us/step - loss: 0.2370\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.2337\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2313\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2285\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2258\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.2236\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2212\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2189\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2169\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2144\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.2126\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2106\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2091\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2071\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 967us/step - loss: 0.2058\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 959us/step - loss: 0.2037\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.2020\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2008\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1994\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.1980\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.1965\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1951\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1939\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1935\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 970us/step - loss: 0.1913\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.1903\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.1891\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.1880\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.1869\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 962us/step - loss: 0.1857\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.1847\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1837\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1825\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.1814\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.1810\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.1798\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.1790\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.1780\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 4s 978us/step - loss: 0.1772\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.1767\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.1756\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.1745\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 3s 976us/step - loss: 0.1741\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 957us/step - loss: 0.1733\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.1724\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.1717\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.1707\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.1703\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.1693\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.1690\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.1683\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.1675\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 966us/step - loss: 0.1665\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.1665\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.1654\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.1650\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 963us/step - loss: 0.1648\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.1638\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1632\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1626\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1619\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 968us/step - loss: 0.1614\n",
      "(120, 60)\n",
      "0.002\n",
      "0.2\n",
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_29 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_29 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_58 (DenseTra (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_59 (DenseTra (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_87 (Sequential)   (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_88 (Sequential)   (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.8938\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.7429\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.6921\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 977us/step - loss: 0.6568\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.6285\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.6039\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.5813\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.5599\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 968us/step - loss: 0.5393\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.5196\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.5000\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 944us/step - loss: 0.4809\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 944us/step - loss: 0.4618\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 979us/step - loss: 0.4445\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.4267\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.4116\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.3964\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 962us/step - loss: 0.3822\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.3696\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.3572\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.3464\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.3364\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 967us/step - loss: 0.3265\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.3179\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.3099\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.3027\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 944us/step - loss: 0.2958\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 963us/step - loss: 0.2896\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.2830\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.2776\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 4s 990us/step - loss: 0.2727\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 4s 992us/step - loss: 0.2669\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 970us/step - loss: 0.2621\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.2581\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.2536\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.2494\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 4s 981us/step - loss: 0.2460\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.2422\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.2391\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.2361\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.2330\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 967us/step - loss: 0.2301\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.2274\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.2251\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.2221\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 964us/step - loss: 0.2204\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.2173\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.2154\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.2134\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 944us/step - loss: 0.2109\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 966us/step - loss: 0.2092\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.2073\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.2054\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.2033\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.2024\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 965us/step - loss: 0.2004\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.1985\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.1969\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.1956\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 967us/step - loss: 0.1940\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1928\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.1912\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.1899\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.1886\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 3s 970us/step - loss: 0.1874\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.1864\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.1850\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1839\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.1832\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 969us/step - loss: 0.1816\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.1811\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.1795\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.1789\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 962us/step - loss: 0.1777\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1768\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.1760\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.1751\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.1741\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 969us/step - loss: 0.1734\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 3s 944us/step - loss: 0.1724\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.1718\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.1708\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 944us/step - loss: 0.1701\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 962us/step - loss: 0.1694\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.1685\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.1676\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 945us/step - loss: 0.1671\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 950us/step - loss: 0.1662\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 963us/step - loss: 0.1658\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.1650\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.1643\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.1636\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 969us/step - loss: 0.1631\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.1621\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.1615\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.1611\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.1604\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 970us/step - loss: 0.1597\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.1593\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 945us/step - loss: 0.1586\n",
      "(150, 75)\n",
      "0.002\n",
      "0.1\n",
      "Model: \"sequential_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_30 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_30 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_60 (DenseTra (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_61 (DenseTra (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_90 (Sequential)   (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_91 (Sequential)   (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.8010\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6582\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6008\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5554\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5140\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4749\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4369\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4005\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3672\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3382\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3138\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2935\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2774\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2642\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2533\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2438\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2357\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2288\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2227\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2170\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2121\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2074\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2032\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1994\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1958\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1925\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1893\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1864\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1838\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1812\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1791\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1767\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1745\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1725\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1706\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1687\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1669\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1652\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1635\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1621\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1605\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1591\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1577\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1564\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1551\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1538\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1526\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1516\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1503\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1493\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1481\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1474\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1462\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1453\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1444\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1434\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1426\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1418\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1409\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1401\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1393\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1386\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1378\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1371\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1363\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1358\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1350\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1343\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1338\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1330\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1326\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1319\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1314\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1312\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1303\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1298\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1292\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1288\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1283\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1278\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1274\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1268\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1264\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1261\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1257\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1251\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1247\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1242\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1239\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1235\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1230\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1227\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1222\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1219\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1217\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1212\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1208\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1204\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1201\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1198\n",
      "(150, 75)\n",
      "0.002\n",
      "0.1\n",
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_31 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_31 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_62 (DenseTra (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_63 (DenseTra (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_93 (Sequential)   (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_94 (Sequential)   (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.8119\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6675\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6108\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5663\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5258\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4871\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4490\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4132\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3796\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3494\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3239\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3026\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2850\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2705\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2588\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2487\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2399\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2326\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2257\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2196\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2145\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2094\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2050\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2008\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1970\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1934\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1901\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1870\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1841\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1814\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1788\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1765\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1741\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1724\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1700\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1683\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1665\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1647\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1632\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1614\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1603\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1585\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1572\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1558\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.1546\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1534\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1521\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 5s 1ms/step - loss: 0.1509\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1499\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1488\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1477\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1467\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1459\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1448\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1438\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1428\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1421\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1412\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1403\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1396\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1386\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1380\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1372\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1365\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1357\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1351\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1344\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1336\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1332\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1325\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1319\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1312\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1307\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1301\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1295\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1290\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1285\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1279\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1274\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1269\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1275\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1260\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1253\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1250\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1245\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1239\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1235\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1231\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1227\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1222\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1219\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1214\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1211\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1206\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1202\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1197\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1195\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1191\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1187\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1184\n",
      "(150, 75)\n",
      "0.002\n",
      "0.1\n",
      "Model: \"sequential_96\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_32 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_32 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_64 (DenseTra (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_65 (DenseTra (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_98\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_96 (Sequential)   (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_97 (Sequential)   (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.7985\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.6572\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5994\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5532\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5111\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4708\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.4329\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3965\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3636\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3349\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3107\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2906\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2742\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2609\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2499\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2405\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2326\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2254\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2193\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2138\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2088\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2043\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2002\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1964\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1928\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1895\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1864\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1834\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1807\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1782\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1757\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1734\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1714\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1693\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1675\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1656\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1640\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1623\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1608\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1593\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1578\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1565\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1551\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1539\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1526\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1515\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1502\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1492\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1481\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1470\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1461\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1450\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1441\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1432\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1423\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1415\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1406\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1397\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1391\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1382\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1375\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1369\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1361\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1355\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1348\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1341\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1334\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1329A\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1322\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1315\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1310\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1304\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1298\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1293\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1287\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1284\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1277\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1271\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1267\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1261\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1257\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1251\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1246\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1243\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1237\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1232\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1228\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1224\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1220\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1215\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1210\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1206\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1202\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1198\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1194\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1190\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1187\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1183\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1179\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1176\n",
      "(100, 50)\n",
      "0.002\n",
      "0.3\n",
      "Model: \"sequential_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_33 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_33 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_100\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_66 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_67 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_99 (Sequential)   (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_100 (Sequential)  (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.9573\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.8038\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.7573\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.7275\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 907us/step - loss: 0.7055\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.6875\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.6723\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.6585\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.6461\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.6348\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 906us/step - loss: 0.6240\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.6137\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.6039\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 924us/step - loss: 0.5942\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 907us/step - loss: 0.5840\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 906us/step - loss: 0.5746\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.5647\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 908us/step - loss: 0.5547\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.5447\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.5341\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.5240\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.5130\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.5031\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 924us/step - loss: 0.4924\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.4816\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.4705\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.4597\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.4497\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 921us/step - loss: 0.4387\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 903us/step - loss: 0.4275\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.4189\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.4080\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 916us/step - loss: 0.3973\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 905us/step - loss: 0.3871\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 905us/step - loss: 0.3778\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.3689\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.3600\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 913us/step - loss: 0.3509\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 906us/step - loss: 0.3427\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 905us/step - loss: 0.3354\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.3272\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 919us/step - loss: 0.3191\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.3126\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 908us/step - loss: 0.3046\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2966\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.2883\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 905us/step - loss: 0.2798\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2721\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.2646\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.2581\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.2520\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.2472\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2425\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 886us/step - loss: 0.2386\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.2348\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.2318\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.2284\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.2300\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 903us/step - loss: 0.2239\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2204\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2185\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2162\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.2138\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2115\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.2095\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.2079\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.2060\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2043\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2025\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2012\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.1997\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.1981\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.1968\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 908us/step - loss: 0.1954\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.1944\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 903us/step - loss: 0.1930\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.1918\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.1905\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.1895\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.1885\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.1874\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.1865\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 919us/step - loss: 0.1854\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.1846\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.1836\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.1827\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.1817\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.1808\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.1801\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.1791\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.1785\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.1778\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.1768\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.1763\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.1753\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.1750\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.1744\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.1737\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 910us/step - loss: 0.1730\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.1723\n",
      "(100, 50)\n",
      "0.002\n",
      "0.3\n",
      "Model: \"sequential_102\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_34 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_34 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_103\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_68 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_69 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_104\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_102 (Sequential)  (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_103 (Sequential)  (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.9721\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.8149\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.7694\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.7389\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.7154\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.6961\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 930us/step - loss: 0.6795\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.6649\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.6516\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.6393\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.6277\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.6163\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.6056\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.5954\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.5849\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.5747\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 924us/step - loss: 0.5645\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.5534\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.5440\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 903us/step - loss: 0.5328\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.5229\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.5119\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.5010\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.4901\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.4790\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.4683\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.4572\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.4466\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.4354\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.4262\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.4150\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.4051\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.3953\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.3869\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.3762\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.3704\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 921us/step - loss: 0.3617\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.3527\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.3439\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.3376\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.3294\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.3207\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.3133\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.3043\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2968\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2865\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.2777\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2698\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2622\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2545\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.2488\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 919us/step - loss: 0.2434\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2385\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2348\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.2306\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.2277\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.2240\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.2212\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2185\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.2161\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 907us/step - loss: 0.2135\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 3s 921us/step - loss: 0.2117\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2093\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.2079\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.2055\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.2037\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.2019\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2001\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.1987\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.1972\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.1957\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 906us/step - loss: 0.1950\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.1929\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.1918\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.1908\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.1900\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 3s 905us/step - loss: 0.1886\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.1873\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.1862\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.1851\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.1840\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.1828\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.1817\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.1810\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.1799\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.1791\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.1784\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.1774\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.1765\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.1757\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.1754\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.1743\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.1738\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.1729\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.1720\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.1715\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.1710\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.1703\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.1699\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.1712\n",
      "(100, 50)\n",
      "0.002\n",
      "0.3\n",
      "Model: \"sequential_105\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_35 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_35 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_106\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_70 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_71 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_107\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_105 (Sequential)  (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_106 (Sequential)  (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.9631\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.8036\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.7560\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.7247\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.7013\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.6824\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.6664\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.6525\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.6401\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.6287\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 914us/step - loss: 0.6178\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.6077\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.5976\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.5877\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 918us/step - loss: 0.5766\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 906us/step - loss: 0.5677\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.5570\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.5461\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.5356\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 915us/step - loss: 0.5242\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.5131\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.5017\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.4902\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.4786\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 920us/step - loss: 0.4672\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.4553\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.4442\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.4329\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.4218\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 918us/step - loss: 0.4105\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 903us/step - loss: 0.4003\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.3891\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.3793\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.3679\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.3573\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.3455\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.3343\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.3215\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.3098\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 923us/step - loss: 0.2969\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.2865\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2765\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 921us/step - loss: 0.2686\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.2617\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.2563\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 906us/step - loss: 0.2509\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.2465\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.2430\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.2389\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.2353\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.2326\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.2293\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.2268\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.2237\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 924us/step - loss: 0.2213\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.2186\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.2164\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.2142\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.2119\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.2100\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.2081\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.2060\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.2045\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.2026\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 3s 923us/step - loss: 0.2009\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.2003\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.1984\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.1966\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.1952\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.1938\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.1924\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.1912\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.1903\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.1892\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.1878\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.1870\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.1857\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.1848\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.1839\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.1828\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.1822\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 905us/step - loss: 0.1813\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.1808\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.1796\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.1787\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.1780\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.1774\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.1767\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.1759\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 914us/step - loss: 0.1752\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 906us/step - loss: 0.1746\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.1736\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.1732\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.1725\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 912us/step - loss: 0.1721\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.1714\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.1708\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.1703\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.1694\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.1691\n",
      "(100, 50)\n",
      "0.001\n",
      "0.2\n",
      "Model: \"sequential_108\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_36 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_36 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_109\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_72 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_73 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_110\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_108 (Sequential)  (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_109 (Sequential)  (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.9956\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.8413\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.7926\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.7651\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.7440\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.7267\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.7118\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.6988\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.6872\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.6766\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.6667\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.6574\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.6485\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.6399\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.6316\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.6236\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.6157\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.6080\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.6003\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.5927\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.5851\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.5777\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.5702\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.5627\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.5551\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.5478\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.5400\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.5327\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.5249\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 905us/step - loss: 0.5173\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 907us/step - loss: 0.5097\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.5021\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 916us/step - loss: 0.4945\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 908us/step - loss: 0.4868\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.4791\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 887us/step - loss: 0.4715\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.4640\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.4565\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.4491\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.4419\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 886us/step - loss: 0.4349\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.4277\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.4205\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.4136\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.4072\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.4006\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 887us/step - loss: 0.3943\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.3883\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 903us/step - loss: 0.3822\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.3764\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.3708\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.3656\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 912us/step - loss: 0.3604\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.3554\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.3506\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.3459\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.3418\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 918us/step - loss: 0.3375\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.3336\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.3298\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.3260\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.3227\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.3192\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.3161\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.3128\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.3101\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.3071\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.3043\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.3020\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.2992\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 3s 887us/step - loss: 0.2968\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.2945\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.2922\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.2901\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2880\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.2859\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.2837\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.2820\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.2801\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 3s 882us/step - loss: 0.2781\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 3s 886us/step - loss: 0.2764\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2747\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 923us/step - loss: 0.2729\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.2713\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2696\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 880us/step - loss: 0.2681\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 883us/step - loss: 0.2666\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.2650\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2636\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 879us/step - loss: 0.2627\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 881us/step - loss: 0.2606\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 883us/step - loss: 0.2594\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 914us/step - loss: 0.2579\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2567\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.2553\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2540\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.2527\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 919us/step - loss: 0.2519\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.2505\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.2491\n",
      "(100, 50)\n",
      "0.001\n",
      "0.2\n",
      "Model: \"sequential_111\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_37 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_37 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_112\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_74 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_75 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_113\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_111 (Sequential)  (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_112 (Sequential)  (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 923us/step - loss: 1.0095\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.8535\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.7999\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.7700\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.7470\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.7273\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.7101\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.6952\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.6821\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.6704\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.6598\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.6498\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.6405\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.6315\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.6228\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.6144\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.6064\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.5981\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.5904\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.5825\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 908us/step - loss: 0.5748\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 907us/step - loss: 0.5669\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 916us/step - loss: 0.5591\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.5513\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.5436 0s - loss: 0.5\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.5358\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 912us/step - loss: 0.5281\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 907us/step - loss: 0.5204\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.5128\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.5050\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.4973\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 916us/step - loss: 0.4897\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 903us/step - loss: 0.4821\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.4745\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.4670\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.4596\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 914us/step - loss: 0.4521\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.4449\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.4377\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.4308\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.4237\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 917us/step - loss: 0.4167\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.4103\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.4039\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.3974\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.3917\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 920us/step - loss: 0.3857\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.3800\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.3747\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.3693\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.3640\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 923us/step - loss: 0.3590\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.3543\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.3497\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.3454\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.3413\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 923us/step - loss: 0.3372\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.3332\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.3295\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.3258\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.3224\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 3s 922us/step - loss: 0.3190\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.3159\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.3128\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.3098\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.3081\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.3044\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.3016\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2990\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2968\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2941\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.2920\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2898\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.2876\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 903us/step - loss: 0.2854\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2835\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 3s 923us/step - loss: 0.2816\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.2797\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2776\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2759\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2741\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.2724\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2707\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2693\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2677\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 901us/step - loss: 0.2661\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 925us/step - loss: 0.2644\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2629\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2615\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2602\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.2587\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 924us/step - loss: 0.2572\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2561\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2548\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.2553\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2532\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 923us/step - loss: 0.2511\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2499\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2487\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2478\n",
      "(100, 50)\n",
      "0.001\n",
      "0.2\n",
      "Model: \"sequential_114\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_38 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_38 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_115\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_76 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_77 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_116\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_114 (Sequential)  (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_115 (Sequential)  (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 912us/step - loss: 0.9950\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.8456\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.7966\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.7683\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.7465\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.7282\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.7124\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 885us/step - loss: 0.6983\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.6858\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.6743\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.6638\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.6539\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.6446\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.6356\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.6270\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 910us/step - loss: 0.6186\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.6105\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.6023\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.5944\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.5867\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.5788\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.5710\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.5631\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.5554\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.5475\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 908us/step - loss: 0.5398\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.5318\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.5240\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.5159\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.5079\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.4998\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.4916\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 886us/step - loss: 0.4836\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 887us/step - loss: 0.4755\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.4675\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.4596\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.4518\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 886us/step - loss: 0.4440\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.4364\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.4288\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.4217\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.4144\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.4075\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.4006\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.3941\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.3877\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.3814\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 887us/step - loss: 0.3757\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.3698\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.3644\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 917us/step - loss: 0.3594\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.3541\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.3493\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.3448\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.3404\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.3360\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.3320\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.3282\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.3245\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.3208\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.3176\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.3143\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.3112\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 3s 902us/step - loss: 0.3081\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.3052\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 927us/step - loss: 0.3025\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2997\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2969\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.2946\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.2921\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.2899\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.2873\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.2854\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.2833\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2812\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.2790\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2771\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.2753\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2734\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2718\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2701\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.2684\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2668\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2651\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2637\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2621\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.2606\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.2593\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2578\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.2566\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2552\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.2539\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.2525\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 905us/step - loss: 0.2513\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2502\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.2489\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.2478\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.2467\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 898us/step - loss: 0.2456\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.2444\n",
      "(100, 50)\n",
      "0.005\n",
      "0.1\n",
      "Model: \"sequential_117\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_39 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_39 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_118\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_78 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_79 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_119\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_117 (Sequential)  (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_118 (Sequential)  (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.8000\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.6688\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.6089\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.5535\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.4954\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.4353\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.3806\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.3424\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.3132\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 897us/step - loss: 0.2958\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 912us/step - loss: 0.2927\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2780\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2661\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.2584\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 913us/step - loss: 0.2511\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 915us/step - loss: 0.2590\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2639\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2608\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 884us/step - loss: 0.2480\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 921us/step - loss: 0.2387\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 903us/step - loss: 0.2267\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 883us/step - loss: 0.2234\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.2189\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.2159\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 915us/step - loss: 0.2125\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.2226\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 887us/step - loss: 0.2158\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.2355\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2209\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.2224\n",
      "Epoch 00030: early stopping\n",
      "(100, 50)\n",
      "0.005\n",
      "0.1\n",
      "Model: \"sequential_120\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_40 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_40 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_121\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_80 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_81 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_122\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_120 (Sequential)  (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_121 (Sequential)  (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.8132\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.6896\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.6286\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 887us/step - loss: 0.5749\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.5175\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.4546\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.3966\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 883us/step - loss: 0.3523\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.3198\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.3000\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 887us/step - loss: 0.2867\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 886us/step - loss: 0.2889\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.2864\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 887us/step - loss: 0.2583\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.2592\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.2522\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.2628\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2595\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2511\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 913us/step - loss: 0.2360\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2432\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 876us/step - loss: 0.2353\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 884us/step - loss: 0.2430\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 880us/step - loss: 0.2306\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 914us/step - loss: 0.2257\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 889us/step - loss: 0.2225\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 888us/step - loss: 0.2193\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 880us/step - loss: 0.2223\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 885us/step - loss: 0.2240\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 924us/step - loss: 0.2224\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 881us/step - loss: 0.2141\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 882us/step - loss: 0.2133\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2011\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 882us/step - loss: 0.1970\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 920us/step - loss: 0.2324\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 880us/step - loss: 0.2421\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 880us/step - loss: 0.2291\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 884us/step - loss: 0.2518\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 884us/step - loss: 0.2315\n",
      "Epoch 00039: early stopping\n",
      "(100, 50)\n",
      "0.005\n",
      "0.1\n",
      "Model: \"sequential_123\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_41 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_41 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 24,350\n",
      "Trainable params: 24,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_124\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_82 (DenseTra (None, 100)               5150      \n",
      "_________________________________________________________________\n",
      "dense_transpose_83 (DenseTra (None, 192)               19492     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_125\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_123 (Sequential)  (None, 50)                24350     \n",
      "_________________________________________________________________\n",
      "sequential_124 (Sequential)  (None, 192)               24642     \n",
      "=================================================================\n",
      "Total params: 24,642\n",
      "Trainable params: 24,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.7966\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 907us/step - loss: 0.6715\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.6085\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.5520\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.4944\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.4357\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.3795\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 903us/step - loss: 0.3367\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.3075\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 895us/step - loss: 0.2908\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.2885\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 906us/step - loss: 0.2736\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 892us/step - loss: 0.2626\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 896us/step - loss: 0.2540\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 909us/step - loss: 0.2468\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 920us/step - loss: 0.2410\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.2358\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.2307\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.2262\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.2393\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 906us/step - loss: 0.2283\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.2196\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 904us/step - loss: 0.2078\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.2051\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.2006\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 906us/step - loss: 0.1975\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 899us/step - loss: 0.1951\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.1933\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.1910\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.1891\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 911us/step - loss: 0.1876\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 900us/step - loss: 0.1871\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 893us/step - loss: 0.1863\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.2059\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 926us/step - loss: 0.2260\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 894us/step - loss: 0.2135\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 891us/step - loss: 0.1994\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 890us/step - loss: 0.1939\n",
      "Epoch 00038: early stopping\n",
      "(120, 60)\n",
      "0.001\n",
      "0.1\n",
      "Model: \"sequential_126\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_42 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_42 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_127\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_84 (DenseTra (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_85 (DenseTra (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_128\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_126 (Sequential)  (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_127 (Sequential)  (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.9200\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 966us/step - loss: 0.7779\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.7282\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 928us/step - loss: 0.6977\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.6738\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.6532\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 957us/step - loss: 0.6346\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.6174\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.6013\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.5858\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 965us/step - loss: 0.5707\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.5558\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.5411\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 932us/step - loss: 0.5264\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.5119\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.4973\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.4824\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.4679\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 929us/step - loss: 0.4531\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.4386\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.4242\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.4103\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.3969\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.3840\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.3720\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.3605\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.3500\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.3402\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.3312\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 967us/step - loss: 0.3230\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.3153\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.3085\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.3020\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.2962\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 966us/step - loss: 0.2907\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.2857\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2811\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.2766\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.2726\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 961us/step - loss: 0.2687\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.2651\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.2616\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.2585\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2553\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.2525\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.2497\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 933us/step - loss: 0.2471\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.2445\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 960us/step - loss: 0.2421\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.2396\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.2374\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.2354\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.2332\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.2313\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.2294\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.2276\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.2258\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.2241\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 960us/step - loss: 0.2224\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.2208\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.2192\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.2176\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2161\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 3s 945us/step - loss: 0.2146\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.2135\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.2121\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 3s 940us/step - loss: 0.2106\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 960us/step - loss: 0.2095\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.2081\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.2069\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 3s 934us/step - loss: 0.2057\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.2045\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 960us/step - loss: 0.2034\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.2023\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.2012\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.2002\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.1992\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.1981\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.1971\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 3s 935us/step - loss: 0.1962\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 3s 942us/step - loss: 0.1952\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.1945\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.1934\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 931us/step - loss: 0.1927\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.1918\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.1910\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 962us/step - loss: 0.1902\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 936us/step - loss: 0.1894\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.1887\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 941us/step - loss: 0.1879\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.1872\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 957us/step - loss: 0.1865\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 939us/step - loss: 0.1858\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.1851\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 937us/step - loss: 0.1844\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.1837\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.1831\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.1825\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 938us/step - loss: 0.1818\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 947us/step - loss: 0.1813\n",
      "(120, 60)\n",
      "0.001\n",
      "0.1\n",
      "Model: \"sequential_129\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_43 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_43 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_130\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_86 (DenseTra (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_87 (DenseTra (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_131\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_129 (Sequential)  (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_130 (Sequential)  (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 4s 992us/step - loss: 0.9424\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.7900\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 3s 964us/step - loss: 0.7397\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 965us/step - loss: 0.7084 0\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 975us/step - loss: 0.6839\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 969us/step - loss: 0.6628\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 3s 961us/step - loss: 0.6436\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 3s 962us/step - loss: 0.6258\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 960us/step - loss: 0.6089\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 4s 983us/step - loss: 0.5926\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.5768\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 3s 976us/step - loss: 0.5611\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.5455\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.5299\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 4s 980us/step - loss: 0.5141\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.4984\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.4828\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 957us/step - loss: 0.4669\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.4516\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 959us/step - loss: 0.4365\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.4216\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.4072\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.3938\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 4s 994us/step - loss: 0.3807\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 957us/step - loss: 0.3687\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 3s 960us/step - loss: 0.3573\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 976us/step - loss: 0.3471\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.3373\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 4s 983us/step - loss: 0.3284\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.3204\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 957us/step - loss: 0.3130\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.3062\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.3000\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.2944\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2888\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.2840\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.2794\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 4s 978us/step - loss: 0.2751\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 3s 957us/step - loss: 0.2709\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2672\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2637\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.2603\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 3s 962us/step - loss: 0.2571\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.2541\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 957us/step - loss: 0.2513\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.2486\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 975us/step - loss: 0.2461\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2435\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 948us/step - loss: 0.2412\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.2389\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 953us/step - loss: 0.2369\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 963us/step - loss: 0.2348\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.2328\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2309\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.2290\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.2273\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 966us/step - loss: 0.2256\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2240\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2223\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 956us/step - loss: 0.2207\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 968us/step - loss: 0.2192\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2178\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 951us/step - loss: 0.2163\n",
      "Epoch 64/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2150\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.2137\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 975us/step - loss: 0.2124\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2112\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.2099\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.2086\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 959us/step - loss: 0.2075\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2063\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 945us/step - loss: 0.2053\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.2042\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.2031\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.2022\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.2010\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.2001\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1992\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.1982\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 3s 975us/step - loss: 0.1972\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.1964\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.1956\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 961us/step - loss: 0.1946\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 958us/step - loss: 0.1938\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.1930\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 946us/step - loss: 0.1921\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.1915\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 945us/step - loss: 0.1905\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.1898\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.1891\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 955us/step - loss: 0.1884\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.1876\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 954us/step - loss: 0.1870\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.1862\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1856\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 952us/step - loss: 0.1848\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 949us/step - loss: 0.1842\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 961us/step - loss: 0.1835\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 3s 965us/step - loss: 0.1829\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 943us/step - loss: 0.1822\n",
      "(120, 60)\n",
      "0.001\n",
      "0.1\n",
      "Model: \"sequential_132\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_44 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_44 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 60)                7260      \n",
      "=================================================================\n",
      "Total params: 30,420\n",
      "Trainable params: 30,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_133\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_88 (DenseTra (None, 120)               7380      \n",
      "_________________________________________________________________\n",
      "dense_transpose_89 (DenseTra (None, 192)               23352     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_134\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_132 (Sequential)  (None, 60)                30420     \n",
      "_________________________________________________________________\n",
      "sequential_133 (Sequential)  (None, 192)               30732     \n",
      "=================================================================\n",
      "Total params: 30,732\n",
      "Trainable params: 30,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "3582/3582 [==============================] - 4s 982us/step - loss: 0.9191\n",
      "Epoch 2/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.7713\n",
      "Epoch 3/100\n",
      "3582/3582 [==============================] - 4s 988us/step - loss: 0.7225\n",
      "Epoch 4/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.6904\n",
      "Epoch 5/100\n",
      "3582/3582 [==============================] - 3s 975us/step - loss: 0.6642\n",
      "Epoch 6/100\n",
      "3582/3582 [==============================] - 3s 974us/step - loss: 0.6414\n",
      "Epoch 7/100\n",
      "3582/3582 [==============================] - 4s 993us/step - loss: 0.6209\n",
      "Epoch 8/100\n",
      "3582/3582 [==============================] - 4s 978us/step - loss: 0.6021\n",
      "Epoch 9/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.5843\n",
      "Epoch 10/100\n",
      "3582/3582 [==============================] - 3s 975us/step - loss: 0.5673\n",
      "Epoch 11/100\n",
      "3582/3582 [==============================] - 3s 974us/step - loss: 0.5507\n",
      "Epoch 12/100\n",
      "3582/3582 [==============================] - 4s 994us/step - loss: 0.5346\n",
      "Epoch 13/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.5188\n",
      "Epoch 14/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.5030\n",
      "Epoch 15/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.4872\n",
      "Epoch 16/100\n",
      "3582/3582 [==============================] - 4s 982us/step - loss: 0.4717\n",
      "Epoch 17/100\n",
      "3582/3582 [==============================] - 4s 979us/step - loss: 0.4560\n",
      "Epoch 18/100\n",
      "3582/3582 [==============================] - 3s 974us/step - loss: 0.4414\n",
      "Epoch 19/100\n",
      "3582/3582 [==============================] - 4s 978us/step - loss: 0.4265\n",
      "Epoch 20/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.4123\n",
      "Epoch 21/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3984\n",
      "Epoch 22/100\n",
      "3582/3582 [==============================] - 3s 975us/step - loss: 0.3850\n",
      "Epoch 23/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.3726\n",
      "Epoch 24/100\n",
      "3582/3582 [==============================] - 3s 970us/step - loss: 0.3608\n",
      "Epoch 25/100\n",
      "3582/3582 [==============================] - 3s 970us/step - loss: 0.3498\n",
      "Epoch 26/100\n",
      "3582/3582 [==============================] - 4s 998us/step - loss: 0.3395\n",
      "Epoch 27/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.3301\n",
      "Epoch 28/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.3214\n",
      "Epoch 29/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.3134\n",
      "Epoch 30/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.3061\n",
      "Epoch 31/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.2993\n",
      "Epoch 32/100\n",
      "3582/3582 [==============================] - 3s 970us/step - loss: 0.2931\n",
      "Epoch 33/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.2874\n",
      "Epoch 34/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.2820\n",
      "Epoch 35/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2772\n",
      "Epoch 36/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.2726\n",
      "Epoch 37/100\n",
      "3582/3582 [==============================] - 3s 969us/step - loss: 0.2683 0s - l\n",
      "Epoch 38/100\n",
      "3582/3582 [==============================] - 3s 975us/step - loss: 0.2643\n",
      "Epoch 39/100\n",
      "3582/3582 [==============================] - 4s 997us/step - loss: 0.2605\n",
      "Epoch 40/100\n",
      "3582/3582 [==============================] - 4s 983us/step - loss: 0.2570\n",
      "Epoch 41/100\n",
      "3582/3582 [==============================] - 3s 974us/step - loss: 0.2535\n",
      "Epoch 42/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.2504\n",
      "Epoch 43/100\n",
      "3582/3582 [==============================] - 4s 982us/step - loss: 0.2474\n",
      "Epoch 44/100\n",
      "3582/3582 [==============================] - 4s 993us/step - loss: 0.2446\n",
      "Epoch 45/100\n",
      "3582/3582 [==============================] - 3s 968us/step - loss: 0.2419\n",
      "Epoch 46/100\n",
      "3582/3582 [==============================] - 3s 965us/step - loss: 0.2393\n",
      "Epoch 47/100\n",
      "3582/3582 [==============================] - 3s 967us/step - loss: 0.2368\n",
      "Epoch 48/100\n",
      "3582/3582 [==============================] - 4s 985us/step - loss: 0.2344\n",
      "Epoch 49/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.2322\n",
      "Epoch 50/100\n",
      "3582/3582 [==============================] - 3s 968us/step - loss: 0.2301\n",
      "Epoch 51/100\n",
      "3582/3582 [==============================] - 3s 967us/step - loss: 0.2280\n",
      "Epoch 52/100\n",
      "3582/3582 [==============================] - 3s 964us/step - loss: 0.2260\n",
      "Epoch 53/100\n",
      "3582/3582 [==============================] - 4s 992us/step - loss: 0.2242\n",
      "Epoch 54/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.2223\n",
      "Epoch 55/100\n",
      "3582/3582 [==============================] - 3s 975us/step - loss: 0.2206\n",
      "Epoch 56/100\n",
      "3582/3582 [==============================] - 3s 976us/step - loss: 0.2188\n",
      "Epoch 57/100\n",
      "3582/3582 [==============================] - 3s 976us/step - loss: 0.2173\n",
      "Epoch 58/100\n",
      "3582/3582 [==============================] - 4s 992us/step - loss: 0.2157\n",
      "Epoch 59/100\n",
      "3582/3582 [==============================] - 4s 979us/step - loss: 0.2142\n",
      "Epoch 60/100\n",
      "3582/3582 [==============================] - 3s 977us/step - loss: 0.2127\n",
      "Epoch 61/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.2113\n",
      "Epoch 62/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.2100\n",
      "Epoch 63/100\n",
      "3582/3582 [==============================] - 3s 976us/step - loss: 0.2085\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.2073\n",
      "Epoch 65/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.2061\n",
      "Epoch 66/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.2049\n",
      "Epoch 67/100\n",
      "3582/3582 [==============================] - 4s 991us/step - loss: 0.2038\n",
      "Epoch 68/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.2025\n",
      "Epoch 69/100\n",
      "3582/3582 [==============================] - 3s 970us/step - loss: 0.2015\n",
      "Epoch 70/100\n",
      "3582/3582 [==============================] - 3s 969us/step - loss: 0.2005\n",
      "Epoch 71/100\n",
      "3582/3582 [==============================] - 4s 994us/step - loss: 0.1994\n",
      "Epoch 72/100\n",
      "3582/3582 [==============================] - 3s 976us/step - loss: 0.1983\n",
      "Epoch 73/100\n",
      "3582/3582 [==============================] - 3s 969us/step - loss: 0.1973\n",
      "Epoch 74/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.1963\n",
      "Epoch 75/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.1955\n",
      "Epoch 76/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1944\n",
      "Epoch 77/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.1936\n",
      "Epoch 78/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.1926\n",
      "Epoch 79/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.1917\n",
      "Epoch 80/100\n",
      "3582/3582 [==============================] - 4s 995us/step - loss: 0.1909\n",
      "Epoch 81/100\n",
      "3582/3582 [==============================] - 4s 980us/step - loss: 0.1900\n",
      "Epoch 82/100\n",
      "3582/3582 [==============================] - 3s 972us/step - loss: 0.1892\n",
      "Epoch 83/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.1883\n",
      "Epoch 84/100\n",
      "3582/3582 [==============================] - 3s 974us/step - loss: 0.1875\n",
      "Epoch 85/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1867\n",
      "Epoch 86/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.1861\n",
      "Epoch 87/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.1853\n",
      "Epoch 88/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.1845\n",
      "Epoch 89/100\n",
      "3582/3582 [==============================] - 3s 976us/step - loss: 0.1838\n",
      "Epoch 90/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1831\n",
      "Epoch 91/100\n",
      "3582/3582 [==============================] - 3s 975us/step - loss: 0.1824\n",
      "Epoch 92/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.1818\n",
      "Epoch 93/100\n",
      "3582/3582 [==============================] - 3s 971us/step - loss: 0.1810\n",
      "Epoch 94/100\n",
      "3582/3582 [==============================] - 4s 1ms/step - loss: 0.1804\n",
      "Epoch 95/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.1798\n",
      "Epoch 96/100\n",
      "3582/3582 [==============================] - 3s 975us/step - loss: 0.1792\n",
      "Epoch 97/100\n",
      "3582/3582 [==============================] - 3s 974us/step - loss: 0.1785\n",
      "Epoch 98/100\n",
      "3582/3582 [==============================] - 3s 973us/step - loss: 0.1779\n",
      "Epoch 99/100\n",
      "3582/3582 [==============================] - 4s 995us/step - loss: 0.1773\n",
      "Epoch 100/100\n",
      "3582/3582 [==============================] - 3s 968us/step - loss: 0.1768\n",
      "(150, 75)\n",
      "0.002\n",
      "0.1\n",
      "Model: \"sequential_135\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_45 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_45 (GaussianN (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_136\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose_90 (DenseTra (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_91 (DenseTra (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_137\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_135 (Sequential)  (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_136 (Sequential)  (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.7619\n",
      "Epoch 2/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.6166\n",
      "Epoch 3/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.5509\n",
      "Epoch 4/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.4950\n",
      "Epoch 5/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.4421\n",
      "Epoch 6/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.3924\n",
      "Epoch 7/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.3486\n",
      "Epoch 8/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.3125\n",
      "Epoch 9/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2846\n",
      "Epoch 10/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2636\n",
      "Epoch 11/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2475\n",
      "Epoch 12/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2351\n",
      "Epoch 13/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2249\n",
      "Epoch 14/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2167\n",
      "Epoch 15/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2094\n",
      "Epoch 16/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2029\n",
      "Epoch 17/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1974\n",
      "Epoch 18/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1923\n",
      "Epoch 19/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1876\n",
      "Epoch 20/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1835\n",
      "Epoch 21/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1799\n",
      "Epoch 22/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1762\n",
      "Epoch 23/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1733\n",
      "Epoch 24/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1699\n",
      "Epoch 25/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1672\n",
      "Epoch 26/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1646\n",
      "Epoch 27/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1622\n",
      "Epoch 28/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1599\n",
      "Epoch 29/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1580\n",
      "Epoch 30/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1557\n",
      "Epoch 31/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1541\n",
      "Epoch 32/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1521\n",
      "Epoch 33/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1503\n",
      "Epoch 34/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1489\n",
      "Epoch 35/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1472\n",
      "Epoch 36/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1457\n",
      "Epoch 37/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1443\n",
      "Epoch 38/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1428\n",
      "Epoch 39/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1415\n",
      "Epoch 40/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1401\n",
      "Epoch 41/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1392\n",
      "Epoch 42/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1377\n",
      "Epoch 43/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1366\n",
      "Epoch 44/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1356\n",
      "Epoch 45/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1344\n",
      "Epoch 46/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1334\n",
      "Epoch 47/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1326\n",
      "Epoch 48/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1316\n",
      "Epoch 49/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1306\n",
      "Epoch 50/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1298\n",
      "Epoch 51/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1289\n",
      "Epoch 52/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1288\n",
      "Epoch 53/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1274\n",
      "Epoch 54/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1265\n",
      "Epoch 55/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1259\n",
      "Epoch 56/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1250\n",
      "Epoch 57/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1244\n",
      "Epoch 58/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1238\n",
      "Epoch 59/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1230\n",
      "Epoch 60/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1224\n",
      "Epoch 61/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1218\n",
      "Epoch 62/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1212\n",
      "Epoch 63/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1205\n",
      "Epoch 64/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1199\n",
      "Epoch 65/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1193\n",
      "Epoch 66/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1186\n",
      "Epoch 67/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1182\n",
      "Epoch 68/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1175\n",
      "Epoch 69/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1170\n",
      "Epoch 70/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1166\n",
      "Epoch 71/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1159\n",
      "Epoch 72/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1154\n",
      "Epoch 73/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1149\n",
      "Epoch 74/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1143\n",
      "Epoch 75/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1139\n",
      "Epoch 76/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1135\n",
      "Epoch 77/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1129\n",
      "Epoch 78/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1125\n",
      "Epoch 79/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1121\n",
      "Epoch 80/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1117\n",
      "Epoch 81/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1111\n",
      "Epoch 82/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1108\n",
      "Epoch 83/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1102\n",
      "Epoch 84/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1099\n",
      "Epoch 85/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1094\n",
      "Epoch 86/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1090\n",
      "Epoch 87/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1093\n",
      "Epoch 88/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1084\n",
      "Epoch 89/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1079\n",
      "Epoch 90/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1075\n",
      "Epoch 91/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1072\n",
      "Epoch 92/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1069\n",
      "Epoch 93/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1065\n",
      "Epoch 94/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1061\n",
      "Epoch 95/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1059\n",
      "Epoch 96/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1053\n",
      "Epoch 97/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1066\n",
      "Epoch 98/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1050\n",
      "Epoch 99/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1046\n",
      "Epoch 100/100\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1042\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "shapes = [(150,75), (120,60), (100,50)]\n",
    "\n",
    "\n",
    "model = KerasRegressor(tuneModel, verbose=0)\n",
    "\n",
    "param_grid = dict(\n",
    "    layer_sizes=shapes,\n",
    "    learn_rate=[0.001, 0.002, 0.005],\n",
    "    gnoise=[0.1, 0.2, 0.3] \n",
    "    )\n",
    "\n",
    "grid_layers = RandomizedSearchCV(model, param_grid, n_iter=15, cv=3, verbose=1, scoring='neg_mean_squared_error')\n",
    "\n",
    "#### Early stop the training if there is no improvement in val_loss for 5 epochs.\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='loss', patience= 5, min_delta=0.0001, verbose=1)\n",
    "\n",
    "grid_result_layers = grid_layers.fit(X_train2, X_train2, epochs=100, callbacks=[early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5456d245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best hyperparameters:  {'learn_rate': 0.002, 'layer_sizes': (150, 75), 'gnoise': 0.1}\n",
      "\n",
      "Best Score : -0.11774\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learn_rate</th>\n",
       "      <th>param_layer_sizes</th>\n",
       "      <th>param_gnoise</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>564.351</td>\n",
       "      <td>4.934</td>\n",
       "      <td>1.371</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(120, 60)</td>\n",
       "      <td>0.200</td>\n",
       "      <td>{'learn_rate': 0.001, 'layer_sizes': (120, 60)...</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.016</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186.807</td>\n",
       "      <td>5.202</td>\n",
       "      <td>1.389</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.005</td>\n",
       "      <td>(120, 60)</td>\n",
       "      <td>0.100</td>\n",
       "      <td>{'learn_rate': 0.005, 'layer_sizes': (120, 60)...</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.014</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>545.844</td>\n",
       "      <td>7.577</td>\n",
       "      <td>1.255</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(100, 50)</td>\n",
       "      <td>0.200</td>\n",
       "      <td>{'learn_rate': 0.002, 'layer_sizes': (100, 50)...</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.015</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10744.292</td>\n",
       "      <td>14744.419</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(100, 50)</td>\n",
       "      <td>0.100</td>\n",
       "      <td>{'learn_rate': 0.001, 'layer_sizes': (100, 50)...</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.015</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>379.732</td>\n",
       "      <td>10.288</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(150, 75)</td>\n",
       "      <td>0.300</td>\n",
       "      <td>{'learn_rate': 0.001, 'layer_sizes': (150, 75)...</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.015</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>158.976</td>\n",
       "      <td>30.562</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.005</td>\n",
       "      <td>(120, 60)</td>\n",
       "      <td>0.200</td>\n",
       "      <td>{'learn_rate': 0.005, 'layer_sizes': (120, 60)...</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.012</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>179.483</td>\n",
       "      <td>18.054</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.005</td>\n",
       "      <td>(150, 75)</td>\n",
       "      <td>0.300</td>\n",
       "      <td>{'learn_rate': 0.005, 'layer_sizes': (150, 75)...</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.013</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>377.341</td>\n",
       "      <td>3.728</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(150, 75)</td>\n",
       "      <td>0.100</td>\n",
       "      <td>{'learn_rate': 0.001, 'layer_sizes': (150, 75)...</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.015</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>149.835</td>\n",
       "      <td>17.131</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>(120, 60)</td>\n",
       "      <td>0.300</td>\n",
       "      <td>{'learn_rate': 0.005, 'layer_sizes': (120, 60)...</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.014</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>341.719</td>\n",
       "      <td>1.642</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(120, 60)</td>\n",
       "      <td>0.200</td>\n",
       "      <td>{'learn_rate': 0.002, 'layer_sizes': (120, 60)...</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.015</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>381.511</td>\n",
       "      <td>3.823</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(150, 75)</td>\n",
       "      <td>0.100</td>\n",
       "      <td>{'learn_rate': 0.002, 'layer_sizes': (150, 75)...</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>324.803</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(100, 50)</td>\n",
       "      <td>0.300</td>\n",
       "      <td>{'learn_rate': 0.002, 'layer_sizes': (100, 50)...</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.017</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>322.834</td>\n",
       "      <td>1.092</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(100, 50)</td>\n",
       "      <td>0.200</td>\n",
       "      <td>{'learn_rate': 0.001, 'layer_sizes': (100, 50)...</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.014</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>115.342</td>\n",
       "      <td>12.878</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.005</td>\n",
       "      <td>(100, 50)</td>\n",
       "      <td>0.100</td>\n",
       "      <td>{'learn_rate': 0.005, 'layer_sizes': (100, 50)...</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.009</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>344.584</td>\n",
       "      <td>5.161</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(120, 60)</td>\n",
       "      <td>0.100</td>\n",
       "      <td>{'learn_rate': 0.001, 'layer_sizes': (120, 60)...</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.015</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         564.351         4.934            1.371           0.078   \n",
       "1         186.807         5.202            1.389           0.102   \n",
       "2         545.844         7.577            1.255           0.086   \n",
       "3       10744.292     14744.419            0.642           0.002   \n",
       "4         379.732        10.288            0.812           0.087   \n",
       "5         158.976        30.562            0.679           0.023   \n",
       "6         179.483        18.054            0.721           0.012   \n",
       "7         377.341         3.728            0.786           0.103   \n",
       "8         149.835        17.131            0.674           0.010   \n",
       "9         341.719         1.642            0.664           0.007   \n",
       "10        381.511         3.823            0.730           0.018   \n",
       "11        324.803         0.318            0.638           0.008   \n",
       "12        322.834         1.092            0.639           0.011   \n",
       "13        115.342        12.878            0.635           0.009   \n",
       "14        344.584         5.161            0.676           0.018   \n",
       "\n",
       "   param_learn_rate param_layer_sizes param_gnoise  \\\n",
       "0             0.001         (120, 60)        0.200   \n",
       "1             0.005         (120, 60)        0.100   \n",
       "2             0.002         (100, 50)        0.200   \n",
       "3             0.001         (100, 50)        0.100   \n",
       "4             0.001         (150, 75)        0.300   \n",
       "5             0.005         (120, 60)        0.200   \n",
       "6             0.005         (150, 75)        0.300   \n",
       "7             0.001         (150, 75)        0.100   \n",
       "8             0.005         (120, 60)        0.300   \n",
       "9             0.002         (120, 60)        0.200   \n",
       "10            0.002         (150, 75)        0.100   \n",
       "11            0.002         (100, 50)        0.300   \n",
       "12            0.001         (100, 50)        0.200   \n",
       "13            0.005         (100, 50)        0.100   \n",
       "14            0.001         (120, 60)        0.100   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'learn_rate': 0.001, 'layer_sizes': (120, 60)...             -0.179   \n",
       "1   {'learn_rate': 0.005, 'layer_sizes': (120, 60)...             -0.152   \n",
       "2   {'learn_rate': 0.002, 'layer_sizes': (100, 50)...             -0.170   \n",
       "3   {'learn_rate': 0.001, 'layer_sizes': (100, 50)...             -0.197   \n",
       "4   {'learn_rate': 0.001, 'layer_sizes': (150, 75)...             -0.150   \n",
       "5   {'learn_rate': 0.005, 'layer_sizes': (120, 60)...             -0.166   \n",
       "6   {'learn_rate': 0.005, 'layer_sizes': (150, 75)...             -0.114   \n",
       "7   {'learn_rate': 0.001, 'layer_sizes': (150, 75)...             -0.140   \n",
       "8   {'learn_rate': 0.005, 'layer_sizes': (120, 60)...             -0.143   \n",
       "9   {'learn_rate': 0.002, 'layer_sizes': (120, 60)...             -0.147   \n",
       "10  {'learn_rate': 0.002, 'layer_sizes': (150, 75)...             -0.118   \n",
       "11  {'learn_rate': 0.002, 'layer_sizes': (100, 50)...             -0.168   \n",
       "12  {'learn_rate': 0.001, 'layer_sizes': (100, 50)...             -0.220   \n",
       "13  {'learn_rate': 0.005, 'layer_sizes': (100, 50)...             -0.194   \n",
       "14  {'learn_rate': 0.001, 'layer_sizes': (120, 60)...             -0.170   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0              -0.159             -0.197           -0.178           0.016   \n",
       "1              -0.142             -0.175           -0.156           0.014   \n",
       "2              -0.152             -0.188           -0.170           0.015   \n",
       "3              -0.180             -0.218           -0.198           0.015   \n",
       "4              -0.132             -0.168           -0.150           0.015   \n",
       "5              -0.137             -0.158           -0.154           0.012   \n",
       "6              -0.109             -0.138           -0.120           0.013   \n",
       "7              -0.124             -0.161           -0.142           0.015   \n",
       "8              -0.177             -0.158           -0.159           0.014   \n",
       "9              -0.128             -0.166           -0.147           0.015   \n",
       "10             -0.099             -0.137           -0.118           0.016   \n",
       "11             -0.150             -0.191           -0.170           0.017   \n",
       "12             -0.196             -0.229           -0.215           0.014   \n",
       "13             -0.176             -0.195           -0.189           0.009   \n",
       "14             -0.153             -0.188           -0.170           0.015   \n",
       "\n",
       "    rank_test_score  \n",
       "0                12  \n",
       "1                 7  \n",
       "2                10  \n",
       "3                14  \n",
       "4                 5  \n",
       "5                 6  \n",
       "6                 2  \n",
       "7                 3  \n",
       "8                 8  \n",
       "9                 4  \n",
       "10                1  \n",
       "11                9  \n",
       "12               15  \n",
       "13               13  \n",
       "14               11  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nBest hyperparameters: \", grid_result_layers.best_params_)     \n",
    "print(\"\\nBest Score : %0.5f\" % grid_result_layers.best_score_)\n",
    "# scores of GridSearch CV\n",
    "scores = pd.DataFrame(grid_result_layers.cv_results_)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93edf40b",
   "metadata": {},
   "source": [
    "#### 4.1.1.2 Fit AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421689ac",
   "metadata": {},
   "source": [
    "#### Train Autoencoder with the hyper-parameters found by random search cv\n",
    "Best hyperparameters:  {'learn_rate': 0.002, 'layer_sizes': (150, 75), 'gnoise': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2e83e3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 150)               28950     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 75)                11325     \n",
      "=================================================================\n",
      "Total params: 40,275\n",
      "Trainable params: 40,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_transpose (DenseTransp (None, 150)               11475     \n",
      "_________________________________________________________________\n",
      "dense_transpose_1 (DenseTran (None, 192)               29142     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 75)                40275     \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 192)               40617     \n",
      "=================================================================\n",
      "Total params: 40,617\n",
      "Trainable params: 40,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.7597 - val_loss: 0.6790\n",
      "Epoch 2/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.6158 - val_loss: 0.6008\n",
      "Epoch 3/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.5491 - val_loss: 0.5364\n",
      "Epoch 4/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.4915 - val_loss: 0.4815\n",
      "Epoch 5/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.4372 - val_loss: 0.4197\n",
      "Epoch 6/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.3883 - val_loss: 0.3683\n",
      "Epoch 7/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.3449 - val_loss: 0.3287\n",
      "Epoch 8/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.3101 - val_loss: 0.2893\n",
      "Epoch 9/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2836 - val_loss: 0.2646\n",
      "Epoch 10/200\n",
      "5373/5373 [==============================] - 7s 1ms/step - loss: 0.2639 - val_loss: 0.2470\n",
      "Epoch 11/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2483 - val_loss: 0.2476\n",
      "Epoch 12/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2362 - val_loss: 0.2235\n",
      "Epoch 13/200\n",
      "5373/5373 [==============================] - 7s 1ms/step - loss: 0.2264 - val_loss: 0.2154\n",
      "Epoch 14/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2178 - val_loss: 0.2089\n",
      "Epoch 15/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2103 - val_loss: 0.2041\n",
      "Epoch 16/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.2044 - val_loss: 0.1986\n",
      "Epoch 17/200\n",
      "5373/5373 [==============================] - 7s 1ms/step - loss: 0.1986 - val_loss: 0.1945\n",
      "Epoch 18/200\n",
      "5373/5373 [==============================] - 7s 1ms/step - loss: 0.1935 - val_loss: 0.1910\n",
      "Epoch 19/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1888 - val_loss: 0.1876\n",
      "Epoch 20/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1853 - val_loss: 0.1847\n",
      "Epoch 21/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1810 - val_loss: 0.1833\n",
      "Epoch 22/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1772 - val_loss: 0.1793\n",
      "Epoch 23/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1741 - val_loss: 0.1769\n",
      "Epoch 24/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1711 - val_loss: 0.1747\n",
      "Epoch 25/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1683 - val_loss: 0.1727\n",
      "Epoch 26/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1654 - val_loss: 0.1707\n",
      "Epoch 27/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1631 - val_loss: 0.1688\n",
      "Epoch 28/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1606 - val_loss: 0.1672\n",
      "Epoch 29/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1585 - val_loss: 0.1655\n",
      "Epoch 30/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1561 - val_loss: 0.1640\n",
      "Epoch 31/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1540 - val_loss: 0.1625\n",
      "Epoch 32/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1524 - val_loss: 0.1610\n",
      "Epoch 33/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1503 - val_loss: 0.1597\n",
      "Epoch 34/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1486 - val_loss: 0.1585\n",
      "Epoch 35/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1470 - val_loss: 0.1591\n",
      "Epoch 36/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1454 - val_loss: 0.1560\n",
      "Epoch 37/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1439 - val_loss: 0.1548\n",
      "Epoch 38/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1426 - val_loss: 0.1543\n",
      "Epoch 39/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1409 - val_loss: 0.1526\n",
      "Epoch 40/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1397 - val_loss: 0.1517\n",
      "Epoch 41/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1387 - val_loss: 0.1506\n",
      "Epoch 42/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1372 - val_loss: 0.1496\n",
      "Epoch 43/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1360 - val_loss: 0.1487\n",
      "Epoch 44/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1348 - val_loss: 0.1560\n",
      "Epoch 45/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1340 - val_loss: 0.1496\n",
      "Epoch 46/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1327 - val_loss: 0.1462\n",
      "Epoch 47/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1318 - val_loss: 0.1454\n",
      "Epoch 48/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1308 - val_loss: 0.1447\n",
      "Epoch 49/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1297 - val_loss: 0.1440\n",
      "Epoch 50/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1288 - val_loss: 0.1432\n",
      "Epoch 51/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1280 - val_loss: 0.1429\n",
      "Epoch 52/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1270 - val_loss: 0.1416\n",
      "Epoch 53/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1261 - val_loss: 0.1410\n",
      "Epoch 54/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1253 - val_loss: 0.1403\n",
      "Epoch 55/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1245 - val_loss: 0.1416\n",
      "Epoch 56/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1237 - val_loss: 0.1391\n",
      "Epoch 57/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1230 - val_loss: 0.1398\n",
      "Epoch 58/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1225 - val_loss: 0.1381\n",
      "Epoch 59/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1215 - val_loss: 0.1375\n",
      "Epoch 60/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1213 - val_loss: 0.1370\n",
      "Epoch 61/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1202 - val_loss: 0.1365\n",
      "Epoch 62/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1198 - val_loss: 0.1361\n",
      "Epoch 63/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1192 - val_loss: 0.1355\n",
      "Epoch 64/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1185 - val_loss: 0.1351\n",
      "Epoch 65/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1180 - val_loss: 0.1357\n",
      "Epoch 66/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1175 - val_loss: 0.1393\n",
      "Epoch 67/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1170 - val_loss: 0.1338\n",
      "Epoch 68/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1163 - val_loss: 0.1334\n",
      "Epoch 69/200\n",
      "5373/5373 [==============================] - 7s 1ms/step - loss: 0.1159 - val_loss: 0.1329\n",
      "Epoch 70/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1152 - val_loss: 0.1325\n",
      "Epoch 71/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1148 - val_loss: 0.1322\n",
      "Epoch 72/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1143 - val_loss: 0.1318\n",
      "Epoch 73/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1137 - val_loss: 0.1315\n",
      "Epoch 74/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1134 - val_loss: 0.1325\n",
      "Epoch 75/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1132 - val_loss: 0.1308\n",
      "Epoch 76/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1124 - val_loss: 0.1304\n",
      "Epoch 77/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1120 - val_loss: 0.1301\n",
      "Epoch 78/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1117 - val_loss: 0.1298\n",
      "Epoch 79/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1112 - val_loss: 0.1294\n",
      "Epoch 80/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1107 - val_loss: 0.1293\n",
      "Epoch 81/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1106 - val_loss: 0.1289\n",
      "Epoch 82/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1098 - val_loss: 0.1285\n",
      "Epoch 83/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1097 - val_loss: 0.1282\n",
      "Epoch 84/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1092 - val_loss: 0.1279\n",
      "Epoch 85/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1091 - val_loss: 0.1303\n",
      "Epoch 86/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1086 - val_loss: 0.1275\n",
      "Epoch 87/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1079 - val_loss: 0.1270\n",
      "Epoch 88/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1080 - val_loss: 0.1271\n",
      "Epoch 89/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1075 - val_loss: 0.1294\n",
      "Epoch 90/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1072 - val_loss: 0.1263\n",
      "Epoch 91/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1068 - val_loss: 0.1260\n",
      "Epoch 92/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1062 - val_loss: 0.1257\n",
      "Epoch 93/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1062 - val_loss: 0.1254\n",
      "Epoch 94/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1057 - val_loss: 0.1251\n",
      "Epoch 95/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1061 - val_loss: 0.1249\n",
      "Epoch 96/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1053 - val_loss: 0.1259\n",
      "Epoch 97/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1049 - val_loss: 0.1268\n",
      "Epoch 98/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1046 - val_loss: 0.1298\n",
      "Epoch 99/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1042 - val_loss: 0.1240\n",
      "Epoch 100/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1040 - val_loss: 0.1247\n",
      "Epoch 101/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1037 - val_loss: 0.1235\n",
      "Epoch 102/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1034 - val_loss: 0.1233\n",
      "Epoch 103/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1030 - val_loss: 0.1261\n",
      "Epoch 104/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1028 - val_loss: 0.1272\n",
      "Epoch 105/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1026 - val_loss: 0.1227\n",
      "Epoch 106/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1022 - val_loss: 0.1226\n",
      "Epoch 107/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1019 - val_loss: 0.1222\n",
      "Epoch 108/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1017 - val_loss: 0.1220\n",
      "Epoch 109/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1017 - val_loss: 0.1218\n",
      "Epoch 110/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1013 - val_loss: 0.1216\n",
      "Epoch 111/200\n",
      "5373/5373 [==============================] - 7s 1ms/step - loss: 0.1011 - val_loss: 0.1214\n",
      "Epoch 112/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1007 - val_loss: 0.1212\n",
      "Epoch 113/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1003 - val_loss: 0.1210\n",
      "Epoch 114/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1002 - val_loss: 0.1207\n",
      "Epoch 115/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.1000 - val_loss: 0.1205\n",
      "Epoch 116/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0996 - val_loss: 0.1237\n",
      "Epoch 117/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0996 - val_loss: 0.1202\n",
      "Epoch 118/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0993 - val_loss: 0.1201\n",
      "Epoch 119/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0990 - val_loss: 0.1199\n",
      "Epoch 120/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0989 - val_loss: 0.1197\n",
      "Epoch 121/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0985 - val_loss: 0.1196\n",
      "Epoch 122/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0983 - val_loss: 0.1193\n",
      "Epoch 123/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0982 - val_loss: 0.1241\n",
      "Epoch 124/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0979 - val_loss: 0.1223\n",
      "Epoch 125/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0979 - val_loss: 0.1188\n",
      "Epoch 126/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0974 - val_loss: 0.1187\n",
      "Epoch 127/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0989 - val_loss: 0.1185\n",
      "Epoch 128/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0973 - val_loss: 0.1194\n",
      "Epoch 129/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0969 - val_loss: 0.1185\n",
      "Epoch 130/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0967 - val_loss: 0.1181\n",
      "Epoch 131/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0965 - val_loss: 0.1180\n",
      "Epoch 132/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0963 - val_loss: 0.1178\n",
      "Epoch 133/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0962 - val_loss: 0.1193\n",
      "Epoch 134/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0959 - val_loss: 0.1203\n",
      "Epoch 135/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0958 - val_loss: 0.1174\n",
      "Epoch 136/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0957 - val_loss: 0.1172\n",
      "Epoch 137/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0954 - val_loss: 0.1171\n",
      "Epoch 138/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0952 - val_loss: 0.1189\n",
      "Epoch 139/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0952 - val_loss: 0.1168\n",
      "Epoch 140/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0949 - val_loss: 0.1167\n",
      "Epoch 141/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0948 - val_loss: 0.1166\n",
      "Epoch 142/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0946 - val_loss: 0.1167\n",
      "Epoch 143/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0943 - val_loss: 0.1163\n",
      "Epoch 144/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0943 - val_loss: 0.1161\n",
      "Epoch 145/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0940 - val_loss: 0.1160\n",
      "Epoch 146/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0946 - val_loss: 0.1178\n",
      "Epoch 147/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0946 - val_loss: 0.1158\n",
      "Epoch 148/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0937 - val_loss: 0.1183\n",
      "Epoch 149/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0936 - val_loss: 0.1158\n",
      "Epoch 150/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0932 - val_loss: 0.1154\n",
      "Epoch 151/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0932 - val_loss: 0.1153\n",
      "Epoch 152/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0930 - val_loss: 0.1152\n",
      "Epoch 153/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0929 - val_loss: 0.1152\n",
      "Epoch 154/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0926 - val_loss: 0.1178\n",
      "Epoch 155/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0926 - val_loss: 0.1149\n",
      "Epoch 156/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0925 - val_loss: 0.1147\n",
      "Epoch 157/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0922 - val_loss: 0.1146\n",
      "Epoch 158/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0922 - val_loss: 0.1145\n",
      "Epoch 159/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0919 - val_loss: 0.1143\n",
      "Epoch 160/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0919 - val_loss: 0.1142\n",
      "Epoch 161/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0917 - val_loss: 0.1152\n",
      "Epoch 162/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0916 - val_loss: 0.1188\n",
      "Epoch 163/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0917 - val_loss: 0.1139\n",
      "Epoch 164/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0913 - val_loss: 0.1138\n",
      "Epoch 165/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0912 - val_loss: 0.1139\n",
      "Epoch 166/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0911 - val_loss: 0.1169\n",
      "Epoch 167/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0909 - val_loss: 0.1136\n",
      "Epoch 168/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0908 - val_loss: 0.1134\n",
      "Epoch 169/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0906 - val_loss: 0.1133\n",
      "Epoch 170/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0905 - val_loss: 0.1132\n",
      "Epoch 171/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0904 - val_loss: 0.1131\n",
      "Epoch 172/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0902 - val_loss: 0.1129\n",
      "Epoch 173/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0901 - val_loss: 0.1129\n",
      "Epoch 174/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0899 - val_loss: 0.1129\n",
      "Epoch 175/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0900 - val_loss: 0.1127\n",
      "Epoch 176/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0897 - val_loss: 0.1126\n",
      "Epoch 177/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0896 - val_loss: 0.1125\n",
      "Epoch 178/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0895 - val_loss: 0.1129\n",
      "Epoch 179/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0894 - val_loss: 0.1126\n",
      "Epoch 180/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0892 - val_loss: 0.1122\n",
      "Epoch 181/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0892 - val_loss: 0.1121\n",
      "Epoch 182/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0890 - val_loss: 0.1120\n",
      "Epoch 183/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0889 - val_loss: 0.1119\n",
      "Epoch 184/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0888 - val_loss: 0.1130\n",
      "Epoch 185/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0887 - val_loss: 0.1118\n",
      "Epoch 186/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0887 - val_loss: 0.1117\n",
      "Epoch 187/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0885 - val_loss: 0.1116\n",
      "Epoch 188/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0883 - val_loss: 0.1115\n",
      "Epoch 189/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0883 - val_loss: 0.1114\n",
      "Epoch 190/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0882 - val_loss: 0.1141\n",
      "Epoch 191/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0881 - val_loss: 0.1113\n",
      "Epoch 192/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0879 - val_loss: 0.1112\n",
      "Epoch 193/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0879 - val_loss: 0.1110\n",
      "Epoch 194/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0878 - val_loss: 0.1110\n",
      "Epoch 195/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0876 - val_loss: 0.1109\n",
      "Epoch 196/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0875 - val_loss: 0.1109\n",
      "Epoch 197/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0875 - val_loss: 0.1108\n",
      "Epoch 198/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0873 - val_loss: 0.1107\n",
      "Epoch 199/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0873 - val_loss: 0.1107\n",
      "Epoch 200/200\n",
      "5373/5373 [==============================] - 6s 1ms/step - loss: 0.0871 - val_loss: 0.1105\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "#tf.random.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "dense_1 = keras.layers.Dense(150, activation=\"selu\")\n",
    "dense_2 = keras.layers.Dense(75, activation=\"selu\")\n",
    "\n",
    "tied_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[X_train2.shape[1]]),\n",
    "    keras.layers.GaussianNoise(0.1),\n",
    "    dense_1,\n",
    "    dense_2\n",
    "])\n",
    "\n",
    "tied_decoder = keras.models.Sequential([\n",
    "    DenseTranspose(dense_2, activation=\"selu\"),\n",
    "    DenseTranspose(dense_1, activation=\"selu\"),\n",
    "])\n",
    "\n",
    "stacked_ae = keras.models.Sequential([tied_encoder, tied_decoder])\n",
    "\n",
    "stacked_ae.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.002))\n",
    "\n",
    "\n",
    "print(tied_encoder.summary())\n",
    "print()\n",
    "print(tied_decoder.summary())\n",
    "print()\n",
    "print(stacked_ae.summary())\n",
    "\n",
    "#### Early stop the training if there is no improvement in val_loss for 5 epochs.\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, min_delta=0.0001, verbose=1)\n",
    "\n",
    "history = stacked_ae.fit(X_train2, X_train2, epochs=200, batch_size=32,\n",
    "                        validation_data=(X_test2, X_test2), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9c85c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_encoder = tied_encoder\n",
    "stacked_decoder = tied_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "632b955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rajar\\anaconda3\\envs\\mlbook-rnn\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: enc\\assets\n",
      "INFO:tensorflow:Assets written to: dec\\assets\n"
     ]
    }
   ],
   "source": [
    "stacked_encoder.save('enc', overwrite=True)\n",
    "stacked_decoder.save('dec', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "efa88bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#stacked_encoder1 = keras.models.load_model(\"enc\")\n",
    "#stacked_decoder1 = keras.models.load_model(\"dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "16f1f707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Check the learning curves of how training loss and validation loss decrease over epochs\n",
    "\n",
    "plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fe132",
   "metadata": {},
   "source": [
    "### 4.1.2 Verify AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799a999",
   "metadata": {},
   "source": [
    "#### Using a couple of test samples, check the similarity of re-constructed output of the AE with the input values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bd836215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the X_test2 set with the encoder\n",
    "encodings_to_verify = stacked_encoder.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0dc21f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the decodings back from the encoded values, using the decoder\n",
    "decodings_to_verify = stacked_decoder.predict(encodings_to_verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f52ed01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.62209385 -0.43169889 -0.3222141  -0.24044296 -0.14269296  0.05895298\n",
      "  0.69542507 -0.6894596  -0.14260903 -0.28276637]\n",
      "\n",
      "[-0.6238017  -0.4711501  -0.3342756  -0.35745323 -0.14541715  0.14540125\n",
      "  0.717012   -0.69536704 -0.14558409 -0.33749497]\n"
     ]
    }
   ],
   "source": [
    "# Test a couple of test data. We can see that the trained AE is able to reconstruct fairly closely to the input values\n",
    "print(X_test2[1,:10])\n",
    "print()\n",
    "print(decodings_to_verify[1, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ac74263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.62209385 -0.43169889 -0.3222141  -0.24044296 -0.14269296  0.05895298\n",
      "  0.69542507 -0.6894596  -0.14260903 -0.28276637]\n",
      "\n",
      "[-0.63418186 -0.47718406 -0.33795103 -0.3394334  -0.150814    0.16881683\n",
      "  0.7257984  -0.7057223  -0.10996556 -0.33454993]\n"
     ]
    }
   ],
   "source": [
    "print(X_test2[10,:10])\n",
    "print()\n",
    "print(decodings_to_verify[10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb329a",
   "metadata": {},
   "source": [
    "### 4.1.3 Extract Features Using AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32893508",
   "metadata": {},
   "source": [
    "#### Now use the encoder part of the trained AE to encode the whole Train1 training set. Essentially this is extracting a low dimensional feature set.\n",
    "This encoded training set will be further split to train and test the stacking base models and blender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dc9ac9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1_AE_encodings = stacked_encoder.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "12269a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31696823,  0.4419001 , -1.3463588 , ...,  0.5295728 ,\n",
       "         0.12739538,  0.26825556],\n",
       "       [ 1.4799452 ,  9.201553  ,  6.889114  , ..., -1.1198701 ,\n",
       "         5.769218  , -0.76616955],\n",
       "       [-1.2488676 , -1.1347367 ,  0.55561393, ...,  0.3561751 ,\n",
       "         0.45408875, -1.1120504 ],\n",
       "       ...,\n",
       "       [-1.1976529 ,  0.34516874,  0.0351708 , ...,  1.3581378 ,\n",
       "        -0.5586594 , -1.4228177 ],\n",
       "       [ 0.52861065,  1.1894176 , -1.4268789 , ..., -0.7166113 ,\n",
       "         0.4134095 ,  0.38037267],\n",
       "       [-1.2110305 , -1.4591731 ,  1.0920972 , ...,  1.0821443 ,\n",
       "         0.45863953,  0.16046421]], dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1_AE_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b6cee387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201905, 75)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1_AE_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e8378140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_train1_AE_encodings.pkl']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(X_train1_AE_encodings, 'X_train1_AE_encodings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "82de94ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_train1.pkl']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(y_train1, 'y_train1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1107d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import joblib\n",
    "#X_train1_AE_encodings = joblib.load('X_train1_AE_encodings.pkl')\n",
    "#y_train1 = joblib.load('y_train1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821409ce",
   "metadata": {},
   "source": [
    "#### Split the encoded data set further for training and testing the stacking classifier model to be built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8bec90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test3, y_test3 will be set aside for testing the stacking blender.\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_train1_AE_encodings, y_train1, test_size=30000, random_state=42, \n",
    "                                                        stratify=y_train1)\n",
    "\n",
    "# X_train4, y_train4, X_test4, y_test4 will be used to train and test individual base models\n",
    "# After training the base models, their predictions on X_test4, along with y_test4 will be \n",
    "# used to train the stacking blender, as this set was not learnt by base models.\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X_train3, y_train3, test_size=50000, random_state=42, stratify=y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4b1bcaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((171905, 75), (171905,), (30000, 75), (30000,))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train3.shape, y_train3.shape, X_test3.shape,  y_test3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "93916995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((121905, 75), (121905,), (50000, 75), (50000,))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train4.shape, y_train4.shape, X_test4.shape, y_test4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0366ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91299ee2",
   "metadata": {},
   "source": [
    "## 4.2 Stacking Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b8b98",
   "metadata": {},
   "source": [
    "We will train a stacking classifier which will classify normal vs attack traffic, using the feature encodings extracted by the autoencoder we trained.\n",
    "The stacking classifier contains three base models.\n",
    "1. SVM <br>\n",
    "2. RF <br>\n",
    "3. XGBoost <br>\n",
    "and <br>\n",
    "a final blender model which will be a Logistic Regression model. <br><br>\n",
    "The idea of the stacking is all the three base models will separately classify (predict) the input data as normal(Label 0) vs Attack(Label 1). Then the predictions of the three base models will be used as features by the blender model to give out it's final verdict. So the input for the blender will be a set of three values (3-D feature set), for example [1, 0, 1] and it will give out the verdict as normal (0) or attack (1). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5b665",
   "metadata": {},
   "source": [
    "### 4.2.1 Base Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd9ee2",
   "metadata": {},
   "source": [
    "#### 4.2.1.1 Base Model-1 ( Support Vector Machine Classifier )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84480db7",
   "metadata": {},
   "source": [
    "##### Build a simple SVM model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f94f7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1df6da37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(class_weight='balanced', probability=True)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train a simple SVM model. Use class_weight='balanced' as the dataset has some class imbalance. \n",
    "### Use probability=True as SVM by default does not calculate class probabilities.\n",
    "\n",
    "svm_model = SVC(kernel='rbf', probability=True, class_weight='balanced')\n",
    "\n",
    "svm_model.fit(X_train4, y_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1102f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions along with probabilities on the train set\n",
    "y_train4_pred_svm, y_train4_pred_prob_svm  = predict_and_proba(svm_model, X_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "49a097cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "\n",
      " [[40390  4363]\n",
      " [ 7743 69409]]\n",
      "\n",
      "TN : 40390\n",
      "\n",
      "FP : 4363\n",
      "\n",
      "FN : 7743\n",
      "\n",
      "TP : 69409\n",
      "\n",
      "ACCURACY :  0.9006931627086666\n",
      "\n",
      "SENSITIVITY :  0.8996396723351306\n",
      "\n",
      "PRECISION :  0.9408583202298976\n",
      "\n",
      "FALSE POSITIVITY RATE :  0.09749067101646818\n",
      "\n",
      "SPECIFICITY :  0.9025093289835319\n",
      "\n",
      "\n",
      " Classification Report :\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87     44753\n",
      "           1       0.94      0.90      0.92     77152\n",
      "\n",
      "    accuracy                           0.90    121905\n",
      "   macro avg       0.89      0.90      0.89    121905\n",
      "weighted avg       0.90      0.90      0.90    121905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix and other evaluation metric results on Train Data\n",
    "print_binary_classification_summary(y_train4, y_train4_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cc9b60dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC SCORE = 0.971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9709228153808671"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse the ROC curve\n",
    "draw_roc(y_train4, y_train4_pred_prob_svm[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c307a",
   "metadata": {},
   "source": [
    "##### We got a good AUC score, accuracy, sensitivity and precision with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cc3180f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions along with probabilities on the test set\n",
    "y_test4_pred_svm, y_test4_pred_prob_svm  = predict_and_proba(svm_model, X_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "35f1e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "\n",
      " [[16520  1835]\n",
      " [ 3207 28438]]\n",
      "\n",
      "TN : 16520\n",
      "\n",
      "FP : 1835\n",
      "\n",
      "FN : 3207\n",
      "\n",
      "TP : 28438\n",
      "\n",
      "ACCURACY :  0.89916\n",
      "\n",
      "SENSITIVITY :  0.8986569758255648\n",
      "\n",
      "PRECISION :  0.939384930466092\n",
      "\n",
      "FALSE POSITIVITY RATE :  0.09997275946608554\n",
      "\n",
      "SPECIFICITY :  0.9000272405339145\n",
      "\n",
      "\n",
      " Classification Report :\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87     18355\n",
      "           1       0.94      0.90      0.92     31645\n",
      "\n",
      "    accuracy                           0.90     50000\n",
      "   macro avg       0.89      0.90      0.89     50000\n",
      "weighted avg       0.90      0.90      0.90     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Confusion matrix and other evaluation metric results on Test Data\n",
    "print_binary_classification_summary(y_test4, y_test4_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ecf16",
   "metadata": {},
   "source": [
    "#### We got a good AUC score, accuracy, sensitivity and precision with test data also. \n",
    "So SVM seems to be a good candidate to be used as a base model.\n",
    "\n",
    "#### Let's tune hyper-parameters of SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504baede",
   "metadata": {},
   "source": [
    "##### First, search for best hyper-parameters using RandomizedSearchCV\n",
    "We try to tune the following hyper-parameters of SVM algorithm.\n",
    "Gamma\n",
    "C\n",
    "We use AUC as the scoring measure to evaluate the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "af0a7ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "hyper_params = [{'gamma': [1e-2, 1e-3, 1e-4, 'scale', 'auto'],\n",
    "                 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "svm_model2 = SVC(kernel='rbf', class_weight='balanced', probability=True)\n",
    "\n",
    "# Use Multi scorer\n",
    "scoring_measures = {'Precision': 'precision', 'Recall': 'recall', 'AUC' : 'roc_auc'}\n",
    "refit = 'AUC'\n",
    "\n",
    "folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=10)\n",
    "\n",
    "random_search_svm =  RandomizedSearchCV(estimator=svm_model2, param_distributions=hyper_params, scoring=scoring_measures, \n",
    "                                       cv=folds, refit=refit, n_jobs=-1, verbose=1, n_iter=10, random_state=100)\n",
    "\n",
    "random_results_svm = random_search_svm.fit(X_train4, y_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d488cff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_test_AUC\n",
      "\n",
      "Best hyperparameters:  {'gamma': 'scale', 'C': 1000}\n",
      "\n",
      "Best Score : 0.98290\n",
      "   mean_test_Precision  mean_test_Recall  mean_test_AUC\n",
      "3                0.972             0.896          0.983\n"
     ]
    }
   ],
   "source": [
    "# Print the best score found by Random Search and the best hyper-param combination\n",
    "svm_rdm_best_score = grid_result_summary(random_results_svm, scoring_measures, refit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a0580",
   "metadata": {},
   "source": [
    "##### Now, use GridSearchCV to further fine tune and search for best hyper-parameters based on the randomly chosen best parameter values by RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a7bbfb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    }
   ],
   "source": [
    "hyper_params = [{'gamma': ['scale', 'auto'],\n",
    "                 'C': [100, 1000]}]\n",
    "\n",
    "svm_model3 = SVC(kernel='rbf', class_weight='balanced', probability=True)\n",
    "\n",
    "# Use Multi scorer\n",
    "scoring_measures = {'Precision': 'precision', 'Recall': 'recall', 'AUC' : 'roc_auc'}\n",
    "refit = 'AUC'\n",
    "\n",
    "folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=10)\n",
    "\n",
    " \n",
    "grid_search_svm = GridSearchCV(estimator=svm_model3, \n",
    "                               param_grid=hyper_params, \n",
    "                               scoring=scoring_measures,\n",
    "                               cv=folds, refit=refit, n_jobs=-1, \n",
    "                               verbose=1)\n",
    "\n",
    "grid_results_svm = grid_search_svm.fit(X_train4, y_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6a81a208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_test_AUC\n",
      "\n",
      "Best hyperparameters:  {'C': 1000, 'gamma': 'scale'}\n",
      "\n",
      "Best Score : 0.98290\n",
      "   mean_test_Precision  mean_test_Recall  mean_test_AUC\n",
      "2                0.972             0.896          0.983\n"
     ]
    }
   ],
   "source": [
    "# Print the best score found by Grid Search and the best hyper-param combination\n",
    "svm_grd_best_score = grid_result_summary(grid_results_svm, scoring_measures, refit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8f8a4f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best fit model from grid search results\n",
    "svm_grd_best_model = grid_results_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d4f4fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions along with probabilities on the train set\n",
    "y_train4_pred_svm_grd, y_train4_pred_prob_svm_grd  = predict_and_proba(svm_grd_best_model, X_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a25da021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "\n",
      " [[42956  1797]\n",
      " [ 7795 69357]]\n",
      "\n",
      "TN : 42956\n",
      "\n",
      "FP : 1797\n",
      "\n",
      "FN : 7795\n",
      "\n",
      "TP : 69357\n",
      "\n",
      "ACCURACY :  0.9213157786801197\n",
      "\n",
      "SENSITIVITY :  0.8989656781418498\n",
      "\n",
      "PRECISION :  0.9747449194704444\n",
      "\n",
      "FALSE POSITIVITY RATE :  0.04015373271065627\n",
      "\n",
      "SPECIFICITY :  0.9598462672893437\n",
      "\n",
      "\n",
      " Classification Report :\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90     44753\n",
      "           1       0.97      0.90      0.94     77152\n",
      "\n",
      "    accuracy                           0.92    121905\n",
      "   macro avg       0.91      0.93      0.92    121905\n",
      "weighted avg       0.93      0.92      0.92    121905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix and evaluation metrics results on Train Data\n",
    "print_binary_classification_summary(y_train4, y_train4_pred_svm_grd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "89843017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC SCORE = 0.984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9844591085181568"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_roc(y_train4, y_train4_pred_prob_svm_grd[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e402d",
   "metadata": {},
   "source": [
    "<br>\n",
    "We have got the following results from the SVM model on the training set which look very good, with accuracy, sensitivity and precision all near to or above 90%, particularly the precision.\n",
    "\n",
    "AUC SCORE = 0.984 <br>\n",
    "ACCURACY :  0.9213157786801197 <br>\n",
    "SENSITIVITY :  0.8989656781418498 <br>\n",
    "PRECISION :  0.9747449194704444 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "df022f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions along with probabilities on the test set\n",
    "y_test4_pred_svm_grd, y_test4_pred_prob_svm_grd  = predict_and_proba(svm_grd_best_model, X_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "55b0e5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "\n",
      " [[17593   762]\n",
      " [ 3198 28447]]\n",
      "\n",
      "TN : 17593\n",
      "\n",
      "FP : 762\n",
      "\n",
      "FN : 3198\n",
      "\n",
      "TP : 28447\n",
      "\n",
      "ACCURACY :  0.9208\n",
      "\n",
      "SENSITIVITY :  0.898941380944857\n",
      "\n",
      "PRECISION :  0.9739121503646136\n",
      "\n",
      "FALSE POSITIVITY RATE :  0.04151457368564424\n",
      "\n",
      "SPECIFICITY :  0.9584854263143557\n",
      "\n",
      "\n",
      " Classification Report :\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90     18355\n",
      "           1       0.97      0.90      0.93     31645\n",
      "\n",
      "    accuracy                           0.92     50000\n",
      "   macro avg       0.91      0.93      0.92     50000\n",
      "weighted avg       0.93      0.92      0.92     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix and evaluation metrics results on Test Data\n",
    "print_binary_classification_summary(y_test4, y_test4_pred_svm_grd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9e411",
   "metadata": {},
   "source": [
    "We have got the following results from the SVM model on the test set which look very good, with accuracy, sensitivity and precision all near to or above 90%, particularly the precision.\n",
    "\n",
    "ACCURACY :  0.9208\n",
    "\n",
    "SENSITIVITY :  0.898941380944857\n",
    "\n",
    "PRECISION :  0.9739121503646136\n",
    "\n",
    "#### If we compare the above metrics of test data predictions with those obtained from train data predictions, they look almost similar which means our SVM model is able to generalize well\n",
    "So we will have this model as base-model1 to be part of the stacking ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "77a81b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base1 = svm_grd_best_model\n",
    "predictions_base1 = y_test4_pred_svm_grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "cfb13d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_model1.pkl']"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(base1, \"base_model1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "167b95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base1 = joblib.load(\"base_model1.pkl\")\n",
    "#y_test4_pred_svm_grd, y_test4_pred_prob_svm_grd  = predict_and_proba(base1, X_test4)\n",
    "#predictions_base1 = y_test4_pred_svm_grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8de9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d56b767",
   "metadata": {},
   "source": [
    "#### 4.2.1.2 Base Model-2 ( Random Forest Classifier )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecb6b7",
   "metadata": {},
   "source": [
    "##### Build a basic Random Forest Classifier first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "176b6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "456f02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "15edfe35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121905, 75)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c32eed8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train a simple RF model. Use class_weight='balanced' as the dataset has some class imbalance. \n",
    "rf1 = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "### Fit on train data\n",
    "rf1.fit(X_train4, y_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "24703eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on train set - Get predictions along with probabilities\n",
    "y_train4_pred_rf1, y_train4_pred_prob_rf1  = predict_and_proba(rf1, X_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d8c8156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "\n",
      " [[44698    55]\n",
      " [  220 76932]]\n",
      "\n",
      "TN : 44698\n",
      "\n",
      "FP : 55\n",
      "\n",
      "FN : 220\n",
      "\n",
      "TP : 76932\n",
      "\n",
      "ACCURACY :  0.9977441450309668\n",
      "\n",
      "SENSITIVITY :  0.9971484861053505\n",
      "\n",
      "PRECISION :  0.9992855936716588\n",
      "\n",
      "FALSE POSITIVITY RATE :  0.001228967890420754\n",
      "\n",
      "SPECIFICITY :  0.9987710321095793\n",
      "\n",
      "\n",
      " Classification Report :\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     44753\n",
      "           1       1.00      1.00      1.00     77152\n",
      "\n",
      "    accuracy                           1.00    121905\n",
      "   macro avg       1.00      1.00      1.00    121905\n",
      "weighted avg       1.00      1.00      1.00    121905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix and key evaluation metrics on Train set results\n",
    "print_binary_classification_summary(y_train4, y_train4_pred_rf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1f9152a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC SCORE = 1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999285448962717"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROC curve on Train set results\n",
    "draw_roc(y_train4, y_train4_pred_prob_rf1[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d047ee",
   "metadata": {},
   "source": [
    "##### Metrics look good, so RF can be a good candidate for base model. But AUC of 1 seems to be overfit. \n",
    "\n",
    "\n",
    "##### Let us try hyperparameter tuning by using RandomSearchCV first\n",
    "\n",
    "We try tuning the following hyper-params for Random Forest <br>\n",
    "max_depth <br>\n",
    "min_samples_leaf <br>\n",
    "min_samples_split <br>\n",
    "n_estimators <br>\n",
    "max_features <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8cfcc483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100], 'min_samples_leaf': [100, 200, 300, 400], 'min_samples_split': [100, 200, 300, 400], 'n_estimators': [100, 200, 300, 400, 500], 'max_features': ['auto', 'sqrt']}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [int(x) for x in np.linspace(5,100,20)],\n",
    "    'min_samples_leaf': [int(x) for x in np.linspace(100,400,4)] ,\n",
    "    'min_samples_split': [int(x) for x in np.linspace(100,400,4)],\n",
    "    'n_estimators': [int(x) for x in np.linspace(100,500,5)], \n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "print(param_grid)\n",
    "\n",
    "rf_rs = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "scoring_rf_rs = {'Precision':'precision', 'Recall':'recall', 'AUC':'roc_auc'}\n",
    "refit = 'AUC'\n",
    "\n",
    "folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=100)\n",
    "\n",
    "random_search_rf =  RandomizedSearchCV(estimator=rf_rs, param_distributions=param_grid, scoring=scoring_rf_rs, \n",
    "                                       cv=folds, refit=refit, n_jobs=-1, verbose=10, n_iter=100, random_state=100)\n",
    "\n",
    "random_results_rf = random_search_rf.fit(X_train4, y_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2ce6e4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_test_AUC\n",
      "\n",
      "Best hyperparameters:  {'n_estimators': 100, 'min_samples_split': 200, 'min_samples_leaf': 100, 'max_features': 'sqrt', 'max_depth': 65}\n",
      "\n",
      "Best Score : 0.98207\n",
      "    mean_test_Precision  mean_test_Recall  mean_test_AUC\n",
      "18                0.960             0.898          0.982\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameter set found from random search cv\n",
    "rf_rand_best_score1 = grid_result_summary(random_results_rf, scoring_rf_rs, refit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54a8ef",
   "metadata": {},
   "source": [
    "##### Based on the best hyperparam values found by RandomizedSearchCV, let us use GridSearchCV to further narrow down the optimal hyper-param values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "287e94a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_depth': [60, 65, 70],\n",
    "    'min_samples_leaf': [80, 100, 120],\n",
    "    'min_samples_split': [150, 200, 250],\n",
    "    'n_estimators': [80, 100, 120] \n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf_grd1 = RandomForestClassifier(class_weight=\"balanced\", max_features='sqrt')\n",
    "\n",
    "# Use Multi scorer\n",
    "scoring_measures = {'Precision': 'precision', 'Recall': 'recall', 'AUC' : 'roc_auc'}\n",
    "refit = 'AUC'\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "\n",
    "grid_search_rf1 = GridSearchCV(estimator=rf_grd1, param_grid=param_grid, cv=folds, \n",
    "                               scoring=scoring_measures, refit=refit, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "\n",
    "grid_results_rf1 = grid_search_rf1.fit(X_train4, y_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5db1b8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_test_AUC\n",
      "\n",
      "Best hyperparameters:  {'max_depth': 70, 'min_samples_leaf': 80, 'min_samples_split': 150, 'n_estimators': 120}\n",
      "\n",
      "Best Score : 0.98295\n",
      "    mean_test_Precision  mean_test_Recall  mean_test_AUC\n",
      "56                0.962             0.901          0.983\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameter set found from grid search cv\n",
    "rf_grd_best_score1 = grid_result_summary(grid_results_rf1, scoring_measures, refit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65fa117",
   "metadata": {},
   "source": [
    "##### Since best params are from the border values in the earlier grid param options, let us try tuning again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "48692b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid based on the results of previous grid search \n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [65, 70, 75],\n",
    "    'min_samples_leaf': [70, 80, 90],\n",
    "    'min_samples_split': [125, 150, 175],\n",
    "    'n_estimators': [100, 120, 140] \n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf_grd2 = RandomForestClassifier(class_weight=\"balanced\", max_features='sqrt')\n",
    "\n",
    "# Use Multi scorer\n",
    "scoring_measures = {'Precision': 'precision', 'Recall': 'recall', 'AUC' : 'roc_auc'}\n",
    "refit = 'AUC'\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "# Refit and give the best model based on f1 score. \n",
    "grid_search_rf2 = GridSearchCV(estimator=rf_grd2, param_grid=param_grid, cv=folds, \n",
    "                               scoring=scoring_measures, refit=refit, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_results_rf2 = grid_search_rf2.fit(X_train4, y_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fc893678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_test_AUC\n",
      "\n",
      "Best hyperparameters:  {'max_depth': 70, 'min_samples_leaf': 70, 'min_samples_split': 125, 'n_estimators': 120}\n",
      "\n",
      "Best Score : 0.98320\n",
      "    mean_test_Precision  mean_test_Recall  mean_test_AUC\n",
      "28                0.962             0.902          0.983\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameter set found from grid search cv\n",
    "rf_grd_best_score2 = grid_result_summary(grid_results_rf2, scoring_measures, refit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54916829",
   "metadata": {},
   "source": [
    "##### Let us stop hyperparameter tuning as we are not seeing any great improvements from the previous gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7d192190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best fit model from latest grid search results\n",
    "rf_grd_best_model = grid_results_rf2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7837039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on train data\n",
    "y_train4_pred_rf_grd, y_train4_pred_prob_rf_grd  = predict_and_proba(rf_grd_best_model, X_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3760eb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "\n",
      " [[42660  2093]\n",
      " [ 6914 70238]]\n",
      "\n",
      "TN : 42660\n",
      "\n",
      "FP : 2093\n",
      "\n",
      "FN : 6914\n",
      "\n",
      "TP : 70238\n",
      "\n",
      "ACCURACY :  0.9261145974324269\n",
      "\n",
      "SENSITIVITY :  0.9103846951472419\n",
      "\n",
      "PRECISION :  0.971063582696216\n",
      "\n",
      "FALSE POSITIVITY RATE :  0.04676781444819342\n",
      "\n",
      "SPECIFICITY :  0.9532321855518066\n",
      "\n",
      "\n",
      " Classification Report :\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90     44753\n",
      "           1       0.97      0.91      0.94     77152\n",
      "\n",
      "    accuracy                           0.93    121905\n",
      "   macro avg       0.92      0.93      0.92    121905\n",
      "weighted avg       0.93      0.93      0.93    121905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Confusion matrix and key evaluation metrics on Train set results\n",
    "print_binary_classification_summary(y_train4, y_train4_pred_rf_grd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7dc1a699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVIAAAFNCAYAAABSVeehAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABGMklEQVR4nO3dd3gVZfbA8e8hlFBCR3ov0ouioKACklAEEUEQUURRXBd0UREL/lwUV9d1XSzgIqJSRAEVBCkinQAqUiJ1QRCkiNIh9JTz+2Mm8RJSLkluJjc5n+fJkzv9vPfOPfedeWfeEVXFGGNM+uXxOgBjjAl2lkiNMSaDLJEaY0wGWSI1xpgMskRqjDEZZInUGGMyyBJpgIjIFhFp43UcXhORsSLyf1m8zQki8kpWbjNQRKSviHybzmVz7D4oIioitbyOI4HkhutIRWQPUBaIA04D3wCDVfW0l3HlNCLSH3hIVVt7HMcEYL+qvuBxHCOAWqp6bxZsawLZoMxZRUQUqK2qO72OBXJXjbSrqhYBmgLNgOe8DefKiUje3LhtL9l7bvyiqjn+D9gDtPcZ/hcw12e4JbAaOAH8BLTxmVYS+Bj4DTgOfOUzrQsQ5S63GmicdJtABeAcUNJnWjPgCJDPHX4Q2OaufwFQ1WdeBQYBPwO7Uyjf7cAWN45lQL0kcTwHbHXX/zEQegVleAbYCFwA8gLPAruAaHed3d156wHn+bPWf8IdPwF4xX3dBtgPPAUcAg4CD/hsrxTwNXAK+BF4BViZyufa2udz2wf099nmGGCuG+cPQE2f5d525z8FrANu8pk2AvgC+MSd/hBwPfCdu52DwGggv88yDYCFwDHgD+B5oCNwEYhx34+f3HmLAR+66zngljHEndYfWAWMAo660/onvAeAuNMOubFtAhoCA93tXHS39XXS/R4IceNK+OzWAZVTeF+T/T4AN+Lst5Xd4SY4+1RddzjZfSOZsp0AfnHX19/9LA4B9/vMPwEY676v0cByLv9e1HJfFwD+Dex13/+xQMEszTFeJ7ksKeSlO1Qldwd82x2u6O60nXFq6OHucBl3+lxgGlACyAfc4o5v5n74Ldyd9H53OwWS2eYS4GGfeN4AxrqvuwE7cRJRXuAFYHWSHWYhTkK/bOcA6gBn3LjzAcPc9eX3iWMzUNldxyr+TGz+lCHKXbagO+4unB+HPEBvd9vlfb4sK5PEN4FLE2ks8LIba2fgLFDCnT7V/SsE1Mf5giWbSIGqOF+wPu66SgFNfbZ5FCcB5gWmAFN9lr3XnT8vTlL/HffHBSeRxgB3uGUsCFyLk1zyAtVwfvSGuPOH4STFp4BQd7iFz7o+SRL3TOB9oDBwFbAGeMTn/YsFHnO3VZBLE2kHnARYHCep1vN57xPf5xT2+6dx9vur3WWbAKWSeV/T+j78A2d/Luiub7DPsmntG7HAAzj72is4iW8MTiKMcD/PIj7liQZudqe/jc++wKWJdBQwG2f/DsP5MX4tS3OM10kuSwrp7FCn3Q9GgcVAcXfaM8DkJPMvwEkq5YF43C96knn+C4xMMm47fyZa3534IWCJ+1pwEsTN7vB8YIDPOvLgJJeqPjtMu1TK9n/A9CTLH+DPWsQe4C8+0zsDu66gDA+m8d5GAd3c1/1JO5GeA/L6TD+Ek6RCcBLY1T7TUqyR4tSyZ6YwbQIwPkmZ/5dKGY4DTdzXI4AVaZR5SMK2cRL5hhTmG4FPIsU5T38Bnx9Ed/mlPu/f3iTrSHxPgXbADvf9ypPS+5xkv0/YB7cnfE5plC3F74P7Oh9OMt+E09YgV7Bv/OwzrRHOvl3WZ9xRLv0x9P3xK4JztJNQG1agFs736QyXHnHcQApHb4H6y03nSO9Q1TCcL3NdoLQ7vipwl4icSPjDOWQsj1MTO6aqx5NZX1XgqSTLVcb5RU7qS+AGESmP8wsbD0T6rOdtn3Ucw9k5Kvosvy+VclUAfk0YUNV4d/6Ulv/VJ0Z/ynDJtkWkn4hE+czfkD/fS38cVdVYn+GzOF+SMji1MN/tpVbuyjiHkSn5PZltACAiQ0Vkm4icdMtQjEvLkLTMdURkjoj8LiKngFd95k8rDl9VcRLRQZ/3732cmmmy2/alqktwTiuMAQ6JyDgRKerntv2NM7XvA6oag5PkGgJvqpu5wK994w+f1+fc9SUdV8RnOPG9UKdh+BiXf7/K4BzBrPPZ7jfu+CyTmxIpAKq6HGdH+Lc7ah/OL3Bxn7/CqvpPd1pJESmezKr2Af9IslwhVf0smW0eB77FOdy5B+eXVn3W80iS9RRU1dW+q0ilSL/h7PwAiIjgfGkO+MxT2ed1FXcZf8vg+0WpCnwADMY5LCyOc9pA/IgzLYdxDv0qpRB3UvuAmle6ERG5Cef0Ry+cI43iwEn+LANcXo7/Av/DaSUuinOuMWH+fUCNFDaXdD37cGqkpX3e76Kq2iCVZS5doeo7qnotzqmPOjiH7Gkuh//vV2rfB0SkIvB3nHPtb4pIAXd8WvtGeiR+/iJSBOfQ/bck8xzBScANfOItpk7DcpbJdYnU9RYQLiJNcBoVuopIBxEJEZFQEWkjIpVU9SDOofd7IlJCRPKJyM3uOj4A/iIiLcRRWERuE5GwFLb5KdAP6Om+TjAWeE5EGgCISDERuesKyjIduE1EbhWRfDjn6i7gNBYkGCQilUSkJDAc55xvespQGOcLe9iN9QGcWkeCP4BKIpL/CuIHQFXjgBnACBEpJCJ1cd6vlEwB2otILxHJKyKlRKSpH5sKw0nYh4G8IvIikFatLgyncee0G9ejPtPmAOVFZIiIFBCRMBFp4U77A6gmInncMh7E+UF9U0SKikgeEakpIrf4ETcicp37WeXDOZw9j3N0k7CtlBI6wHhgpIjUdj/rxiJSKpn5Uvw+uD/SE3AaywbgnBse6S6X1r6RHp1FpLW7P40EvlfVS2rs7hHYB8AoEbnK3XZFEemQwW1fkVyZSFX1MDAJeNH9YLrh1DIO4/wiP82f7819OOfu/odzPm+Iu461wMM4h1rHcRp4+qey2dlAbeB3Vf3JJ5aZwOvAVPewcTPQ6QrKsh2n8eRdnF/nrjiXel30me1TnC/wLziHd6+kpwyquhV4E6cF+w+c81yrfGZZgnP1wO8icsTfMvgYjHOY/TswGfgM50chuVj24pz7fArnkC8KpwElLQtwDv124JzmOE/qpxAAhuIcSUTjfGkTfohQ1WicBpmubtw/A23dyZ+7/4+KyHr3dT8gP39eRfEF7mGzH4q62z/uxn4Up+ESnORW3z28/SqZZf+D86P7Lc6Pwoc4DUaXSOP78DjOaYj/c4+oHgAeEJGb/Ng30uNTnNrvMZwGv5Sux30GZ9/93v0OLcJpVMsyueKC/NxMnJsRHlLVRV7HcqVE5HWgnKre73UsJmtJkN1gkCtrpCZ7EpG67iGniMj1OIePM72Oy5i02J0TJjsJwzmcr4BzePgmMMvTiIzxgx3aG2NMBtmhvTHGZJAlUmOMyaCgO0daunRprVatmtdhGGNymHXr1h1R1XTdERV0ibRatWqsXbvW6zCMMTmMiPya9lzJs0N7Y4zJIEukxhiTQZZIjTEmgyyRGmNMBlkiNcaYDLJEaowxGWSJ1BhjMihgiVREPhKRQyKyOYXpIiLviMhOEdkoItcEKhZjjAmkQNZIJ+A8kjYlnXA6Oq6N8zjZ/wYwFmOMCZiAJVJVXYHTs3VKugGT1PE9UFych8MZY0xQ8fIW0Ypc+oiH/e64g96EE7xi4+I5fPoCv504z5HTFzh1LoZT52M5fT6WMxdjOXMhlrMX4zh70fl/ISae87FxxFsXithbYO9BZgiKe+1FZCDO4T9VqlTxOBpvxMTFc+D4OfYeO8uOP6LZd+ws2w5Gs//4Wf6IvkBcfPLfhgJ581CkQF4K5g+hcH7nf8F8IZQsnJ8QycgDHnMOexsgYw/7DF7xcbH8b+kM6ra9M0Pr8TKRHuDSx+1W4tJHCCdS1XHAOIDmzZvn+N/P8zFx7Dl6hk37T7Jh3wnmbzrI8bMxl8xTpEBe6pQtQsuapShXNJQKxQtSvlgoV4WFUqxgPooVzEfhAiHkDbELM4xJzvnz5+nTpw/fffUVz/RsxccZWJeXiXQ2MFhEpgItgJPu42pznQMnzhG19wSbfzvJ2j3H+HHP8cRpYQXyUq9CUWqULkzNMkVoWLEYtcsWoVTh/IhVpYxJl+joaLp168bSpUt555136NatW4bWF7BEKiKfAW2A0iKyH+exqvkAVHUsMA/ncbo7gbM4j3bN8eLjla0HT7F8x2G2HjzFuj3H+f3UeQBC8ggNKxbjkVtqUPuqMJpUKkbNMkXIk8cSpjGZ5ciRI3Tu3Jn169czefJk7r03pac8+y9giVRV+6QxXYFBgdp+dnLw5Dm+XLefH3YfY9XOIySczixZOD+ta5WmUcViNK1SnEYVixGaL8TbYI3J4fbs2cPu3buZOXMmXbt2zZR1BkVjU7A5cyGWyJ8P8+Oe4yzc+gd7j51NnHZ9tZJ0blSO8AblqFi8oIdRGpO7HD9+nBIlStC8eXN2795NkSJFMm3dlkgzyaHo86zYcYSvf/qN5TsOJ46/vlpJelxTifD6ZalXPszOaxrjgQ0bNtCxY0dGjhzJwIEDMzWJgiXSDDl1Pob5mw4yb9PvicmzQrFQHmxVnRtqluLmOqUpkNcO1Y3xUmRkJF26dKFYsWK0adMmINuwRJoOZy7E8u6SnYxdvgtwWtZ7XFOJe1tWoUml4tY4ZEw2MXfuXHr27Em1atX49ttvqVy5ctoLpYMl0itwPiaO//tqM7N++o2LsfFE1C9L35ZVaV2rNCGWPI3JVvbs2UP37t1p3Lgx8+fPp0yZdD0g1C+WSP0QGxfPJ9//ynvLdnEo+gKNKhbj6Q5Xc3OdwH0wxpiMqVatGp988gkdO3akaNGiAd2WJdJUnI+J490lPzNx9a+cvhDLDTVKMabvNVxXraTXoRljkqGqvPbaa9x44420adOGXr16Zcl2LZEmQ1WZv/l3Xp23jf3Hz1G1VCFe7FKfu5pXslZ3Y7Kp+Ph4nnjiCd555x0GDRoUsIal5FgiTWLT/pOMnLOVNXuOUbdcGJ8MaEHr2qW9DssYk4qYmBgGDBjA5MmTGTJkCG+++WaWbt8SqevI6Qu8Pv9/fL5uP6UK5+fV7o3ofV1la0QyJpu7cOECvXr1Yvbs2YwcOZLhw4dn+ZGjJVJg+Y7DPDX9J46duUDv5pUZ3qUeRUPzeR2WMcYP+fLlo2jRoowePZpBg7y56zxXJ9JjZy7ywlebmLfpd2qUKczkAddTr3xgW/eMMZnj8OHDnDt3jipVqjBp0iRP2y9ybSL9cc8xhkyN4sCJc9x/Q1We61zPOgwxJkjs27eP8PBwQkNDWb9+PXnyeNvvbq5LpPHxyoTVe/jXgv+RN08eZvz1Rq6pUsLrsIwxftq+fTvh4eGcPHmSOXPmeJ5EIZcl0j9OnWfQlPWs/fU4t9Qpw5u9mlC6SAGvwzLG+Gn9+vV07NgREWHZsmU0a9bM65CAXJRI9xw5w/0fr+Fw9AVGdK3P/TdWs2tCjQkiqsrQoUMpVKgQCxcupHbt2l6HlChXJNJ1vx6n/0drEIEJD1zP9dXtziRjgomqIiJMmzaNCxcuUKlSJa9DuoT3JxcCbOaG/fQd/z1hoXmZ89hNlkSNCTKTJ0/mjjvu4OLFi5QpUybbJVHI4Yl0+tp9PDHtJ+qVL8qswa2pUqqQ1yEZY67AO++8Q79+/Th9+jQXL170OpwU5dhEumLHYYbP3MSNNUsxbeANlAmzRiVjgoWqMmLECP72t7/RvXt35s6dm+m92memHJlI//f7Ke7/eA21rgrj3T7NyJ83RxbTmBzr73//Oy+99BIPPPAA06dPJzQ01OuQUpXjGpt2HzlD3w9+oHzRUMbf35xSdnmTMUGne/fuxMfHM3LkyKC4uiZHVdVOnovh4UlruRAbz6QB19tTOo0JIufOneOTTz4BoFmzZrzyyitBkUQhByVSVeXhSWvZeeg04/pdS62rwrwOyRjjp5MnT9KxY0f69evHTz/95HU4VyzHHNrP2XiQNbuP8XznutxY0/oPNSZYHDp0iI4dO7Jp0yY+++wzmjRp4nVIVyxHJNKLsfG8sWA7NcoU5sFW1b0Oxxjjp19//ZWIiAj27dvH7Nmz6dSpk9chpUuOSKQfRP7C3mNn+W/fa8gbkmPOVhiT461du5YjR46wcOFCWrVq5XU46Rb0WefU+Rj+s3AHba8uQ6dG5b0Oxxjjh+joaAB69OjBrl27gjqJQg5IpJO/+5W4eGVwu1peh2KM8cPSpUupXr06S5YsAaB48eLeBpQJgjqRno+JY9J3e2hYsaj1KWpMEPjqq6/o1KkT5cqVo27dul6Hk2mCOpHOWH+AP05dYGjE1UFzvZkxudXEiRPp0aMHTZs2ZcWKFVSoUMHrkDJNUCfSL9bto07ZItxSp4zXoRhjUhEZGUn//v1p164dixYtomTJnNULW9Am0m0HT7F+7wnuuray1UaNyeZat27N2LFjmTNnTrbufCS9gjaRztxwAIDOja2l3pjsKD4+nuHDh7Nr1y5EhEceeYQCBXJm3xdBmUhVlVlRB2hfr6zdT29MNhQTE8O9997Lq6++yowZM7wOJ+CCMpFuPXiKP05dIKJBWa9DMcYkcfbsWe644w4+++wz/vnPf/L00097HVLABeWdTZ+v3Q9AG2tkMiZbOXnyJF26dGHVqlWMGzeOhx9+2OuQskRQJtLvfzlKpRIFuapo9u7s1ZjcJiQkhDx58jB16lR69erldThZJugSqSps/yOax9ranUzGZBe//vorJUuWJCwsjGXLluW6K2mC7hzpuZg4VKFu+aJeh2KMAbZu3cqNN97Igw8+CJDrkigEYSK9GBsPQK2rct61aMYEmzVr1nDTTTcRHx/Piy++6HU4ngm+RBrnJNLKJezRysZ4afHixbRr147ixYuzatUqGjVq5HVIngm6RHohJo6KxQtSMH+I16EYk2tdvHiRhx9+mOrVq7Ny5Upq1KjhdUieCmgiFZGOIrJdRHaKyLPJTK8iIktFZIOIbBSRzmmtMyZOqVDcWuuN8VL+/PmZN28ey5cvp3x5u7swYIlUREKAMUAnoD7QR0TqJ5ntBWC6qjYD7gbeS2u9MXHxlLXLnozxxJtvvsmTTz6JqlK3bt0c1/lIegWyRno9sFNVf1HVi8BUoFuSeRRIaH4vBvyW1kovxsVTJixn3q9rTHalqgwfPpyhQ4eyf/9+4uLivA4pWwnkdaQVgX0+w/uBFknmGQF8KyKPAYWB9v6sOD5eMyM+Y4wf4uLiGDRoEO+//z4DBw7kvffeIyTE2ih8ed3Y1AeYoKqVgM7AZBG5LCYRGSgia0VkLUDZYnZob0xWefDBB3n//fd57rnnGDt2rCXRZASyRnoAqOwzXMkd52sA0BFAVb8TkVCgNHDIdyZVHQeMAyhQvraWLJQ/UDEbY5Lo1q0bjRo1YujQoV6Hkm0FMpH+CNQWkeo4CfRu4J4k8+wFbgUmiEg9IBQ4nNaKSxWxc6TGBNLx48f57rvv6Ny5M3feeafX4WR7AUukqhorIoOBBUAI8JGqbhGRl4G1qjobeAr4QESewGl46q+qaZ4ALRoadF0EGBM0Dh48SIcOHdi1axe7d+/mqquu8jqkbC+gGUlV5wHzkox70ef1VuCKH2hduIAlUmMC4ZdffiE8PJw//viDWbNmWRL1U1BmJEukxmS+zZs3ExERwfnz51m8eDEtWiS9yMakJCgzUmg+ry82MCbnmT17NiJCZGQkDRo08DqcoCJ+nJLMVgqUr62/79xCicLWcm9MZjh37hwFCxZEVTly5AhlyuTOJ0+IyDpVbZ6eZYOyamcdlhiTOb744gtq1arF9u3bEZFcm0QzKigTab6QoAzbmGzlgw8+oHfv3lSrVs0alTIoKDNSSJ7c1wO3MZnp9ddfZ+DAgURERPDtt99SokQJr0MKakGXSC2FGpMxkyZN4tlnn+Xuu+9m1qxZFC5c2OuQgl7wtdpbJjUmQ+666y6OHTvGY489ZvfNZ5Kgq5EG2UUGxmQLFy5c4Pnnn+fEiRMULFiQIUOGWBLNREGXSO38qDFX5vTp03Tt2pXXXnuNb775xutwcqTgO7Q3xvjt2LFj3HbbbaxZs4aPPvqIu+++2+uQciRLpMbkUAcPHiQiIoIdO3bwxRdf0L17d69DyrEskRqTQ8XGxhIfH8/8+fNp166d1+HkaJZIjclhdu/eTZUqVahcuTIbN260RqUsEHSNTcaYlK1evZprrrmGF190equ0JJo1LJEak0MsWLCA8PBwypQpw8CBA70OJ1exRGpMDjB9+nS6du1KnTp1iIyMpGrVql6HlKtYIjUmyB0+fJgHH3yQli1bsmzZMsqWLet1SLmONTYZE+TKlCnDokWLaNy4MYUKFfI6nFzJaqTGBCFVZdiwYYwfPx6Ali1bWhL1kCVSY4JMbGwsDz30EG+88QabNm3yOhyDJVJjgsqFCxfo3bs3H330ES+++CJvvfWW1yEZruAcqYgUUtWzgQzGGJOy2NhYunTpwqJFixg1ahRDhgzxOiTjSrNGKiI3ishW4H/ucBMReS/gkRljLpE3b17atWvHxIkTLYlmM2k+RVREfgB6ArNVtZk7brOqNsyC+C5TqGIdPXtghxebNsYTBw4c4LfffuO6667zOpQcLSNPEfXr0F5V94lc0g9oXHo2Zoy5Mj///DPh4eGoKj///DP589tjyLMjfxLpPhG5EVARyQf8DdgW2LCMMVFRUXTo0IH4+Hi++eYbS6LZmD+t9n8BBgEVgQNAU+CvAYzJmFxv5cqVtGnThvz58xMZGcm1117rdUgmFf7USK9W1b6+I0SkFbAqMCEZY95//33Kli3LwoULqVKlitfhmDT409i0XlWvSWtcVrHGJpOTXbx4kfz583PhwgWio6MpXbq01yHlGgFpbBKRG4AbgTIi8qTPpKKAZ50c2qPvTE713//+l/fee4/ly5dTsmRJChQo4HVIxk+pnSPNDxTBSbZhPn+ncC6HMsZkAlXlH//4B3/961+pXr06BQsW9Dokc4VSrJGq6nJguYhMUNVfszAmY3KN+Ph4hg4dyqhRo7jvvvv48MMPyZcvn9dhmSvkT2PTWRF5A2gAhCaMVFV7mpYxGTRy5EhGjRrF448/zqhRo8iTx7q/CEb+JNIpwDSgC86lUPcDhwMZlDG5xcCBAylevDiPP/44SW56MUHEn5+/Uqr6IRCjqstV9UHAaqPGpFN0dDQjR44kNjaW8uXL87e//c2SaJDzp0Ya4/4/KCK3Ab8BJQMXkjE515EjR+jUqRMbNmygXbt2tGrVyuuQTCbwJ5G+IiLFgKeAd3EufxoSyKCMyYn27dtHREQEe/bs4auvvrIkmoOkmUhVdY778iTQFhLvbDLG+GnHjh2Eh4dz4sQJFixYwM033+x1SCYTpXZBfgjQC+ce+29UdbOIdAGeBwoCzbImRGOC3/HjxwkJCWHZsmU0a2ZfnZwmxVtERWQCUBlYA7TAOTfaHHhWVb/KovguU7hiHT1jt4iaILFv3z4qV64MQExMjF0jmo1l5BbR1FrtmwPhqvoc0Bnn8qdWXiZRY4LJ3LlzqVOnDpMnTwawJJqDpZZIL6pqPICqngd+UdWjWROWMcFtypQp3HHHHTRs2JBOnTp5HY4JsNQSaV0R2ej+bfIZ3iQiG/1ZuYh0FJHtIrJTRJ5NYZ5eIrJVRLaIyKfpKYQx2cno0aO59957uemmm1iyZIn14JQLpNZqXy8jK3Ybq8YA4cB+4EcRma2qW33mqQ08h3PK4LiIXJWRbRrjtZ9++onHHnuMbt26MXXqVEJDQ9NeyAS91DotyWhHJdcDO1X1FwARmQp0A7b6zPMwMEZVj7vbPJTBbRrjqSZNmjB//nzat29P3rx+P+3cBLlA9pBQEdjnM7zfHeerDlBHRFaJyPci0jG5FYnIQBFZKyJr4+PjAxSuMekTExPDwIEDWb58OQAdO3a0JJrLeN3VTF6gNtAG6AN8ICLFk86kquNUtbmqNrfecUx2cu7cOXr06MEHH3zAmjVrvA7HeMSvrCQiBUXk6itc9wGc61ATVHLH+doPzFbVGFXdDezASazGZHunTp2iU6dOzJkzhzFjxvD00097HZLxSJqJVES6AlHAN+5wUxGZ7ce6fwRqi0h1EckP3A0kXe4rnNooIlIa51D/Fz9jN8YzJ0+epG3btqxatYopU6bw17/ag3VzM39qpCNwGo5OAKhqFFA9rYVUNRYYDCwAtgHTVXWLiLwsIre7sy0AjorIVmAp8LRdq2qCQVhYGM2aNWPWrFn06dPH63CMx/x5iuj3qtpSRDaoajN33EZVbZwlESZht4gaL23fvp3Q0FCqVq3qdSgmkwXqFtEEW0TkHiBERGqLyLvA6vRszJhgtm7dOlq3bs19991HWhUQk7v4k0gfw3le0wXgU5zu9IYEMCZjsp1ly5bRtm1bChcuzIcffmg92ptL+HOxW11VHQ4MD3QwxmRHs2fPplevXtSsWZNvv/2WihWTXg5tcjt/zpEuBcoBXwDTVHVzVgSWEjtHarJSfHw8N9xwA6rK/PnzKVWqlNchmQDJyDlSf3rIbysi5XA6eX5fRIriJNRX0rNBY4JFbGwsefPmZc6cOYSGhhIWFuZ1SCab8uuCfFX9XVXfwXkccxTwYiCDMsZLqsqLL75It27duHjxImXKlLEkalLlzwX59URkhNuVXkKLfaWAR2aMB+Lj43n88ccZOXIk5cqVw25JNv7wp7HpI2Aa0EFVfwtwPMZ4JiYmhgceeIApU6bw1FNP8cYbb1jrvPGLP+dIb8iKQIzx2sCBA5kyZQqvvvoqzz77rCVR47fUniI6XVV7uYf0vk37AqhXdzYZEyiPP/44N954Iw8//LDXoZggk9pTRMur6kERSfZeuEzo+Dld7PInk5kOHTrEF198YZ2OmMDcIqqqB92Xf1XVX33/ANvrTND79ddfad26NUOHDmXPnj1eh2OCmD9NkuHJjLPHIpqgtnXrVlq1asXhw4dZuHAh1apV8zokE8RSO0f6KE7Ns0aSp4aGAasCHZgxgfLjjz/SqVMn8ubNy/Lly2nc2E73m4xJrdX+U2A+8Brg+yjlaFU9FtCojAmgXbt2UaxYMRYsWECtWrW8DsfkAKk1NhVV1VMiUjK56V4lU2tsMun1xx9/ULZsWQDOnz9vj0o2lwhUf6Sfuv/XAWvd/+t8ho0JGhMmTKB69eqsWuWclbIkajJTas+17+L+T/OxIsZkZ//5z3946qmnCA8Pp0mTJl6HY3Igf+61byUihd3X94rIf0SkSuBDMyZjVJUXXniBp556ip49e/L1119TpEgRr8MyOZA/lz/9FzgrIk2Ap4BdwOSARmVMJpg5cyb/+Mc/eOihh5g6dSoFChTwOiSTQ/mTSGPVaZHqBoxW1TE4l0AZk611796dzz//nHHjxhESEuJ1OCYH8yeRRovIc8B9wFwRyQPkC2xYKROsIwmTsrNnz9K/f3927dqFiNCzZ0/rfMQEnD+JtDfOg+8eVNXfcfoifSOgURmTDidOnCAiIoJJkybxww8/eB2OyUXSTKRu8pwCFBORLsB5VZ0U8MiMuQJ//PEHbdq0Yc2aNUybNo177rnH65BMLuJPq30vYA1wF85zm34QkZ6BDswYf+3bt4/WrVvz888/M2fOHO666y6vQzK5jD895A8HrlPVQwAiUgZYhPNUUWM8V6JECWrXrs2kSZO44Qbrh9xkPX8SaZ6EJOo6ip8PzTMmkDZs2ECtWrUICwtj3rx5XodjcjF/EuI3IrJARPqLSH9gLmB7rfHUokWLuOmmmxgyZIjXoRjj1zObnhaRO4HW7qhxqjozsGGlwq5kyfVmzJhBnz59uPrqq3nllVe8DseYVPsjrQ38G6gJbAKGquqBrArMmOR89NFHPPzww7Ro0YK5c+dSokQJr0MyJtVu9CKBScAKoCtwo6remYWxJatIpav19P7tXodhPBAdHU3dunVp1KgRX375JYULF/Y6JJODZKQbvdQO7cNU9QP39XYRWZ+eDRiTUQk/9mFhYURGRlKpUiXy58/vcVTG/Cm1RBoqIs3486xkQd9hVbXEagIuLi6OQYMGUahQId58801q1KjhdUjGXCa1RHoQ+I/P8O8+wwq0C1RQxgBcvHiR++67j+nTp/Pcc895HY4xKUqtY+e2WRmIMb7OnDlDjx49WLBgAW+88QZDhw71OiRjUuTPBfnGZClVpWvXrixfvpzx48czYMAAr0MyJlUpttpnV9ZqnzvMmDEDVaVHjx5eh2JyiUC12huTpX755ReioqK48847ufNOz6+0M8ZvaSZScXrF7QvUUNWX3ec1lVPVNQGPzuQamzZtokOHDsTHxxMREWHPVjJBxZ977d8DbgD6uMPRwJiARWRyne+++46bb74ZEWHJkiWWRE3Q8SeRtlDVQcB5AFU9DtjV0CZTfPvtt7Rv357SpUuzatUq6tev73VIxlwxfxJpjIiE4Fw7mtAfaXxAozK5xqpVq6hVqxaRkZFUq1bN63CMSRd/Euk7wEzgKhH5B7ASeNWflYtIRxHZLiI7ReTZVObrISIqIulqMTPB59ixYwCMGDGC1atXU65cOY8jMib9/Hlm0xRgGPAazt1Od6jq52kt59ZixwCdgPpAHxG57LhNRMKAvwH2tLJc4vXXX6du3brs3r0bEbHOR0zQ8+eZTVWAs8DXwGzgjDsuLdcDO1X1F1W9CEwFuiUz30jgddxzsCbnUlWeeeYZnn32Wdq3b0/FihW9DsmYTOHPdaRzcc6PChAKVAe2Aw3SWK4isM9neD/QwncGEbkGqKyqc0XkaX+DNsEnLi6Ov/zlL4wfP55HH32U0aNHkyePPbHG5Az+9JDfyHfYTX5/zeiGRSQPTico/f2YdyAwEKBguZoZ3bTxwKhRoxg/fjwvvPACL7/8Ms7lycbkDFd8Z5OqrheRFmnPyQGgss9wJXdcgjCgIbDM/VKVA2aLyO2qujbJNscB48C5RfRKYzbeGzRoEJUqVeLuu+/2OhRjMp0/dzY96TOYB7gG+M2Pdf8I1BaR6jgJ9G7gnoSJqnoSKO2znWU4jzNZi8kRjh07xrBhw3jzzTcpVqyYJVGTY/lzkirM568AzjnT5BqNLqGqscBgYAGwDZiuqltE5GURuT39IZtg8Ntvv3HzzTczefJk1q+3PsBNzpZqjdS9hClMVdPVGaSqziPJo5tV9cUU5m2Tnm2Y7GfXrl20b9+eI0eOMH/+fNq2ta5tTc6W2lNE86pqrIi0ysqATHDbvHkz4eHhxMTEsGTJEq677jqvQzIm4FKrka7BOR8aJSKzgc+BMwkTVXVGgGMzQaho0aLUqFGD8ePHU69ePa/DMSZL+NNqHwocxXlGU8L1pApYIjWJ1q9fT5MmTahSpQorV660y5tMrpJaY9NVbov9ZmCT+3+L+39zFsRmgsS0adNo2bIlb7zxBoAlUZPrpFYjDQGK8OfjmH3ZtZwGgPfff59HH32U1q1b8+ijj3odjjGeSPVxzKr6cpZFYoKKqvLPf/6T559/ni5dujB9+nQKFizodVjGeCK1Q3s7PjMp2r17Ny+//DJ9+/ZlxowZlkRNrpZajfTWLIvCBA1VRUSoUaMGa9asoUGDBtb5iMn1UvwGqOqxrAzEZH/nz5+nZ8+efPjhhwA0atTIkqgx+HeLqDFER0dz2223MWPGDM6cOZP2AsbkIvZce5Omo0eP0qlTJ9avX8/EiRPp16+f1yEZk61YIjWpOnv2LDfffDO7du1ixowZ3H679TdjTFKWSE2qChUqxAMPPEDz5s1p06aN1+EYky2JanBdWx9W6WqN3r/d6zByvKioKM6fP0/Lli29DsWYLCEi61Q1XU8ythqpuUxkZCRdunShWrVqbNiwwVrmjUmDfUPMJebOnUtERATlypXj66+/tiRqjB/sW2ISffrpp9xxxx3Ur1+fyMhIqlTx56nbxhhLpAZw7lj66quvaNWqFUuXLuWqq67yOiRjgoadI83lVJXo6GiKFi3K5MmTiY+Pt/vmjblCViPNxeLj43niiSe48cYbOXnyJAUKFLAkakw6WCLNpWJjY3nwwQd5++23ufXWWwkLC/M6JGOCliXSXCih85GJEyfy0ksv8dZbb1nrvDEZYOdIc6EnnniCWbNm8e677zJ48GCvwzEm6NmdTbnQb7/9xnfffUePHj28DsWYbCMjdzbZ8VwusX//fp566iliY2OpUKGCJVFjMpEl0lxgx44dtGrVivHjx7Njxw6vwzEmx7FEmsOtX7+e1q1bc+7cOZYtW0b9+vW9DsmYHMcSaQ4WGRlJ27ZtKViwICtXrqRZs2Zeh2RMjmSJNAfLly8fderUYdWqVdSpU8frcIzJsazVPgfavHkzDRs2BP586qcxJnXWam8SvfvuuzRu3JgZM2YAWBI1JgtYIs0hVJWXXnqJxx9/nG7dutG5c2evQzIm17A7m3KAhM5H3nnnHfr3788HH3xA3rz20RqTVaxGmgOsWLGCd955hyeeeIIPP/zQkqgxWcy+cUEsoSGpTZs2fPfdd7Ro0cLOiRrjAauRBqmTJ0/SuXNnVqxYAUDLli0tiRrjEUukQejQoUO0bduWRYsWcfDgQa/DMSbXs0P7ILN3717Cw8PZt28fs2bNstZ5Y7IBS6RB5MCBA7Rq1Yro6Gi+/fZbWrdu7XVIxhjs0D6olC9fnu7du7N8+XJLosZkI3aLaBBYsWIFVatWpWrVql6HYkyOZbeI5mCzZ88mIiKCIUOGeB2KMSYFwZdIc9EVPpMmTeLOO++kSZMmjB8/3utwjDEpCGgiFZGOIrJdRHaKyLPJTH9SRLaKyEYRWSwiduzqevvtt7n//vtp06YNixcvplSpUl6HZIxJQcASqYiEAGOATkB9oI+IJO2efQPQXFUbA18A/wpUPMHkwoULTJw4kTvvvJO5c+dSpEgRr0MyxqQikJc/XQ/sVNVfAERkKtAN2Jowg6ou9Zn/e+DeAMaT7cXHx3Px4kVCQ0NZvHgxYWFhdt+8MUEgkIf2FYF9PsP73XEpGQDMD2A82VpMTAz9+vWjR48exMXFUaJECUuixgSJbNHYJCL3As2BN1KYPlBE1orI2ri4uKwNLgucPXuW7t27M2XKFG666Sby5MkWH4sxxk+BrPIcACr7DFdyx11CRNoDw4FbVPVCcitS1XHAOICwylcH14WvaThx4gRdu3Zl1apVvP/++wwcONDrkIwxVyiQifRHoLaIVMdJoHcD9/jOICLNgPeBjqp6yJ+V5rSrn3r37s0PP/zA1KlT6dWrl9fhGGPSIWCJVFVjRWQwsAAIAT5S1S0i8jKwVlVn4xzKFwE+d7uA26uqtwcqpuzotdde4/Dhw3To0MHrUIwx6RR0t4gWrXy1ntoX3LeIbtu2jblz5zJ06FCvQzHGuDJyi6g1C2exH3/8kU6dOpEvXz769+9P6dKlvQ7JGJNB1jychRYvXky7du0oWrQoK1eutCRqTA5hiTSLzJw5k86dO1OtWjVWrlxJzZo1vQ7JGJNJLJFmkbNnz9K8eXOWL19OhQoVvA7HGJOJrLEpwHbu3EmtWrUAiIuLIyQkxOOIjDHJsf5IsyFV5YUXXqBBgwZs2LABwJKoMTmUtdoHQFxcHIMHD2bs2LE89NBDNG7c2OuQjDEBZDXSTHbx4kX69u3L2LFjeeaZZxg3bpzVRI3J4axGmskmTZrEtGnTeP311xk2bJjX4RhjsoAl0kw2YMAAatWqRZs2bbwOxRiTRezQPhP8/vvvdOzYkV27diEilkSNyWUskWbQ7t27ad26NZGRkezdu9frcIwxHrBD+wzYsmUL4eHhnD9/nsWLF9OyZUuvQzLGeMASaTpt2rSJNm3aUKBAAVasWEHDhg29DskY4xE7tE+n6tWrExERwcqVKy2JGpPLWSK9QgsXLuT06dMUKVKEzz77jBo1angdkjHGY5ZIr8CHH35Ix44deemll7wOxRiTjVgi9dMbb7zBQw89REREBCNGjPA6HGNMNmKJNA2qynPPPcewYcPo3bs3s2bNonDhwl6HZYzJRiyRpuHQoUNMnDiRRx55hClTppA/f36vQzLGZDN2+VMKYmJiCAkJoWzZsqxbt45y5crhPunUGGMuYTXSZJw5c4YuXbrw9NNPA1C+fHlLosaYFFkiTeLYsWOEh4ezaNEiGjRo4HU4xpggYIf2Pg4ePEhERAQ7duzg888/58477/Q6JGNMELBE6oqNjeXWW29l7969zJs3j1tvvdXrkHKMmJgY9u/fz/nz570OxRhCQ0OpVKkS+fLly7R1WiJ15c2bl1dffZXy5cvTokULr8PJUfbv309YWBjVqlWzc83GU6rK0aNH2b9/P9WrV8+09eb6c6Tfffcd06ZNA+COO+6wJBoA58+fp1SpUpZEjedEhFKlSmX60VGuTqQLFiygffv2vPTSS8TExHgdTo5mSdRkF4HYF3NtIp0+fTpdu3alTp06LF26NFPPlxhjcpdcmUjHjRvH3XffTYsWLVi6dClly5b1OiQTYCEhITRt2pSGDRvStWtXTpw4kThty5YttGvXjquvvpratWszcuRIVDVx+vz582nevDn169enWbNmPPXUUx6UIHUbNmxgwIABXoeRogsXLtC7d29q1apFixYt2LNnT7Lzvf322zRs2JAGDRrw1ltvJY6PioqiZcuWNG3alObNm7NmzRrA6QOjadOmiZ9tSEgIx44d4+LFi9x8883ExsZmQelwTr4G019YpTqaUcOHD9dOnTrpmTNnMrwuk7atW7d6HYIWLlw48XW/fv30lVdeUVXVs2fPao0aNXTBggWqqnrmzBnt2LGjjh49WlVVN23apDVq1NBt27apqmpsbKy+9957mRpbTExMhtfRs2dPjYqKytJtXokxY8boI488oqqqn332mfbq1euyeTZt2qQNGjTQM2fOaExMjN566636888/q6pqeHi4zps3T1VV586dq7fccstly8+ePVvbtm2bODxixAj95JNPko0nuX0SWKvpzEu5ptVeVdm/fz+VK1dm5MiRxMXFkTdvril+tvHS11vY+tupTF1n/QpF+XtX/2+euOGGG9i4cSMAn376Ka1atSIiIgKAQoUKMXr0aNq0acOgQYP417/+xfDhw6lbty7g1GwfffTRy9Z5+vRpHnvsMdauXYuI8Pe//50ePXpQpEgRTp8+DcAXX3zBnDlzmDBhAv379yc0NJQNGzbQqlUrZsyYQVRUFMWLFwegdu3arFy5kjx58vCXv/wl8Xlgb731Fq1atbpk29HR0WzcuJEmTZoAsGbNGv72t79x/vx5ChYsyMcff8zVV1/NhAkTmDFjBqdPnyYuLo558+bx2GOPsXnzZmJiYhgxYgTdunVjz5493HfffZw5cwaA0aNHc+ONN/r9/iZn1qxZib2m9ezZk8GDB6Oql5yv3LZtGy1atKBQoUIA3HLLLcyYMYNhw4YhIpw65ew3J0+epEKFCpdt47PPPqNPnz6Jw3fccQfPPfccffv2zVDs/sgVmSQuLo5HHnmEWbNmsXHjRsqXL29JNJeKi4tj8eLFiYfBW7Zs4dprr71knpo1a3L69GlOnTrF5s2b/TqUHzlyJMWKFWPTpk0AHD9+PM1l9u/fz+rVqwkJCSEuLo6ZM2fywAMP8MMPP1C1alXKli3LPffcwxNPPEHr1q3Zu3cvHTp0YNu2bZesZ+3atZc8paFu3bpERkaSN29eFi1axPPPP8+XX34JwPr169m4cSMlS5bk+eefp127dnz00UecOHGC66+/nvbt23PVVVexcOFCQkND+fnnn+nTpw9r1669LP6bbrqJ6Ojoy8b/+9//pn379peMO3DgAJUrVwacSw2LFSvG0aNHKV26dOI8DRs2ZPjw4Rw9epSCBQsyb948mjdvDjg/IB06dGDo0KHEx8ezevXqS9Z/9uxZvvnmG0aPHn3J+n788cc0P4fMkOOzyYULF+jbty9ffvklL7zwAuXKlfM6pFztSmqOmencuXM0bdqUAwcOUK9ePcLDwzN1/YsWLWLq1KmJwyVKlEhzmbvuuouQkBAAevfuzcsvv8wDDzzA1KlT6d27d+J6t27dmrjMqVOnEp/QkODgwYOUKVMmcfjkyZPcf//9/Pzzz4jIJVekhIeHU7JkSQC+/fZbZs+ezb///W/AuUxt7969VKhQgcGDBxMVFUVISAg7duxINv7IyMg0y3gl6tWrxzPPPENERASFCxemadOmie/Pf//7X0aNGkWPHj2YPn06AwYMYNGiRYnLfv3117Rq1SqxbOAcPeTPn5/o6GjCwsIyNdakcnRj0+nTp+natStffvklo0aNYuTIkXYZTi5VsGBBoqKi+PXXX1FVxowZA0D9+vVZt27dJfP+8ssvFClShKJFi9KgQYPLpl8J3/0t6bWLvv3a3nDDDezcuZPDhw/z1VdfJd6eHB8fz/fff09UVBRRUVEcOHDgkiSaUDbfdf/f//0fbdu2ZfPmzXz99deXTPPdpqry5ZdfJq5779691KtXj1GjRlG2bFl++ukn1q5dy8WLF5Mt20033ZTY0OP755vgElSsWJF9+/YBzl2EJ0+epFSpUpfNN2DAANatW8eKFSsoUaIEderUAWDixImJ78ldd92V2NiUYOrUqZcc1ie4cOECoaGhycafmXJ0In3llVdYsmQJEyZMYMiQIV6HY7KBQoUK8c477/Dmm28SGxtL3759WblyZeKX/9y5czz++OMMGzYMgKeffppXX301sVYWHx/P2LFjL1tveHh4YnKGPw/ty5Yty7Zt24iPj2fmzJkpxiUidO/enSeffJJ69eolJpmIiAjefffdxPmioqIuW7ZevXrs3LkzcfjkyZNUrFgRgAkTJqS4zQ4dOvDuu+8mXqGwYcOGxOXLly9Pnjx5mDx5MnFxcckuHxkZmZiEff+SHtYD3H777UycOBFwzhW3a9cu2UrNoUOHANi7dy8zZszgnnvuAaBChQosX74cgCVLllC7du1Lyrt8+XK6det2yboSTh1kyaWN6W2l8urvSlrtz5w5o0uWLPF7fhMY2a3VXlW1S5cuOmnSJFVV3bhxo95yyy1ap04drVmzpo4YMULj4+MT5/3666/1mmuu0bp162q9evX06aefvmz90dHR2q9fP23QoIE2btxYv/zyS1VV/fzzz7VGjRraokULHTRokN5///2qqnr//ffr559/fsk6fvzxRwV0woQJieMOHz6svXr10kaNGmm9evUSW76TatiwoZ46dUpVVVevXq21a9fWpk2b6vDhw7Vq1aqqqvrxxx/roEGDEpc5e/asDhw4UBs2bKj169fX2267TVVVd+zYoY0aNdLGjRvrsGHDLnvv0uPcuXPas2dPrVmzpl533XW6a9cuVVU9cOCAdurUKXG+1q1ba7169bRx48a6aNGixPGRkZF6zTXXaOPGjfX666/XtWvXJk77+OOPtXfv3pdt8/PPP9cnn3wy2Xgyu9Ve1Od6uWBQtPLVemrf9hSn79y5k2HDhvHxxx9TrFixLIzMpGTbtm3Uq1fP6zBytFGjRhEWFsZDDz3kdSjZxp133sk///nPxNMDvpLbJ0Vknao2T8+2ctSh/U8//UTr1q1ZsWJF4uUixuQGjz76KAUKFPA6jGzj4sWL3HHHHckm0UDIMYl01apV3HLLLeTLl4/IyEgaNWrkdUjGZJnQ0FDuu+8+r8PINvLnz0+/fv2ybHtBl0iFy09QL1myhPDwcMqWLcuqVavsMDIbCrZTSCbnCsS+GHSJNDm1atUiPDycyMhIqlSp4nU4JonQ0FCOHj1qydR4TtXpjzSzL4kKusamYpXr6sl9/wNg4cKF3HrrreTJkyN+D3Is6yHfZCcp9ZCfkcamgN7ZJCIdgbeBEGC8qv4zyfQCwCTgWuAo0FtV96S1XlXltddeY/jw4YwdO5ZHHnkk84M3mSZfvnyZ2hu5MdlNwKpyIhICjAE6AfWBPiJSP8lsA4DjqloLGAW8ntZ6FWXo0KEMHz6ce++9lwcffDCzQzfGmCsSyGPi64GdqvqLql4EpgLdkszTDZjovv4CuFXSuIfz/LHf+c9//sNjjz3GxIkTrUNmY4znAplIKwL7fIb3u+OSnUdVY4GTwOU34PqIORvNiBEjePvtt+3cqDEmWwiK3p9EZCAw0B28MGLEiM0JfRvmQKWBI14HEUA5uXw5uWyQ88t3dXoXDGQiPQBU9hmu5I5Lbp79IpIXKIbT6HQJVR0HjAMQkbXpbVkLBla+4JWTywa5o3zpXTaQx8Y/ArVFpLqI5AfuBmYnmWc2cL/7uiewRIPteixjTK4XsBqpqsaKyGBgAc7lTx+p6hYReRmnl5XZwIfAZBHZCRzDSbbGGBNUAnqOVFXnAfOSjHvR5/V54K4rXO24TAgtO7PyBa+cXDaw8qUo6O5sMsaY7MauHzLGmAzKtolURDqKyHYR2SkizyYzvYCITHOn/yAi1TwIM938KN+TIrJVRDaKyGIRqepFnOmRVtl85ushIioiQdUS7E/5RKSX+/ltEZFPszrGjPBj36wiIktFZIO7f3b2Is70EJGPROSQiGxOYbqIyDtu2TeKyDV+rTi9XesH8g+ncWoXUAPID/wE1E8yz1+Bse7ru4FpXsedyeVrCxRyXz8aLOXzp2zufGHACuB7oLnXcWfyZ1cb2ACUcIev8jruTC7fOOBR93V9YI/XcV9B+W4GrgE2pzC9MzAfEKAl8IM/682uNdKA3F6ajaRZPlVdqqpn3cHvca7DDQb+fHYAI3H6Vgi2LqH8Kd/DwBhVPQ6gqoeyOMaM8Kd8ChR1XxcDfsvC+DJEVVfgXCGUkm6A8zAv1e+B4iJSPq31ZtdEGpDbS7MRf8rnawDOr2QwSLNs7uFSZVWdm5WBZRJ/Prs6QB0RWSUi37u9oAULf8o3ArhXRPbjXJXzWNaEliWu9LsJBMktormZiNwLNAdu8TqWzCAieYD/AP09DiWQ8uIc3rfBOZJYISKNVPWEl0Floj7ABFV9U0RuwLkWvKGqxnsdmFeya430Sm4vJbXbS7Mpf8qHiLQHhgO3q+qFLIoto9IqWxjQEFgmIntwzkPNDqIGJ38+u/3AbFWNUdXdwA6cxBoM/CnfAGA6gKp+B4Ti3IefE/j13UwquybSnH57aZrlE5FmwPs4STSYzrGlWjZVPamqpVW1mqpWwzn/e7uqpvs+5yzmz775FU5tFBEpjXOo/0sWxpgR/pRvL3ArgIjUw0mkh7M0ysCZDfRzW+9bAidV9WCaS3ndipZK61pnnF/yXcBwd9zLOF86cD68z4GdwBqghtcxZ3L5FgF/AFHu32yvY86ssiWZdxlB1Grv52cnOKcvtgKbgLu9jjmTy1cfWIXToh8FRHgd8xWU7TPgIBCDc+QwAPgL8Befz26MW/ZN/u6bdmeTMcZkUHY9tDfGmKBhidQYYzLIEqkxxmSQJVJjjMkgS6TGGJNBlkhTICJxIhLl81ctlXlPZ8L2JojIbndb6907Rq50HeNFpL77+vkk01ZnNEZ3PQnvy2YR+VpEiqcxf9Os6h3IJ7YK7vA/RGRfej4fERnjrmuriJzz2Q96ZmK8/UUkXkQa+4zbnNk9mSX9DETk9tR65bqC9fYXkcPu+/I/EXnCz2Uq+DHfGyLyu4gMzWicWcLr67qy6x9wOhDzprKOCUBP93UEsDGr4k/venE6jRmexvz9gdEBiCNvWmXGuWuqfEbeC6AayfQUlNz207Hu/jgXt0/zGbcZqJbJ71WgPoPE9eL0c3EEpw+F1JZZhp/XZuLc0z80s+MOxJ/VSP0kIkXE6Rd0vYhsEpHLejQSkfIissKnxnaTOz5CRL5zl/1cRIqksbkVQC132SfddW0WkSHuuMIiMldEfnLH93bHLxOR5iLyT6CgG8cUd9pp9/9UEbnNJ+YJItJTRELcWsCP4vTD+Igfb8t3uB06iMj1bhk3iMhqEbnavTPmZaC3G0tvN/aPRGSNO29y76O4sWx23+uE8rURkUgRmY1zsXuqVPV79eeuFD8l3b6IVBOffi1FZKiIjHBf1xSRb0RknbtM3RRWOwdoICKXPQo4pf1GRDq7NcB14vSdOccd7+9n0F9ERotIMRH5VZz+DxL2q30iku8K4gdAVY/i3BxT3l3Xi+6+tFlExrmfaU+cfiOmuLEUFJFrRWS5u50F4kdPS9mS15k8u/4Bcfx5V9FMnI4oirrTSuPsNAk3NJx2/z/Fn3eChODcV14aJzEWdsc/A7yYzPYm8GeN9C7gB+BanLsrCgNFgC1AM6AH8IHPssXc/8twf+25vHaWEGN3YKL7Oj9OTzcFgYHAC+74AsBaoHoycZ72Kd/nQEd3uChuLQ1oD3zpvu6PT20IeBW4131dHOcOmsJJttEDWOhuoyxOra08zm2XZ5KLK7kypzXez/2gGm6NNOn2SVJbBYYCI9zXi4Ha7usWOLcwJ113f2A00M/nM9nsrjfZ/Qbnjr59PjF8Bsy5ws8gcRiYBbR1X/cGxl9p/O7rKjjflVB3uKTPfJOBrsnso/mA1UAZn+1/5LPcCIKkRmq9P6XsnKo2TRgQkXzAqyJyMxCPUxMrC/zus8yPwEfuvF+papSI3IJ7S5043aXmx6nJJecNEXkB577lATj3M89U1TNuDDOAm4BvgDdF5HWcL1HkFZRrPvC2iBQAOgIrVPWciEQAjeXPc4DFcDra2J1k+YIiEuWWfxtOwkuYf6KI1MbprzJfCtuPAG6XP899heJ8Cbf5zNMa+ExV44A/RGQ5cB1wClijTkcgXklz+27N8Ubgc/mzi9wCqSzyKTBcRKr7jGtJ8vtNXeAXnxg+w/kRBP8/A1/TcBLYUpz76t+7wvh7u9+JusBgdR5oCdBWRIYBhYCSOJWAr5MsezVOBzYL3e2E4Ny+GXQskfqvL1AGuFZVY8TpuSjUdwZVXeHuVLcBE0TkP8BxYKGq9vFjG0+r6hcJAyJya3IzqeoOcfr07Ay8IiKLVfVlfwqhqudFZBnQAecLNDVhc8BjqrogjVWcU9WmIlII51Hbg4B3cDpqXqqq3cVpLFmWwvIC9FDV7f7Em4wz6Vwu+WBEFuD8IK5V1YeucPuxXNpgm7A/5AFO+P4Qp0adR5e/iVPrTAyNZPYbEUltnf5+Br5m41QQSuIcAS3BOQLyN/5pqjpYnN67vnVPe5wA3sOpee5zT3eEJrOsAFtU9YobVrMbO0fqv2LAITeJtgWqJp1BnOcq/aGqHwDjcR5p8D3QSkQSznkWFpE6fm4zErhDRAqJSGGcw/JIcVo9z6rqJ8Ab7naSinFrxsmZBjzAn7VbcJLiownLiEgdd5vJUqf3/seBp+TPbgwTuhvr7zNrNM4pjgQLgMfErYKI08tVcuXuLc552zI4j4dYk1IsGaGqHVS1qZ9JNKk/gKtEpJRbw+/irvMUsFtE7oLEc75N0ljXBJzD8TLucEr7zXaghvzZst/bZx3+fgaJVPU0zpHU2zhHN3HpiV+d3rsmA3/jz6R5xK3d+l7p4BvLdqCMuFeouOdmG6S2nezKEqn/pgDNRWQTzjmt/yUzTxvgJxHZgLODv62qh3F26s9EZCN/Hp6lSVXX43zB1uCcMx2vqhuARsAa9xD778ArySw+DtgobmNTEt/idBS9SJ3HSYCT+LcC68VpQHmfNI5Y3Fg24nT0+y/gNbfsvsstBeq7jQu9cWpN+dzYtrjDSc101/sTTg1pmKr+nsx8qRKRf4nTi3shEdnv1owyjarG4DTkrME5xeG7T/QFBojITziHtZc1qiVZ10Wcmv1V7nCy+42qnsN5Xtk3IrIOJzGddFfj72eQ1DTgXvd/uuJ3vY7zAx0HfIBzvncBTqJOMAEY6+67IThJ9nV3O1E4pxSCjvX+ZHIMETmtqmldERH0RKSIqp52a/VjgJ9VdZTXcWU294fvtKr+2+tY0mI1UpOTnBKfC/JzsIfdGt0WnMP5970NJ/OJyBs4teRMPSceKFYjNcaYDLIaqTHGZJAlUmOMySBLpMYYk0GWSI0xJoMskRpjTAZZIjXGmAz6f++sGA1XfV49AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC SCORE = 0.987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9871380562766459"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw ROC curve with predictions on train data\n",
    "draw_roc(y_train4, y_train4_pred_prob_rf_grd[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe29f4",
   "metadata": {},
   "source": [
    "With train set, we have obtained the following metrics with Random Forest model.\n",
    "\n",
    "AUC SCORE = 0.987 <br>\n",
    "ACCURACY :  0.9261145974324269 <br>\n",
    "SENSITIVITY :  0.9103846951472419 <br>\n",
    "PRECISION :  0.971063582696216 <br>\n",
    "\n",
    "#### All the key metrics above are above 90% and particularly precision is very good at 97% with the training data. \n",
    "\n",
    "Let us see how the RF model fares with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8b40a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set\n",
    "y_test4_pred_rf_grd, y_test4_pred_prob_rf_grd  = predict_and_proba(rf_grd_best_model, X_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b6e1b2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "\n",
      " [[17273  1082]\n",
      " [ 3022 28623]]\n",
      "\n",
      "TN : 17273\n",
      "\n",
      "FP : 1082\n",
      "\n",
      "FN : 3022\n",
      "\n",
      "TP : 28623\n",
      "\n",
      "ACCURACY :  0.91792\n",
      "\n",
      "SENSITIVITY :  0.904503081055459\n",
      "\n",
      "PRECISION :  0.963575155697694\n",
      "\n",
      "FALSE POSITIVITY RATE :  0.05894851539090166\n",
      "\n",
      "SPECIFICITY :  0.9410514846090984\n",
      "\n",
      "\n",
      " Classification Report :\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89     18355\n",
      "           1       0.96      0.90      0.93     31645\n",
      "\n",
      "    accuracy                           0.92     50000\n",
      "   macro avg       0.91      0.92      0.91     50000\n",
      "weighted avg       0.92      0.92      0.92     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Confusion matrix and key evaluation metrics on Test set results\n",
    "print_binary_classification_summary(y_test4, y_test4_pred_rf_grd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e072a6",
   "metadata": {},
   "source": [
    "With test set, we observe the following metrics with Random Forest model. \n",
    "\n",
    "ACCURACY :  0.91792 <br>\n",
    "SENSITIVITY :  0.904503081055459 <br>\n",
    "PRECISION :  0.963575155697694 <br>\n",
    "\n",
    "##### They all appear very close to what was observed with the train set, which means the RF model is able to generalize well\n",
    "\n",
    "So we will have this model as base-model2 to be part of the stacking ensemble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e901b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "base2 = rf_grd_best_model\n",
    "predictions_base2 = y_test4_pred_rf_grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c41c1a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_model2.pkl']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(base2, \"base_model2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "916e0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base2 = joblib.load(\"base_model2.pkl\")\n",
    "#y_test4_pred_rf_grd, y_test4_pred_prob_rf_grd  = predict_and_proba(base2, X_test4)\n",
    "#predictions_base2 = y_test4_pred_rf_grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c919db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e44b25f2",
   "metadata": {},
   "source": [
    "#### 4.2.1.3 Base Model-3 (XGBoost Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f768eac",
   "metadata": {},
   "source": [
    "##### Let us train a basic XGB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0f1de0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e24d647e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    77152\n",
       "0    44753\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train4.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4edfad6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5800627333056823"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the ratio of negative class to positive class, this ratio will be passed as scale_pos_weight\n",
    "# to XGB to address class imbalance\n",
    "y_train4.value_counts()[0] / y_train4.value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28f1a7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              eval_metric='logloss', gamma=0, gpu_id=0, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=12,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=0.58, subsample=1,\n",
       "              tree_method='gpu_hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train XGB. Set scale_pos_weight.\n",
    "xgbm = XGBClassifier(scale_pos_weight=0.58,  tree_method='gpu_hist', eval_metric='logloss')\n",
    "\n",
    "xgbm.fit(X_train4, y_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4522f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on train set\n",
    "y_train4_pred_xgb, y_train4_pred_prob_xgb  = predict_and_proba(xgbm, X_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7fde8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "\n",
      " [[43683  1070]\n",
      " [ 4620 72532]]\n",
      "\n",
      "TN : 43683\n",
      "\n",
      "FP : 1070\n",
      "\n",
      "FN : 4620\n",
      "\n",
      "TP : 72532\n",
      "\n",
      "ACCURACY :  0.9533243099134572\n",
      "\n",
      "SENSITIVITY :  0.94011820821236\n",
      "\n",
      "PRECISION :  0.9854623515665335\n",
      "\n",
      "FALSE POSITIVITY RATE :  0.023909011686367394\n",
      "\n",
      "SPECIFICITY :  0.9760909883136326\n",
      "\n",
      "\n",
      " Classification Report :\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94     44753\n",
      "           1       0.99      0.94      0.96     77152\n",
      "\n",
      "    accuracy                           0.95    121905\n",
      "   macro avg       0.94      0.96      0.95    121905\n",
      "weighted avg       0.96      0.95      0.95    121905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix and key classification metrics from prediction results on train set.\n",
    "print_binary_classification_summary(y_train4, y_train4_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "561de76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC SCORE = 0.994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9944393842693389"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROC curve on train set predictions\n",
    "draw_roc(y_train4, y_train4_pred_prob_xgb[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e66f71",
   "metadata": {},
   "source": [
    "##### Accuracy, Sensitivity, Precision etc look very good. AUC score is very close to 1. XGB looks a good candidate for base model.\n",
    "\n",
    "##### Let us try hyperparameter tuning by using RandomSearchCV first.\n",
    "\n",
    "We try tuning the following hyper-params for XGBoost <br>\n",
    "max_depth <br>\n",
    "learning_rate <br>\n",
    "n_estimators <br>\n",
    "subsample <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc2124d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': [5, 7, 9, 11, 13, 15], 'learning_rate': [0.025, 0.05, 0.075, 0.1], 'n_estimators': [100, 200, 300, 400, 500], 'subsample': [0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6, 0.7000000000000001, 0.8, 0.9, 1.0]}\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# creating a KFold object \n",
    "folds = StratifiedKFold(n_splits = 3, shuffle = True, random_state=100)\n",
    "\n",
    "\n",
    "# specify range of hyperparameters\n",
    "param_grid = {\n",
    "             'max_depth': [5,7,9,11,13,15],\n",
    "             'learning_rate': [0.025, 0.05, 0.075, 0.10],\n",
    "             'n_estimators':  [int(x) for x in np.linspace(100,500,5)],\n",
    "             'subsample': [float(x) for x in np.linspace(0.1, 1, 10)]\n",
    "             }          \n",
    "\n",
    "print(param_grid)\n",
    "\n",
    "# Use Multi scorer\n",
    "scoring_measures = {'Precision': 'precision', 'Recall': 'recall', 'AUC' : 'roc_auc'}\n",
    "refit = 'AUC'\n",
    "\n",
    "# specify model\n",
    "xgb_model_rdm = XGBClassifier(random_state=123, tree_method='gpu_hist', scale_pos_weight=0.58, eval_metric='logloss')\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(estimator=xgb_model_rdm, param_distributions=param_grid, scoring=scoring_measures, \n",
    "                                       cv=folds, refit=refit, verbose=1, n_iter=50)\n",
    "\n",
    "# fit the model\n",
    "random_results_xgb = random_search_xgb.fit(X_train4, y_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b1bf5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_test_AUC\n",
      "\n",
      "Best hyperparameters:  {'subsample': 0.5, 'n_estimators': 500, 'max_depth': 15, 'learning_rate': 0.025}\n",
      "\n",
      "Best Score : 0.98609\n",
      "   mean_test_Precision  mean_test_Recall  mean_test_AUC\n",
      "9             0.953309          0.932562       0.986089\n"
     ]
    }
   ],
   "source": [
    "## Print the results of random search \n",
    "xgb_rand_best_score = grid_result_summary(random_results_xgb, scoring_measures, refit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d98d44",
   "metadata": {},
   "source": [
    "##### Based on the best hyperparam values found by RandomizedSearchCV, let us use GridSearchCV to further narrow down the optimal hyper-param values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30659513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# creating a KFold object \n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state=100)\n",
    "\n",
    "\n",
    "# specify range of hyperparameters\n",
    "\n",
    "### Due to computational constraints, we omit gamma and n_estimators from gridsearch.\n",
    "param_grid = {\n",
    "             'max_depth': [13, 15, 17],\n",
    "             'learning_rate': [0.01, 0.025, 0.05],\n",
    "             'subsample': [0.4, 0.5, 0.6, 0.7],\n",
    "             'min_child_weight': [0.10, 0.20, 0.30]\n",
    "             #'gamma': np.arange(0.0,40.0,0.005),\n",
    "             }      \n",
    "\n",
    "#print(param_grid)\n",
    "\n",
    "# Use Multi scorer\n",
    "scoring_measures = {'Precision': 'precision', 'Recall': 'recall', 'AUC' : 'roc_auc'}\n",
    "refit = 'AUC'\n",
    "\n",
    "# specify model\n",
    "\n",
    "xgb_model_grd = XGBClassifier(random_state=123, \n",
    "                          scale_pos_weight=0.58, \n",
    "                          tree_method='gpu_hist', \n",
    "                          eval_metric='logloss', \n",
    "                          n_estimators=500) \n",
    "\n",
    "\n",
    "# set up GridSearchCV()\n",
    "grid_search_xgb = GridSearchCV(estimator = xgb_model_grd, \n",
    "                       param_grid = param_grid, \n",
    "                       scoring = scoring_measures, \n",
    "                       refit=refit,\n",
    "                       cv = folds, \n",
    "                       verbose = 1)    \n",
    "\n",
    "# fit the model\n",
    "grid_results_xgb = grid_search_xgb.fit(X_train4, y_train4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30f9261e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_test_AUC\n",
      "\n",
      "Best hyperparameters:  {'learning_rate': 0.025, 'max_depth': 13, 'min_child_weight': 0.1, 'subsample': 0.7}\n",
      "\n",
      "Best Score : 0.98666\n",
      "    mean_test_Precision  mean_test_Recall  mean_test_AUC\n",
      "39             0.955828          0.933093       0.986663\n"
     ]
    }
   ],
   "source": [
    "## Print the results of grid search \n",
    "xgb_grd_best_score = grid_result_summary(grid_results_xgb, scoring_measures, refit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79c986af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best fit model\n",
    "xgb_grd_best_model = grid_results_xgb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f3774e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on train data\n",
    "y_train4_pred_xgb_grd, y_train4_pred_prob_xgb_grd  = predict_and_proba(xgb_grd_best_model, X_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc36cc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "\n",
      " [[44635   118]\n",
      " [  966 76186]]\n",
      "\n",
      "TN : 44635\n",
      "\n",
      "FP : 118\n",
      "\n",
      "FN : 966\n",
      "\n",
      "TP : 76186\n",
      "\n",
      "ACCURACY :  0.9911078298675198\n",
      "\n",
      "SENSITIVITY :  0.9874792617171299\n",
      "\n",
      "PRECISION :  0.9984535542042356\n",
      "\n",
      "FALSE POSITIVITY RATE :  0.00263669474672089\n",
      "\n",
      "SPECIFICITY :  0.9973633052532791\n",
      "\n",
      "\n",
      " Classification Report :\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     44753\n",
      "           1       1.00      0.99      0.99     77152\n",
      "\n",
      "    accuracy                           0.99    121905\n",
      "   macro avg       0.99      0.99      0.99    121905\n",
      "weighted avg       0.99      0.99      0.99    121905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix and key classification metrics on train set predictions\n",
    "print_binary_classification_summary(y_train4, y_train4_pred_xgb_grd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fef0ab68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC SCORE = 1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9997812984771206"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROC curve on train set predictions\n",
    "draw_roc(y_train4, y_train4_pred_prob_xgb_grd[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d93f2f",
   "metadata": {},
   "source": [
    "##### With train set, we observe the following metrics with XGBoost model. \n",
    "\n",
    "ACCURACY :  0.9911078298675198 <br>\n",
    "\n",
    "SENSITIVITY :  0.9874792617171299 <br>\n",
    "\n",
    "PRECISION :  0.9984535542042356 <br>\n",
    "\n",
    "The metrics does seem very high, accuracy, sensitivity and precision all above 98%. \n",
    "\n",
    "##### Let us check the prediction results on test set to see if it is a overfit case or a very robust model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ce31f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set\n",
    "y_test4_pred_xgb_grd, y_test4_pred_prob_xgb_grd  = predict_and_proba(xgb_grd_best_model, X_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b87d6025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "\n",
      " [[17038  1317]\n",
      " [ 2152 29493]]\n",
      "\n",
      "TN : 17038\n",
      "\n",
      "FP : 1317\n",
      "\n",
      "FN : 2152\n",
      "\n",
      "TP : 29493\n",
      "\n",
      "ACCURACY :  0.93062\n",
      "\n",
      "SENSITIVITY :  0.9319955759203665\n",
      "\n",
      "PRECISION :  0.9572541382667965\n",
      "\n",
      "FALSE POSITIVITY RATE :  0.07175156633070008\n",
      "\n",
      "SPECIFICITY :  0.9282484336692999\n",
      "\n",
      "\n",
      " Classification Report :\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91     18355\n",
      "           1       0.96      0.93      0.94     31645\n",
      "\n",
      "    accuracy                           0.93     50000\n",
      "   macro avg       0.92      0.93      0.93     50000\n",
      "weighted avg       0.93      0.93      0.93     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix and key classification metrics on train set predictions\n",
    "print_binary_classification_summary(y_test4, y_test4_pred_xgb_grd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efb358",
   "metadata": {},
   "source": [
    "##### With test set, we observe the following metrics with XGBoost model. \n",
    "\n",
    "ACCURACY :  0.93062 <br>\n",
    "\n",
    "SENSITIVITY :  0.9319955759203665 <br>\n",
    "\n",
    "PRECISION :  0.9572541382667965 <br>\n",
    "    \n",
    "The metrics look very good - accuracy, sensitivity and precision all above 93%.\n",
    "\n",
    "##### So this XGBoost model performs well on test set also and can be taken as one of the base models in the stacking ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe83e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "base3 = xgb_grd_best_model\n",
    "predictions_base3 = y_test4_pred_xgb_grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "391efd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_model3.pkl']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(base3, 'base_model3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ec115c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base3 = joblib.load('base_model3.pkl')\n",
    "#y_test4_pred_xgb_grd, y_test4_pred_prob_xgb_grd  = predict_and_proba(base3, X_test4)\n",
    "#predictions_base3 = y_test4_pred_xgb_grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79a658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d7c71dd",
   "metadata": {},
   "source": [
    "### 4.2.2 Stacking Blender Model (Logistic Regression Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e845ded5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the length of predictions of base models, which we will use for training the blender model.\n",
    "len(predictions_base1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c4638a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_base2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b57bbbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_base3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07feaa",
   "metadata": {},
   "source": [
    "##### Combine the base model predictions as the new feature set, for training and testing the blender model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8fee2a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictions of the three individual base models will be the features for training the Stacking blender.\n",
    "### So, stack together the three predictions in to a single 3D vector for each input data record.\n",
    "\n",
    "X_test4_base_predictions = np.column_stack((predictions_base1, predictions_base2, predictions_base3))\n",
    "X_train5 = X_test4_base_predictions \n",
    "y_train5 = y_test4\n",
    "\n",
    "# X_test3 was previously set aside for testing the stacking blender. Get the base model predictions for that to \n",
    "# input those to blender to test.\n",
    "y_test3_pred_base1, y_test3_pred_prob_base1  = predict_and_proba(base1, X_test3)\n",
    "y_test3_pred_base2, y_test3_pred_prob_base2  = predict_and_proba(base2, X_test3)\n",
    "y_test3_pred_base3, y_test3_pred_prob_base3  = predict_and_proba(base3, X_test3)\n",
    "\n",
    "X_test3_base_predictions = np.column_stack((y_test3_pred_base1, y_test3_pred_base2, y_test3_pred_base3))\n",
    "X_test5 = X_test3_base_predictions \n",
    "y_test5 = y_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9555b65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3), (50000,), (30000, 3), (30000,))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train5.shape, y_train5.shape, X_test5.shape, y_test5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b430a72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " ...\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n",
      "\n",
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " ...\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train5)\n",
    "print()\n",
    "print(X_test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2d7c3bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89166     1\n",
      "21632     1\n",
      "239770    1\n",
      "143220    1\n",
      "125048    1\n",
      "Name: label, dtype: int32\n",
      "\n",
      "30765     0\n",
      "125310    1\n",
      "222323    1\n",
      "95680     1\n",
      "116781    0\n",
      "Name: label, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(y_train5[:5])\n",
    "print()\n",
    "print(y_test5[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd253216",
   "metadata": {},
   "source": [
    "##### Build a Logistic Regression model and fit on the predictions of the base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5a52a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_blender = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3f0451a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_blender.fit(X_train5, y_train5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "82e96ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions of the blender on the train set\n",
    "y_train5_pred_blend, y_train5_pred_prob_blend  = predict_and_proba(lr_blender, X_train5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "31758384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "\n",
      " [[17417   938]\n",
      " [ 2816 28829]]\n",
      "\n",
      "TN : 17417\n",
      "\n",
      "FP : 938\n",
      "\n",
      "FN : 2816\n",
      "\n",
      "TP : 28829\n",
      "\n",
      "ACCURACY :  0.92492\n",
      "\n",
      "SENSITIVITY :  0.9110127982303682\n",
      "\n",
      "PRECISION :  0.9684885947525783\n",
      "\n",
      "FALSE POSITIVITY RATE :  0.05110324162353582\n",
      "\n",
      "SPECIFICITY :  0.9488967583764641\n",
      "\n",
      "\n",
      " Classification Report :\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90     18355\n",
      "           1       0.97      0.91      0.94     31645\n",
      "\n",
      "    accuracy                           0.92     50000\n",
      "   macro avg       0.91      0.93      0.92     50000\n",
      "weighted avg       0.93      0.92      0.93     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix and key classification metric values for the train set predictions\n",
    "print_binary_classification_summary(y_train5, y_train5_pred_blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "822d5395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC SCORE = 0.955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.955163086610307"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROC curve for the train set predictions\n",
    "draw_roc(y_train5, y_train5_pred_prob_blend[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb19e2",
   "metadata": {},
   "source": [
    "#### We observe the following evaluation metric values on the train set\n",
    "\n",
    "AUC SCORE = 0.955\n",
    "\n",
    "ACCURACY :  0.92492\n",
    "\n",
    "SENSITIVITY :  0.9110127982303682\n",
    "\n",
    "PRECISION :  0.9684885947525783\n",
    "    \n",
    "##### All the key metrics values including sensitivity are above 90% and precision is very good at 96.8%. This is very good and indicates that the LR blender does a very good job in final predictions on train set.\n",
    "\n",
    "\n",
    "##### Let us test the blender with it's test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8cff4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "y_test5_pred_blend, y_test5_pred_prob_blend  = predict_and_proba(lr_blender, X_test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ba0253c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "\n",
      " [[10401   612]\n",
      " [ 1679 17308]]\n",
      "\n",
      "TN : 10401\n",
      "\n",
      "FP : 612\n",
      "\n",
      "FN : 1679\n",
      "\n",
      "TP : 17308\n",
      "\n",
      "ACCURACY :  0.9236333333333333\n",
      "\n",
      "SENSITIVITY :  0.9115710749460157\n",
      "\n",
      "PRECISION :  0.9658482142857143\n",
      "\n",
      "FALSE POSITIVITY RATE :  0.05557068918550804\n",
      "\n",
      "SPECIFICITY :  0.944429310814492\n",
      "\n",
      "\n",
      " Classification Report :\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90     11013\n",
      "           1       0.97      0.91      0.94     18987\n",
      "\n",
      "    accuracy                           0.92     30000\n",
      "   macro avg       0.91      0.93      0.92     30000\n",
      "weighted avg       0.93      0.92      0.92     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix and key classification metric values for the test set predictions\n",
    "print_binary_classification_summary(y_test5, y_test5_pred_blend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08caec22",
   "metadata": {},
   "source": [
    "#### We observe the following evaluation metric values on the train set\n",
    "\n",
    "ACCURACY :  0.9236333333333333\n",
    "\n",
    "SENSITIVITY :  0.9115710749460157\n",
    "\n",
    "PRECISION :  0.9658482142857143\n",
    "\n",
    "##### All the key metrics values including sensitivity are above 90% and precision is very good at 96.5%. This is very similar to the results on train set and indicates that the LR blender does a very good job in generalization and final predictions on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a9ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aebd6108",
   "metadata": {},
   "source": [
    "# 5. Final Whole Model Validation \n",
    "\n",
    "#### Test finally the whole model pipeline with X_test1 set aside previously\n",
    "\n",
    "The procedure will be as follows.\n",
    "\n",
    "1. Pass the test data to the trained Autoencoder to get the feature extractions.\n",
    "2. Pass the features extracted, separately to each of the three base models trained so far. \n",
    "   Get the predictions (normal/attack) from each base model. It will be label 0 or 1.\n",
    "3. Stack together the predictions from each of the three base models, in to a 3D feature vector.\n",
    "4. Pass this new feature set finally to the stacking blender to get the final classification prediction.\n",
    "5. Validate the results using standard classification metrics and confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8d1e2c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 192)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1b54d8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.62209643, -0.43169889, -0.3222141 , ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       [-0.62209643, -0.43169889, -0.3222141 , ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       [-0.4084536 ,  0.48085395,  0.11347547, ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       ...,\n",
       "       [-0.62210931, -0.43169889, -0.3222141 , ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       [-0.62210416, -0.43169889, -0.3222141 , ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ],\n",
       "       [-0.61935012, -0.43169889, -0.25997273, ..., -0.08674712,\n",
       "        -0.01780678, -0.0022255 ]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f973be",
   "metadata": {},
   "source": [
    "#### Pass the test data to the trained Autoencoder, to get the feature extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3df96c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1_AE_encodings = stacked_encoder.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "150a4eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 75)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1_AE_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a677418c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5138482 ,  1.1586665 , -1.4243598 , ..., -0.68094534,\n",
       "         0.41030177,  0.3592629 ],\n",
       "       [ 0.50728583,  1.0989724 , -1.42438   , ..., -0.5968498 ,\n",
       "         0.44042745,  0.33662507],\n",
       "       [-1.3518404 , -1.247841  , -0.925708  , ...,  0.48217323,\n",
       "         0.54721713, -1.0700496 ],\n",
       "       ...,\n",
       "       [ 0.38996333,  1.1446431 , -1.4798985 , ...,  0.1321939 ,\n",
       "         0.53201187,  0.47879806],\n",
       "       [10.292419  ,  0.587174  ,  3.0315006 , ...,  4.557115  ,\n",
       "        -1.4975767 , -0.8466593 ],\n",
       "       [-0.85561603,  0.3667158 , -1.2281933 , ...,  0.73536325,\n",
       "         0.7157045 , -0.8397492 ]], dtype=float32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1_AE_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cfc8b0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_test1_AE_encodings.pkl']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X_test1_AE_encodings, 'X_test1_AE_encodings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fdb75bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test1_AE_encodings = joblib.load('X_test1_AE_encodings.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a0931b",
   "metadata": {},
   "source": [
    "#### Pass the features extracted, separately to each of the three base models trained so far. Get the predictions (normal/attack) from each base model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a41c0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1_pred_base1, y_test1_pred_prob_base1  = predict_and_proba(base1, X_test1_AE_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f8fceac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1_pred_base2, y_test1_pred_prob_base2  = predict_and_proba(base2, X_test1_AE_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2cb40be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1_pred_base3, y_test1_pred_prob_base3  = predict_and_proba(base3, X_test1_AE_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c22ac7",
   "metadata": {},
   "source": [
    "#### Stack together the predictions from each of the three base models, in to a 3D feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4a91cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1_base_predictions = np.column_stack((y_test1_pred_base1, y_test1_pred_base2, y_test1_pred_base3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306621f1",
   "metadata": {},
   "source": [
    "#### Pass this new feature set finally to the stacking blender to get the final classification prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "02e3735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1_pred_blender, y_test1_pred_prob_blender  = predict_and_proba(lr_blender, X_test1_base_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f1f3d",
   "metadata": {},
   "source": [
    "#### Validate the results using standard classification metrics and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "10c22c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "\n",
      " [[10455   558]\n",
      " [ 1652 17335]]\n",
      "\n",
      "TN : 10455\n",
      "\n",
      "FP : 558\n",
      "\n",
      "FN : 1652\n",
      "\n",
      "TP : 17335\n",
      "\n",
      "ACCURACY :  0.9263333333333333\n",
      "\n",
      "SENSITIVITY :  0.9129931005424764\n",
      "\n",
      "PRECISION :  0.9688146202425529\n",
      "\n",
      "FALSE POSITIVITY RATE :  0.050667393080904385\n",
      "\n",
      "SPECIFICITY :  0.9493326069190956\n",
      "\n",
      "\n",
      " Classification Report :\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90     11013\n",
      "           1       0.97      0.91      0.94     18987\n",
      "\n",
      "    accuracy                           0.93     30000\n",
      "   macro avg       0.92      0.93      0.92     30000\n",
      "weighted avg       0.93      0.93      0.93     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_binary_classification_summary(y_test1, y_test1_pred_blender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20878e3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ACCURACY :  0.9263333333333333\n",
    "\n",
    "SENSITIVITY :  0.9129931005424764\n",
    "\n",
    "PRECISION :  0.9688146202425529\n",
    "\n",
    "All the key metrics values including sensitivity are above 91% and precision is very good at around 97%. \n",
    "\n",
    "#### The final whole model (Auto encoder and Stacking)  does a very good job in final predictions on test set. FPR is also very low at just 5% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0fe009",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08432c7",
   "metadata": {},
   "source": [
    "### The final whole model (Auto encoder and Stacking ensemble) does a very good job in final predictions on test set. FPR is also very low at just 5%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
